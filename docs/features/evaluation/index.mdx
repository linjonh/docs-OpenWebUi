---
sidebar_position: 6
title: "📝 评估"
---


## 为什么要评估模型？

认识一下 **Alex**，一位中型公司的机器学习工程师。Alex 知道市面上有很多 AI 模型——如 GPTs、LLaMA 等等——但哪一个最适合目前的工作？这些模型在纸面上看起来都很厉害，但 Alex 不能仅仅依靠公开排行榜。这些模型在不同的场景中表现各异，有些模型甚至可能是在评估数据集上训练过的（很狡猾！）。此外，这些模型的生成文本方式有时会感觉...不太合适。

这就是 Open WebUI 的用武之地。它为 Alex 和团队提供了一种简单的方法，根据实际需要评估模型。没有复杂的数学，没有繁重的工作，只需在与模型交互时投个赞成或反对票。

### 总结

- **为什么评估很重要**: 模型众多，但并非都适合你的具体需求。一般的公开排行榜依赖性不可靠。
- **如何解决**: Open WebUI 提供了一个内置的评估系统。用赞成/反对来对模型回复进行评分。
- **幕后机制**: 评分会调整你的个性化排行榜，且评分的对话快照将用于未来模型的微调！
- **评估选项**: 
  - **竞技场模式**: 随机选模供你比较。
  - **普通交互**: 像平常一样聊天并对回复进行评分。

---

### 为什么公开评估不够？

- 公开排行榜未能针对**你的**特定应用场景量身定制。
- 一些模型是在评估数据集上训练的，影响了结果的公平性。
- 一个模型可能整体表现不错，但其交流风格或回复可能不符合你想要的“感觉”。

### 解决方案：使用 Open WebUI 进行个性化评估

Open WebUI 内置了评估功能，让你和团队在与模型交互的过程中轻松发现最适合的模型。

如何操作？很简单！

- **在聊天时**，如果喜欢某个回复就投个赞成票，不喜欢就投反对票。如果消息有**关联消息**（例如重新生成的回复或部分模型对比的一部分），你就在为你的**个人排行榜**贡献数据。
- **排行榜**可在管理员部分轻松访问，帮助你追踪哪些模型根据团队需求表现最佳。

一个很酷的功能？**每当你评分某个回复时**，系统会捕获一个**对话快照**，以后可以用来优化模型甚至用于未来模型培训。（请注意，这功能仍在开发中！）

---

### 两种评估 AI 模型的方法

Open WebUI 提供了两种简单直接的 AI 模型评估方式。

### **1. 竞技场模式**

**竞技场模式**从可用模型池中随机选择，确保评估公平且不偏袒。这有助于消除手动比较的潜在缺陷：**生态有效性**——确保你不会有意或无意地偏袒某个模型。

使用方法：
- 从竞技场模型选择器中选择一个模型。
- 像平常一样使用，但现在你处于“竞技场模式”中。
  
你提供反馈影响排行榜时，需要所谓的**关联消息**。什么是关联消息？关联消息是由相同查询生成的任何替代回复（例如消息重生成或多个模型并排生成的回复）。通过这种方式，你是在**正面交锋**中进行比较。

- **评分技巧**: 当你对一个回复点赞时，另一个会自动获得反对票。所以请谨慎，只对你认为真正优秀的回复点赞！
- 一旦你评分了回复，就可以查看排行榜了解模型的表现。

以下是竞技场模式界面的预览：

![竞技场模式示例](/images/evaluation/arena.png)

需要更深入的玩法？你甚至可以复制一个 [**Chatbot Arena**](https://lmarena.ai/) 样式的设置！

![Chatbot Arena 示例](/images/evaluation/arena-many.png)

### **2. 普通交互**

如果你不想切换到“竞技场模式”，也可以正常使用 Open WebUI 并在日常操作中对模型回复进行评分。在觉得合适时对模型回复投赞成或反对票即可。然而，**如果你希望你的反馈被用于排行榜排名**，你需要**切换模型并与不同的模型进行交互**。这可以确保有一个**关联消息**进行比较——只有对两个不同模型的比较会影响排名。

例如，这是你如何在普通交互中进行评分：

![普通模型评分界面](/images/evaluation/normal.png)

以下是设置多个模型比较（类似竞技场）的示例：

![多模型比较示例](/images/evaluation/normal-many.png)

---

## 排行榜

评分后，请检查管理员面板下的 **排行榜**。这里可以直观地看到模型的表现，它们通过 **Elo 评分系统** 排名（类似象棋排名！）。你将获得一个真实的视图，了解哪些模型在评估过程中真正脱颖而出。

这是一个示例排行榜布局：

![排行榜示例](/images/evaluation/leaderboard.png)

### 基于主题的重新排名

在评级聊天时，可以为其 **添加主题标签** 以获取更详细的见解。这在处理不同领域（如 **客户服务、创意写作、技术支持** 等）时特别有用。

#### 自动标签
Open WebUI 尝试根据对话主题**自动为聊天添加标签**。然而，具体自动标签功能可能会因所使用的模型而有所差异，**有时可能会失败**或误解对话内容。如果出现这种情况，最佳做法是**手动为聊天添加标签**以确保反馈准确。

- **如何手动添加标签**：当你对响应进行评级时，将有机会根据对话内容添加自己的标签。
  
不要跳过这一步！标签功能非常强大，因为它允许你**基于特定主题重新排名模型**。例如，你可能想了解哪个模型在回答技术支持问题方面的表现最佳，而不是处理一般客户咨询时的表现。

以下是重新排名的示例：

![根据主题重新排名的排行榜](/images/evaluation/leaderboard-reranked.png)

---

### 小提示：用于模型微调的聊天快照

每当你对模型的响应进行评级时，Open WebUI 会 *捕获该聊天的快照*。这些快照最终可用于**微调你自己的模型**，这样你的评估就能融入 AI 的持续改进过程。

*(敬请期待有关此功能的更多更新，其开发工作正在积极推进！)*

---

## 总结

**简单来说**，Open WebUI 的评估系统有两个明确目标：
1. 帮助你**轻松比较模型**。
2. 最终找到最适合你个人需求的模型。

系统的核心在于使 AI 模型评估对每位用户而言都变得**简单、透明和可定制**。无论是通过竞技场模型还是正常聊天交互，**你都完全掌控确定哪种 AI 模型最适合你的具体用例**！

**一如既往**，你的所有数据都安全地保存在**你的实例中**，除非你明确选择 **加入社区共享**，否则数据不会共享。始终优先考虑你的隐私和数据自治。