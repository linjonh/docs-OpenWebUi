---

sidebar_position: 4  
title: "🌐 开始使用与 OpenAI 兼容的服务器"

---

## 概述

Open WebUI 不仅支持 OpenAI/Ollama/Llama.cpp——您可以连接**任何实现与 OpenAI 兼容的 API 的服务器**，无论是本地运行还是远程运行。这非常适合运行不同的语言模型，或者如果您已经有自己喜欢的后端或生态系统。本指南将向您展示如何：

- 设置一个与 OpenAI 兼容的服务器（包括一些流行选项）
- 将其连接到 Open WebUI
- 立即开始聊天

## 第一步：选择一个与 OpenAI 兼容的服务器

有许多服务器和工具提供与 OpenAI 兼容的 API。以下是一些最受欢迎的选择：

- [Llama.cpp](https://github.com/ggml-org/llama.cpp)：非常高效，可在 CPU 和 GPU 上运行
- [Ollama](https://ollama.com/)：超级用户友好且跨平台
- [LM Studio](https://lmstudio.ai/)：用于 Windows/Mac/Linux 的功能丰富的桌面应用
- [Lemonade (ONNX TurnkeyML)](https://github.com/onnx/turnkeyml)：基于 ONNX 的快速后端，支持 NPU/iGPU 加速

选择适合您工作流的选项即可！

---

#### 🍋 开始使用 Lemonade (ONNX TurnkeyML)

Lemonade 是一个即插即用的基于 ONNX 的与 OpenAI 兼容的服务器。以下是在 Windows 上尝试的方法：

1. [下载最新的 `.exe`](https://github.com/onnx/turnkeyml/releases)
2. 运行 `Lemonade_Server_Installer.exe`
3. 使用 Lemonade 的安装程序安装并下载一个模型
4. 一旦运行，您的 API 端点将是：

   ```
   http://localhost:8000/api/v0
   ```

![Lemonade 服务器](/images/getting-started/lemonade-server.png)

有关详情，请参阅[相关文档](https://github.com/onnx/turnkeyml)。

---

## 第二步：将服务器连接到 Open WebUI

1. 在浏览器中打开 Open WebUI。
2. 进入 ⚙️ **管理设置** → **连接** → **OpenAI Connections**。
3. 点击 ➕ **添加连接**。

   - **URL**：使用您的服务器 API 端点（例如，Ollama 的 `http://localhost:11434/v1`，或者您自己的 Llama.cpp 服务器的地址）。
   - **API Key**：除非需要，否则保持为空。

4. 点击保存。

*提示：如果在 Docker 中运行 Open WebUI，而您的模型服务器在主机上运行，请使用 `http://host.docker.internal:<你的端口>/v1`。*

##### **对于 Lemonade:** 添加 Lemonade 时，使用 `http://localhost:8000/api/v0` 作为 URL。

![Lemonade 连接](/images/getting-started/lemonade-connection.png)

---

## 第三步：开始聊天！

在聊天菜单中选择您连接的服务器的模型，然后开始吧！

就是这样！无论您选择 Llama.cpp、Ollama、LM Studio 还是 Lemonade，都可以轻松地在 Open WebUI 中尝试和管理多个模型服务器。

--- 

🚀 享受打造您的完美本地 AI 设置吧！