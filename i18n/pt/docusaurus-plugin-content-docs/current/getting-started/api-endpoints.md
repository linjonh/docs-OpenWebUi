---
sidebar_position: 400
title: "üîó Endpoints da API"
---

Este guia fornece informa√ß√µes essenciais sobre como interagir com os endpoints da API de forma eficaz para alcan√ßar integra√ß√£o e automa√ß√£o perfeitas usando nossos modelos. Observe que esta √© uma configura√ß√£o experimental e pode sofrer atualiza√ß√µes futuras para melhorias.

## Autentica√ß√£o

Para garantir acesso seguro √† API, √© necess√°ria autentica√ß√£o üõ°Ô∏è. Voc√™ pode autenticar suas solicita√ß√µes de API usando o mecanismo Bearer Token. Obtenha sua chave de API em **Configura√ß√µes > Conta** na Open WebUI ou, alternativamente, use um JWT (JSON Web Token) para autentica√ß√£o.

## Endpoints Not√°veis da API

### üìú Recuperar Todos os Modelos

- **Endpoint**: `GET /api/models`
- **Descri√ß√£o**: Busca todos os modelos criados ou adicionados via Open WebUI.
- **Exemplo**:

  ```bash
  curl -H "Authorization: Bearer YOUR_API_KEY" http://localhost:3000/api/models
  ```

### üí¨ Completa√ß√µes de Chat

- **Endpoint**: `POST /api/chat/completions`
- **Descri√ß√£o**: Serve como um endpoint de completas de chat compat√≠vel com a API OpenAI para modelos no Open WebUI, incluindo modelos Ollama, modelos OpenAI e modelos de Fun√ß√£o do Open WebUI.

- **Exemplo com Curl**:

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "llama3.1",
        "messages": [
          {
            "role": "user",
            "content": "Por que o c√©u √© azul?"
          }
        ]
      }
  ```
  
- **Exemplo em Python**:
  ```python
  import requests
  
  def chat_with_model(token):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      data = {
        "model": "granite3.1-dense:8b",
        "messages": [
          {
            "role": "user",
            "content": "Por que o c√©u √© azul?"
          }
        ]
      }
      response = requests.post(url, headers=headers, json=data)
      return response.json()
  ```

### ü¶ô Suporte ao Proxy da API Ollama

Se voc√™ deseja interagir diretamente com os modelos Ollama ‚Äî incluindo gera√ß√£o de embeddings ou streaming bruto de prompts ‚Äî o Open WebUI oferece uma passagem transparente para a API Ollama nativa via rota proxy.

- **URL Base**: `/ollama/<api>`
- **Refer√™ncia**: [Documenta√ß√£o da API Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)

#### üîÅ Gerar Completa√ß√µes (Streaming)

```bash
curl http://localhost:3000/ollama/api/generate -d {
  "model": "llama3.2",
  "prompt": "Por que o c√©u √© azul?"
}
```

#### üì¶ Listar Modelos Dispon√≠veis

```bash
curl http://localhost:3000/ollama/api/tags
```

#### üß† Gerar Embeddings

```bash
curl -X POST http://localhost:3000/ollama/api/embed -d {
  "model": "llama3.2",
  "input": ["Open WebUI √© incr√≠vel!", "Vamos gerar embeddings."]
}
```

Isso √© ideal para construir √≠ndices de pesquisa, sistemas de recupera√ß√£o ou pipelines personalizados usando modelos Ollama integrados ao Open WebUI.

### üß© Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)

O recurso de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) permite aprimorar as respostas incorporando dados de fontes externas. Abaixo, voc√™ encontrar√° os m√©todos para gerenciar arquivos e cole√ß√µes de conhecimento via a API, e como us√°-los em completas de chat de forma eficaz.

#### Envio de Arquivos

Para utilizar dados externos em respostas RAG, voc√™ precisa primeiro enviar os arquivos. O conte√∫do do arquivo enviado √© automaticamente extra√≠do e armazenado em um banco de dados vetorial.

- **Endpoint**: `POST /api/v1/files/`
- **Exemplo com Curl**:

  ```bash
  curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Accept: application/json" \
  -F "file=@/path/to/your/file" http://localhost:3000/api/v1/files/
  ```

- **Exemplo em Python**:

  ```python
  import requests
  
  def upload_file(token, file_path):
      url = http://localhost:3000/api/v1/files/
      headers = {
          Authorization: fBearer {token},
          Accept: application/json
      }
      files = {file: open(file_path, rb)}
      response = requests.post(url, headers=headers, files=files)
      return response.json()
  ```

#### Adicionar Arquivos √†s Cole√ß√µes de Conhecimento

Ap√≥s o upload, voc√™ pode agrupar arquivos em uma cole√ß√£o de conhecimento ou referenci√°-los individualmente em chats.

- **Endpoint**: `POST /api/v1/knowledge/{id}/file/add`
- **Exemplo com Curl**:

  ```bash
  curl -X POST http://localhost:3000/api/v1/knowledge/{knowledge_id}/file/add \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {"file_id": "your-file-id-here"}
  ```

- **Exemplo em Python**:

  ```python
  import requests

  def add_file_to_knowledge(token, knowledge_id, file_id):
      url = fhttp://localhost:3000/api/v1/knowledge/{knowledge_id}/file/add
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      data = {file_id: file_id}
      response = requests.post(url, headers=headers, json=data)
      return response.json()
  ```

#### Usando Arquivos e Cole√ß√µes em Conclus√µes de Conversa

Voc√™ pode referenciar tanto arquivos individuais quanto cole√ß√µes inteiras em suas consultas RAG para respostas enriquecidas.

##### Usando um Arquivo Individual em Conclus√µes de Conversa

Este m√©todo √© ben√©fico quando voc√™ deseja focar a resposta do modelo de chat no conte√∫do de um arquivo espec√≠fico.

- **Endpoint**: `POST /api/chat/completions`
- **Exemplo Curl**:

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "gpt-4-turbo",
        "messages": [
          {"role": "user", "content": "Explique os conceitos neste documento."}
        ],
        "files": [
          {"type": "file", "id": "your-file-id-here"}
        ]
      }
  ```

- **Exemplo Python**:

  ```python
  import requests

  def chat_with_file(token, model, query, file_id):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      payload = {
          model: model,
          messages: [{role: user, content: query}],
          files: [{type: file, id: file_id}]
      }
      response = requests.post(url, headers=headers, json=payload)
      return response.json()
  ```

##### Usando uma Cole√ß√£o de Conhecimento em Conclus√µes de Conversa

Aproveite uma cole√ß√£o de conhecimento para melhorar a resposta quando a consulta pode se beneficiar de um contexto mais amplo ou de v√°rios documentos.

- **Endpoint**: `POST /api/chat/completions`
- **Exemplo Curl**:

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "gpt-4-turbo",
        "messages": [
          {"role": "user", "content": "Forne√ßa insights sobre as perspectivas hist√≥ricas abordadas na cole√ß√£o."}
        ],
        "files": [
          {"type": "collection", "id": "your-collection-id-here"}
        ]
      }
  ```

- **Exemplo Python**:

  ```python
  import requests
  
  def chat_with_collection(token, model, query, collection_id):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      payload = {
          model: model,
          messages: [{role: user, content: query}],
          files: [{type: collection, id: collection_id}]
      }
      response = requests.post(url, headers=headers, json=payload)
      return response.json()
  ```

Esses m√©todos permitem uma utiliza√ß√£o eficaz de conhecimento externo via arquivos enviados e cole√ß√µes de conhecimento organizadas, melhorando as capacidades das aplica√ß√µes de chat utilizando a API Open WebUI. Esteja usando arquivos individualmente ou dentro de cole√ß√µes, voc√™ pode personalizar a integra√ß√£o com base em suas necessidades espec√≠ficas.

## Vantagens de Usar o Open WebUI como um Provedor LLM Unificado

O Open WebUI oferece uma variedade de benef√≠cios, tornando-se uma ferramenta essencial para desenvolvedores e empresas:

- **Interface Unificada**: Simplifique suas intera√ß√µes com diferentes LLMs por meio de uma √∫nica plataforma integrada.
- **Facilidade de Implementa√ß√£o**: In√≠cio r√°pido de integra√ß√£o com documenta√ß√£o abrangente e suporte da comunidade.

## Links da Documenta√ß√£o Swagger

:::important
Certifique-se de definir a vari√°vel de ambiente `ENV` como `dev` para acessar a documenta√ß√£o Swagger de qualquer um desses servi√ßos. Sem essa configura√ß√£o, a documenta√ß√£o n√£o estar√° dispon√≠vel.
:::

Acesse uma documenta√ß√£o detalhada da API para diferentes servi√ßos fornecidos pelo Open WebUI:

| Aplica√ß√£o   | Caminho da Documenta√ß√£o |
|-------------|-------------------------|
| Principal   | `/docs`                 |


Seguindo estas diretrizes, voc√™ pode integrar rapidamente e come√ßar a utilizar a API Open WebUI. Caso encontre algum problema ou tenha quest√µes, sinta-se √† vontade para entrar em contato atrav√©s da nossa Comunidade Discord ou consultar as FAQs. Feliz codifica√ß√£o! üåü
