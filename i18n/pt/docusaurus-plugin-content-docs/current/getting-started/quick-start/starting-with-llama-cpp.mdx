---
sidebar_position: 3
title: "ü¶ôCome√ßando com Llama.cpp"
---

## Vis√£o Geral

Open WebUI torna simples e flex√≠vel conectar e gerenciar um servidor local Llama.cpp para executar modelos de linguagem eficientes e quantificados. Seja voc√™ compilando Llama.cpp por conta pr√≥pria ou usando bin√°rios pr√©-compilados, este guia ir√° orient√°-lo sobre como:

- Configurar seu servidor Llama.cpp
- Carregar modelos grandes localmente
- Integrar com Open WebUI para uma interface perfeita

Vamos come√ßar!

---

## Etapa 1: Instalar o Llama.cpp

Para executar modelos com Llama.cpp, voc√™ primeiro precisa instalar o servidor Llama.cpp localmente.

Voc√™ pode:

- üì¶ [Baixar bin√°rios pr√©-compilados](https://github.com/ggerganov/llama.cpp/releases)
- üõ†Ô∏è Ou compil√°-lo a partir do c√≥digo-fonte seguindo as [instru√ß√µes oficiais de compila√ß√£o](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)

Ap√≥s a instala√ß√£o, certifique-se de que `llama-server` est√° dispon√≠vel no caminho local do sistema ou anote sua localiza√ß√£o.

---

## Etapa 2: Baixar um Modelo Suportado

Voc√™ pode carregar e executar v√°rios LLMs quantificados no formato GGUF usando Llama.cpp. Um exemplo impressionante √© o modelo DeepSeek-R1 1.58-bit otimizado pela UnslothAI. Para baixar esta vers√£o:

1. Visite o [reposit√≥rio Unsloth DeepSeek-R1 no Hugging Face](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)
2. Baixe a vers√£o quantificada de 1.58-bit ‚Äì cerca de 131GB.

Alternativamente, use Python para baixar programaticamente:

```python
# pip install huggingface_hub hf_transfer

from huggingface_hub import snapshot_download

snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",
    local_dir = "DeepSeek-R1-GGUF",
    allow_patterns = ["*UD-IQ1_S*"],  # Baixar apenas a variante de 1.58-bit
)
```

Isso ir√° baixar os arquivos do modelo para um diret√≥rio como:
```
DeepSeek-R1-GGUF/
‚îî‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/
    ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
    ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf
    ‚îî‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf
```

üìç Anote o caminho completo para o primeiro arquivo GGUF ‚Äî voc√™ precisar√° dele na Etapa 3.

---

## Etapa 3: Servir o Modelo com Llama.cpp

Inicie o servidor de modelos usando o bin√°rio llama-server. Navegue at√© sua pasta do Llama.cpp (por exemplo, build/bin) e execute:

```bash
./llama-server \
  --model /seu/caminho/completo/para/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
  --port 10000 \
  --ctx-size 1024 \
  --n-gpu-layers 40
```

üõ†Ô∏è Ajuste os par√¢metros para atender √†s necessidades de sua m√°quina:

- --model: Caminho para o arquivo de modelo .gguf
- --port: 10000 (ou escolha outra porta aberta)
- --ctx-size: Comprimento do contexto de token (pode aumentar se tiver RAM suficiente)
- --n-gpu-layers: Camadas descarregadas para GPU para desempenho mais r√°pido

Uma vez que o servidor esteja em execu√ß√£o, ele expor√° uma API local compat√≠vel com OpenAI em:

```
http://127.0.0.1:10000
```

---

## Etapa 4: Conectar Llama.cpp ao Open WebUI

Para controlar e consultar seu modelo em execu√ß√£o localmente diretamente do Open WebUI:

1. Abra o Open WebUI no seu navegador
2. V√° para ‚öôÔ∏è Configura√ß√µes Administrativas ‚Üí Conex√µes ‚Üí Conex√µes OpenAI
3. Clique em ‚ûï Adicionar Conex√£o e insira:

- URL: `http://127.0.0.1:10000/v1`  
  (Ou use `http://host.docker.internal:10000/v1` se estiver executando o WebUI dentro do Docker)
- API Key: `none` (deixe em branco)

üí° Uma vez salvo, o Open WebUI come√ßar√° a usar seu servidor local Llama.cpp como backend!

![Conex√£o Llama.cpp no Open WebUI](/images/tutorials/deepseek/connection.png)

---

## Dica R√°pida: Experimente o Modelo atrav√©s da Interface de Chat

Uma vez conectado, selecione o modelo no menu de chat do Open WebUI e comece a interagir!

![Pr√©via do Chat do Modelo](/images/tutorials/deepseek/response.png)

---

## Voc√™ Est√° Pronto para Come√ßar!

Depois de configurado, o Open WebUI facilita:

- Gerenciar e alternar entre modelos locais servidos pelo Llama.cpp
- Usar a API compat√≠vel com OpenAI sem precisar de uma chave
- Experimentar modelos massivos como o DeepSeek-R1 ‚Äî diretamente do seu computador!

---

üöÄ Divirta-se experimentando e construindo!