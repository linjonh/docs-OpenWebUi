---

sidebar_position: 4  
title: "üåê Come√ßando com Servidores Compat√≠veis com OpenAI"

---

## Vis√£o Geral

Open WebUI n√£o √© apenas para OpenAI/Ollama/Llama.cpp‚Äîvoc√™ pode conectar **qualquer servidor que implemente a API compat√≠vel com OpenAI**, rodando localmente ou remotamente. Isso √© perfeito caso voc√™ queira usar diferentes modelos de linguagem ou j√° tenha um backend ou ecossistema favorito. Este guia mostrar√° como:

- Configurar um servidor compat√≠vel com OpenAI (com algumas op√ß√µes populares)
- Conect√°-lo ao Open WebUI
- Come√ßar a conversar imediatamente

## Etapa 1: Escolher um Servidor Compat√≠vel com OpenAI

H√° muitos servidores e ferramentas que exp√µem uma API compat√≠vel com OpenAI. Aqui est√£o alguns dos mais populares:

- [Llama.cpp](https://github.com/ggml-org/llama.cpp): Extremamente eficiente, funciona em CPU e GPU
- [Ollama](https://ollama.com/): Super amig√°vel e multiplataforma
- [LM Studio](https://lmstudio.ai/): Aplicativo desktop completo para Windows/Mac/Linux
- [Lemonade (ONNX TurnkeyML)](https://github.com/onnx/turnkeyml): Backend r√°pido baseado em ONNX com acelera√ß√£o por NPU/iGPU

Escolha o que melhor se adapta ao seu fluxo de trabalho!

---

#### üçã Come√ßando com Lemonade (ONNX TurnkeyML)

Lemonade √© um servidor compat√≠vel com OpenAI baseado em ONNX, pronto para uso. Veja como experiment√°-lo no Windows:

1. [Baixe o √∫ltimo `.exe`](https://github.com/onnx/turnkeyml/releases)
2. Execute `Lemonade_Server_Installer.exe`
3. Instale e baixe um modelo usando o instalador do Lemonade
4. Uma vez em execu√ß√£o, o endpoint da API ser√°:

   ```
   http://localhost:8000/api/v0
   ```

![Servidor Lemonade](/images/getting-started/lemonade-server.png)

Consulte [a documenta√ß√£o](https://github.com/onnx/turnkeyml) para mais detalhes.

---

## Etapa 2: Conecte seu Servidor ao Open WebUI

1. Abra o Open WebUI no seu navegador.
2. V√° para ‚öôÔ∏è **Configura√ß√µes de Administra√ß√£o** ‚Üí **Conex√µes** ‚Üí **Conex√µes OpenAI**.
3. Clique em ‚ûï **Adicionar Conex√£o**.

   - **URL**: Use o endpoint da API do seu servidor (por exemplo, `http://localhost:11434/v1` para Ollama, ou o endere√ßo do seu pr√≥prio servidor Llama.cpp).
   - **Chave API**: Deixe em branco, a menos que seja necess√°rio.

4. Clique em Salvar.

*Dica: Caso esteja executando Open WebUI no Docker e seu servidor de modelo na m√°quina host, use `http://host.docker.internal:<sua-porta>/v1`.*

##### **Para Lemonade:** Ao adicionar o Lemonade, use `http://localhost:8000/api/v0` como o URL.

![Conex√£o Lemonade](/images/getting-started/lemonade-connection.png)

---

## Etapa 3: Comece a Conversar!

Selecione o modelo do seu servidor conectado no menu de bate-papo e comece!

√â isso a√≠! Seja usando Llama.cpp, Ollama, LM Studio ou Lemonade, voc√™ pode experimentar e gerenciar facilmente v√°rios servidores de modelos‚Äîtudo no Open WebUI.

--- 

üöÄ Aproveite para criar sua configura√ß√£o de IA local perfeita!