---
sidebar_position: 1
title: "ğŸ‹ Execute DeepSeek R1 Dynamic 1.58-bit com Llama.cpp"
---

Um grande agradecimento ao **UnslothAI** por seus incrÃ­veis esforÃ§os! GraÃ§as ao seu trabalho Ã¡rduo, agora podemos executar o modelo completo **DeepSeek-R1** com 671 bilhÃµes de parÃ¢metros em sua forma quantizada dinÃ¢mica de 1,58 bits (comprimido para apenas 131 GB) no **Llama.cpp**! E o melhor de tudo? VocÃª nÃ£o precisa mais se preocupar em precisar de GPUs ou servidores corporativos robustos â€” Ã© possÃ­vel executar este modelo em sua mÃ¡quina pessoal (embora lentamente na maioria do hardware de consumo).

:::note
O Ãºnico modelo verdadeiro **DeepSeek-R1** no Ollama Ã© a versÃ£o **671B** disponÃ­vel aqui: [https://ollama.com/library/deepseek-r1:671b](https://ollama.com/library/deepseek-r1:671b). Outras versÃµes sÃ£o modelos **destilados**.
:::

Este guia foca em executar o **modelo completo DeepSeek-R1 Dynamic 1.58-bit quantizado** utilizando **Llama.cpp** integrado com **Open WebUI**. Para este tutorial, demonstraremos os passos com uma mÃ¡quina **M4 Max + 128GB RAM**. VocÃª pode adaptar as configuraÃ§Ãµes para o seu prÃ³prio equipamento.

---

## Passo 1: Instalar o Llama.cpp

VocÃª pode escolher entre:
- [Baixar os binÃ¡rios prÃ©-compilados](https://github.com/ggerganov/llama.cpp/releases)
- **Ou compilar vocÃª mesmo**: Siga as instruÃ§Ãµes aqui: [Guia de ConstruÃ§Ã£o do Llama.cpp](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)

## Passo 2: Baixar o Modelo Fornecido pelo UnslothAI

Acesse a [pÃ¡gina do Unsloth no Hugging Face](https://huggingface.co/unsloth/DeepSeek-R1-GGUF) e baixe a **versÃ£o quantizada dinÃ¢mica** apropriada do DeepSeek-R1. Para este tutorial, usaremos a versÃ£o **1,58-bit (131GB)**, que Ã© altamente otimizada e surpreendentemente funcional.


:::tip
ConheÃ§a o seu "diretÃ³rio de trabalho" â€” onde seu script Python ou sessÃ£o de terminal estÃ¡ sendo executado. Os arquivos do modelo serÃ£o baixados para uma subpasta desse diretÃ³rio por padrÃ£o, entÃ£o tenha certeza de conhecer o caminho! Por exemplo, se vocÃª executar o comando abaixo em `/Users/yourname/Documents/projects`, seu modelo baixado serÃ¡ salvo em `/Users/yourname/Documents/projects/DeepSeek-R1-GGUF`.
:::

Para entender mais sobre o processo de desenvolvimento do UnslothAI e por que essas versÃµes quantizadas dinÃ¢micas sÃ£o tÃ£o eficientes, confira o post no blog: [UnslothAI DeepSeek R1 Dynamic Quantization](https://unsloth.ai/blog/deepseekr1-dynamic).

Aqui estÃ¡ como baixar o modelo programaticamente:
```python
# Instale as dependÃªncias do Hugging Face antes de executar isto:
# pip install huggingface_hub hf_transfer

from huggingface_hub import snapshot_download

snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",  # Especifique o repositÃ³rio no Hugging Face
    local_dir = "DeepSeek-R1-GGUF",         # O modelo serÃ¡ baixado neste diretÃ³rio
    allow_patterns = ["*UD-IQ1_S*"],        # Baixe apenas a versÃ£o 1,58-bit
)
```

Uma vez que o download estiver completo, vocÃª encontrarÃ¡ os arquivos do modelo em uma estrutura de diretÃ³rios como esta:
```
DeepSeek-R1-GGUF/
â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S/
â”‚   â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
â”‚   â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf
â”‚   â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf
```

:::info
ğŸ› ï¸ Atualize os caminhos nos prÃ³ximos passos para **corresponder Ã  sua estrutura de diretÃ³rios especÃ­fica**. Por exemplo, se seu script estava em `/Users/tim/Downloads`, o caminho completo para o arquivo GGUF seria: 
`/Users/tim/Downloads/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`.
:::

## Passo 3: Certifique-se de que o Open WebUI estÃ¡ instalado e em execuÃ§Ã£o

Se vocÃª ainda nÃ£o tem o **Open WebUI** instalado, sem problemas! Ã‰ uma configuraÃ§Ã£o simples. Basta seguir a [documentaÃ§Ã£o do Open WebUI aqui](https://docs.openwebui.com/). Depois de instalado, inicie o aplicativo â€” conectaremos isso em um passo posterior para interagir com o modelo DeepSeek-R1.


## Passo 4: Servir o Modelo Usando Llama.cpp

Agora que o modelo foi baixado, o prÃ³ximo passo Ã© executÃ¡-lo usando o **modo servidor do Llama.cpp**. Antes de comeÃ§ar:

1. **Localize o binÃ¡rio `llama-server`.**
   Se vocÃª compilou a partir do cÃ³digo-fonte (conforme descrito no Passo 1), o executÃ¡vel `llama-server` estarÃ¡ localizado em `llama.cpp/build/bin`. Navegue atÃ© este diretÃ³rio usando o comando `cd`:
   ```bash
   cd [caminho-para-llama-cpp]/llama.cpp/build/bin
   ```

   Substitua `[caminho-para-llama-cpp]` pelo local onde vocÃª clonou ou compilou o Llama.cpp. Por exemplo:
   ```bash
   cd ~/Documents/workspace/llama.cpp/build/bin
   ```

2. **Aponte para a pasta do seu modelo.**
   Use o caminho completo para os arquivos GGUF baixados no Passo 2. Ao servir o modelo, especifique a primeira parte dos arquivos GGUF divididos (por exemplo, `DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`).

Aqui estÃ¡ o comando para iniciar o servidor:
```bash
./llama-server \
    --model /[seu-diretorio]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --port 10000 \
    --ctx-size 1024 \
    --n-gpu-layers 40
```


:::tip
ğŸ”‘ **ParÃ¢metros para personalizar com base na sua mÃ¡quina:**

- **`--model`:** Substitua `/[your-directory]/` pelo caminho onde os arquivos GGUF foram baixados na Etapa 2.
- **`--port`:** O padrÃ£o do servidor Ã© `8080`, mas sinta-se Ã  vontade para alterÃ¡-lo com base na disponibilidade de portas.
- **`--ctx-size`:** Determina o comprimento do contexto (nÃºmero de tokens). VocÃª pode aumentar se seu hardware permitir, mas tenha cuidado com o aumento no uso de RAM/VRAM.
- **`--n-gpu-layers`:** Defina o nÃºmero de camadas que deseja transferir para sua GPU para uma inferÃªncia mais rÃ¡pida. O nÃºmero exato depende da capacidade de memÃ³ria da sua GPU â€” consulte a tabela da Unsloth para recomendaÃ§Ãµes especÃ­ficas.
:::

Por exemplo, se seu modelo foi baixado para `/Users/tim/Documents/workspace`, o comando seria assim:
```bash
./llama-server \
    --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --port 10000 \
    --ctx-size 1024 \
    --n-gpu-layers 40
```

Assim que o servidor iniciar, ele hospedarÃ¡ um **endpoint de API local compatÃ­vel com OpenAI** em:
```
http://127.0.0.1:10000
```

:::info
ğŸ–¥ï¸ **Servidor Llama.cpp em execuÃ§Ã£o**

![Captura de tela do servidor](/images/tutorials/deepseek/serve.png)

ApÃ³s executar o comando, vocÃª deve ver uma mensagem confirmando que o servidor estÃ¡ ativo e ouvindo na porta 10000.
:::

Certifique-se de **manter esta sessÃ£o do terminal aberta**, pois ela serve o modelo para todas as etapas subsequentes.

## Etapa 5: Conectar Llama.cpp ao Open WebUI

1. VÃ¡ para **Admin Settings** no Open WebUI.
2. Acesse **Connections > OpenAI Connections.**
3. Adicione os seguintes detalhes para a nova conexÃ£o:
   - URL: `http://127.0.0.1:10000/v1` (ou `http://host.docker.internal:10000/v1` ao executar o Open WebUI no Docker)
   - Chave de API: `none`

:::info
ğŸ–¥ï¸ **Adicionando conexÃ£o no Open WebUI**

![Captura de tela de conexÃ£o](/images/tutorials/deepseek/connection.png)

ApÃ³s executar o comando, vocÃª deve ver uma mensagem confirmando que o servidor estÃ¡ ativo e ouvindo na porta 10000.
:::

Depois de salvar a conexÃ£o, vocÃª pode comeÃ§ar a consultar **DeepSeek-R1** diretamente do Open WebUI! ğŸ‰

---

## Exemplo: Gerando Respostas

Agora vocÃª pode usar a interface de chat do Open WebUI para interagir com o **modelo DeepSeek-R1 Dynamic 1.58-bit**.

![Captura de tela da resposta](/images/tutorials/deepseek/response.png)

---

## Notas e ConsideraÃ§Ãµes

- **Desempenho:**
  Executar um modelo massivo de 131GB como o DeepSeek-R1 em hardware pessoal serÃ¡ **lento**. Mesmo com nosso M4 Max (128GB RAM), as velocidades de inferÃªncia foram modestas. Mas o fato de funcionar Ã© um testemunho das otimizaÃ§Ãµes da UnslothAI.

- **Requisitos de VRAM/MemÃ³ria:**
  Certifique-se de ter VRAM e RAM do sistema suficientes para um desempenho Ã³timo. Com GPUs de baixa capacidade ou configuraÃ§Ãµes apenas com CPU, espere velocidades mais lentas (mas ainda Ã© possÃ­vel!).

---

GraÃ§as Ã  **UnslothAI** e **Llama.cpp**, rodar um dos maiores modelos de raciocÃ­nio de cÃ³digo aberto, **DeepSeek-R1** (versÃ£o 1.58-bit), estÃ¡ finalmente acessÃ­vel para indivÃ­duos. Embora seja desafiador executar tais modelos em hardware consumidor, a capacidade de fazÃª-lo sem uma infraestrutura computacional massiva Ã© um marco tecnolÃ³gico significativo.

â­ Um grande agradecimento Ã  comunidade por ampliar os limites da pesquisa em IA aberta.

Boas experimentaÃ§Ãµes! ğŸš€
