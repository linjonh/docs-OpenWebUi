---
sidebar_position: 11
title: "üñ•Ô∏è Configura√ß√£o Local de LLM com IPEX-LLM na GPU Intel"
---

:::warning
Este tutorial √© uma contribui√ß√£o da comunidade e n√£o √© suportado pela equipe Open WebUI. Ele serve apenas como uma demonstra√ß√£o de como personalizar o Open WebUI para seu caso de uso espec√≠fico. Deseja contribuir? Confira o tutorial de contribui√ß√£o.
:::

:::note
Este guia foi verificado com a configura√ß√£o do Open WebUI feita por meio da [Instala√ß√£o Manual](/getting-started/index.md).
:::

# Configura√ß√£o Local de LLM com IPEX-LLM na GPU Intel

:::info
[**IPEX-LLM**](https://github.com/intel-analytics/ipex-llm) √© uma biblioteca PyTorch para executar LLM em CPUs e GPUs Intel (por exemplo, PC local com iGPU, GPU discreta como Arc A-Series, Flex e Max) com lat√™ncia muito baixa.
:::

Este tutorial demonstra como configurar o Open WebUI com o **backend Ollama acelerado por IPEX-LLM hospedado na GPU Intel**. Ao seguir este guia, voc√™ poder√° configurar o Open WebUI mesmo em um PC de baixo custo (ou seja, apenas com GPU integrada) com uma experi√™ncia suave.

## Iniciar o Ollama Serve na GPU Intel

Consulte [este guia](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html) na documenta√ß√£o oficial do IPEX-LLM sobre como instalar e executar o Ollama serve acelerado pelo IPEX-LLM na GPU Intel.

:::tip
Se voc√™ desejar acessar o servi√ßo Ollama a partir de outra m√°quina, certifique-se de definir ou exportar a vari√°vel de ambiente `OLLAMA_HOST=0.0.0.0` antes de executar o comando `ollama serve`.
:::

## Configurar o Open WebUI

Acesse as configura√ß√µes Ollama atrav√©s de **Configura√ß√µes -> Conex√µes** no menu. Por padr√£o, a **URL Base do Ollama** √© pr√©-configurada como `https://localhost:11434`, conforme ilustrado na imagem abaixo. Para verificar o status da conex√£o com o servi√ßo Ollama, clique no **bot√£o Atualizar** localizado ao lado da caixa de texto. Se o WebUI n√£o for capaz de estabelecer conex√£o com o servidor Ollama, voc√™ ver√° uma mensagem de erro informando, `WebUI n√£o conseguiu conectar-se ao Ollama`.

![Falha na Configura√ß√£o do Ollama no Open WebUI](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings_0.png)

Se a conex√£o for bem-sucedida, voc√™ ver√° uma mensagem informando `Conex√£o com o Servi√ßo Verificada`, conforme ilustrado abaixo.

![Sucesso na Configura√ß√£o do Ollama no Open WebUI](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings.png)

:::tip
Se voc√™ quiser usar um servidor Ollama hospedado em uma URL diferente, basta atualizar a **URL Base do Ollama** para o novo endere√ßo e pressionar o **bot√£o Atualizar** para re-confirmar a conex√£o com o Ollama.
:::
