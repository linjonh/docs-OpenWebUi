---
sidebar_position: 2
title: "üó®Ô∏è Kokoro-FastAPI Usando Docker"
---

:::warning
Este tutorial √© uma contribui√ß√£o da comunidade e n√£o √© suportado pela equipe do Open WebUI. Ele serve apenas como uma demonstra√ß√£o de como personalizar o Open WebUI para seu caso de uso espec√≠fico. Quer contribuir? Confira o tutorial de contribui√ß√£o.
:::

## O que √© `Kokoro-FastAPI`?

[Kokoro-FastAPI](https://github.com/remsky/Kokoro-FastAPI) √© um wrapper FastAPI dockerizado para o modelo de texto-para-fala [Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M) que implementa a especifica√ß√£o do endpoint da API OpenAI. Ele oferece texto-para-fala de alto desempenho com velocidades de gera√ß√£o impressionantes.

## Principais Funcionalidades

- Endpoint de fala compat√≠vel com OpenAI com combina√ß√£o de voz inline
- Infer√™ncia acelerada por GPU NVIDIA ou Onnx em CPU
- Suporte a streaming com fragmenta√ß√£o vari√°vel
- Suporte a v√°rios formatos de √°udio (`.mp3`, `.wav`, `.opus`, `.flac`, `.aac`, `.pcm`)
- Interface web integrada em localhost:8880/web (ou container adicional no reposit√≥rio para gradio)
- Endpoints de fonema para convers√£o e gera√ß√£o

## Vozes

- af
- af_bella
- af_irulan
- af_nicole
- af_sarah
- af_sky
- am_adam
- am_michael
- am_gurney
- bf_emma
- bf_isabella
- bm_george
- bm_lewis

## Idiomas

- en_us
- en_uk

## Requisitos

- Docker instalado no seu sistema
- Open WebUI em execu√ß√£o
- Para suporte a GPU: GPU NVIDIA com CUDA 12.3
- Para apenas CPU: Nenhum requisito especial

## ‚ö°Ô∏è In√≠cio R√°pido

### Voc√™ pode escolher entre vers√µes para GPU ou CPU

### Vers√£o GPU (Requer GPU NVIDIA com CUDA 12.8)

Usando docker run:

```bash
docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu
```

Ou docker compose, criando um arquivo `docker-compose.yml` e executando `docker compose up`. Por exemplo:

```yaml
name: kokoro
services:
    kokoro-fastapi-gpu:
        ports:
            - 8880:8880
        image: ghcr.io/remsky/kokoro-fastapi-gpu:v0.2.1
        restart: always
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
```

:::info
Voc√™ pode precisar instalar e configurar [o NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
:::

### Vers√£o CPU (Infer√™ncia otimizada para ONNX)

Com docker run:

```bash
docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu
```

Com docker compose:

```yaml
name: kokoro
services:
    kokoro-fastapi-cpu:
        ports:
            - 8880:8880
        image: ghcr.io/remsky/kokoro-fastapi-cpu
        restart: always
```

## Configurando o Open WebUI para usar o `Kokoro-FastAPI`

Para usar o Kokoro-FastAPI com o Open WebUI, siga estas etapas:

- Abra o Painel Administrativo e v√° para `Configura√ß√µes` -> `√Åudio`
- Defina suas configura√ß√µes de TTS para corresponder ao seguinte:
- - Motor de Texto-para-Fala: OpenAI
  - URL Base da API: `http://localhost:8880/v1` # voc√™ pode precisar usar `host.docker.internal` em vez de `localhost`
  - Chave de API: `not-needed`
  - Modelo de TTS: `kokoro`
  - Voz de TTS: `af_bella` # tamb√©m aceita mapeamento de vozes existentes do OAI para compatibilidade

:::info
A chave de API padr√£o √© a string `not-needed`. Voc√™ n√£o precisa alterar esse valor caso n√£o precise de seguran√ßa adicional.
:::

## Construindo o Cont√™iner Docker

```bash
git clone https://github.com/remsky/Kokoro-FastAPI.git
cd Kokoro-FastAPI
cd docker/cpu # ou docker/gpu
docker compose up --build
```

**√â isso!**

Para mais informa√ß√µes sobre como construir o cont√™iner Docker, incluindo altera√ß√£o de portas, consulte o reposit√≥rio [Kokoro-FastAPI](https://github.com/remsky/Kokoro-FastAPI)
