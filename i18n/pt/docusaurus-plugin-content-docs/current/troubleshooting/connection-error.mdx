---
sidebar_position: 0
title: "ğŸš§ Problemas de Conectividade do Servidor"
---

Estamos aqui para ajudar vocÃª a configurar tudo e garantir que funcione perfeitamente. Abaixo, vocÃª encontrarÃ¡ instruÃ§Ãµes passo a passo adaptadas para diferentes cenÃ¡rios para resolver problemas comuns de conexÃ£o com Ollama e servidores externos como Hugging Face.

## ğŸŒŸ ConexÃ£o com o Servidor Ollama

### ğŸš€ Acessando Ollama a partir do Open WebUI

Tem dificuldade para se conectar ao Ollama a partir do Open WebUI? Isso pode ocorrer porque o Ollama nÃ£o estÃ¡ escutando em uma interface de rede que permite conexÃµes externas. Vamos resolver isso:

1. **Configure Ollama para Escutar Amplamente** ğŸ§:
   Defina `OLLAMA_HOST` como `0.0.0.0` para fazer o Ollama escutar em todas as interfaces de rede.

2. **Atualize as VariÃ¡veis de Ambiente**:
   Certifique-se de que `OLLAMA_HOST` esteja definido corretamente no seu ambiente de implantaÃ§Ã£o.

3. **Reinicie o Ollama** ğŸ”„:
   Ã‰ necessÃ¡rio reiniciar para que as alteraÃ§Ãµes entrem em vigor.

ğŸ’¡ ApÃ³s a configuraÃ§Ã£o, verifique se o Ollama Ã© acessÃ­vel visitando a interface do WebUI.

Para mais instruÃ§Ãµes detalhadas sobre como configurar o Ollama, consulte a [DocumentaÃ§Ã£o Oficial do Ollama](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux).

### ğŸ³ Erro de ConexÃ£o com Docker

Se vocÃª estiver vendo um erro de conexÃ£o ao tentar acessar o Ollama, pode ser porque o container Docker do WebUI nÃ£o consegue se comunicar com o servidor Ollama que estÃ¡ sendo executado no seu host. Vamos corrigir isso:

1. **Ajuste as ConfiguraÃ§Ãµes de Rede** ğŸ› ï¸:
   Use a opÃ§Ã£o `--network=host` no seu comando do Docker. Isso conecta diretamente seu container Ã  rede do host.

2. **Altere a Porta**:
   Lembre-se de que a porta interna muda de 3000 para 8080.

**Comando Exemplo do Docker**:
```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
ğŸ”— ApÃ³s executar o comando acima, seu WebUI deve estar disponÃ­vel em `http://localhost:8080`.

## ğŸ”’ Problema de ConexÃ£o SSL com Hugging Face

Encontrou um erro de SSL? Isso pode ser um problema com o servidor do Hugging Face. Veja o que fazer:

1. **Verifique o Status do Servidor Hugging Face**:
   Verifique se hÃ¡ alguma interrupÃ§Ã£o ou problema conhecido no servidor deles.

2. **Troque o Endpoint**:
   Se o Hugging Face estiver fora do ar, altere o endpoint no seu comando do Docker.

**Comando Exemplo do Docker para Problemas de ConexÃ£o**:
```bash
docker run -d -p 3000:8080 -e HF_ENDPOINT=https://hf-mirror.com/ --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

## ğŸ Podman no MacOS

Executando no MacOS com Podman? Veja como garantir a conectividade:

1. **Habilite o Loopback do Host**:
   Use `--network slirp4netns:allow_host_loopback=true` no seu comando.

2. **Defina OLLAMA_BASE_URL**:
   Certifique-se de que ele aponte para `http://host.containers.internal:11434`.

**Comando Exemplo do Podman**:
```bash
podman run -d --network slirp4netns:allow_host_loopback=true -p 3000:8080 -e OLLAMA_BASE_URL=http://host.containers.internal:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

