---
sidebar_position: 3
title: "ğŸ§  SoluÃ§Ã£o de Problemas RAG (GeraÃ§Ã£o Aumentada com RecuperaÃ§Ã£o)"
---

A GeraÃ§Ã£o Aumentada com RecuperaÃ§Ã£o (RAG) permite que modelos de linguagem processem conteÃºdo externo â€” documentos, bases de conhecimento, e mais â€” recuperando informaÃ§Ãµes relevantes e alimentando-as no modelo. Mas quando as coisas nÃ£o funcionam como esperado (por exemplo, o modelo "alucina" ou perde informaÃ§Ãµes relevantes), muitas vezes o problema nÃ£o Ã© do modelo â€” Ã© uma questÃ£o de contexto.

Vamos analisar as causas mais comuns e suas soluÃ§Ãµes para melhorar a precisÃ£o da RAG! ğŸš€

## Problemas Comuns na RAG e Como Corrigi-los ğŸ› ï¸

### 1. O Modelo "NÃ£o Consegue Ver" Seu ConteÃºdo ğŸ‘ï¸âŒ

Este Ã© o problema mais comum â€” e geralmente Ã© causado por questÃµes no processo de ingestÃ£o de conteÃºdo. O modelo nÃ£o alucina porque estÃ¡ errado, ele alucina porque nunca recebeu o conteÃºdo correto em primeiro lugar.

âœ… SoluÃ§Ã£o: Verifique suas configuraÃ§Ãµes de extraÃ§Ã£o de conteÃºdo

- Navegue para: **ConfiguraÃ§Ãµes Administrativas > Documentos**.
- Certifique-se de que vocÃª estÃ¡ utilizando um mecanismo de extraÃ§Ã£o de conteÃºdo robusto, como:
  - Apache Tika
  - Docling
  - Extractores personalizados (dependendo dos tipos de documentos)

ğŸ“Œ Dica: Experimente fazer o upload de um documento e visualize o conteÃºdo extraÃ­do. Se estiver em branco ou faltando seÃ§Ãµes importantes, ajuste suas configuraÃ§Ãµes de extraÃ§Ã£o ou use um mecanismo diferente.

---

### 2. Apenas uma Pequena Parte do Documento EstÃ¡ Sendo Usada ğŸ“„â¡ï¸âœ‚ï¸

A Open WebUI Ã© projetada para trabalhar com modelos que possuem janelas de contexto limitadas por padrÃ£o. Por exemplo, muitos modelos locais (como os modelos padrÃ£o da Ollama) sÃ£o limitados a 2048 tokens. Por conta disso, a Open WebUI reduz agressivamente o conteÃºdo recuperado para caber dentro do espaÃ§o disponÃ­vel assumido.

âœ… SoluÃ§Ãµes:

- VÃ¡ para **ConfiguraÃ§Ãµes Administrativas > Documentos**
- Escolha uma das opÃ§Ãµes:
  - ğŸ’¡ Ative â€œIgnorar Embedding e RecuperaÃ§Ã£oâ€ â€” Isso envia o conteÃºdo completo diretamente, sem aplicar filtros de recuperaÃ§Ã£o rigorosos.
  - ğŸ” Ative â€œModo de Contexto Completoâ€ â€” Isso injeta conteÃºdo mais abrangente no prompt do modelo.

ğŸ“Œ Aviso: Tenha cuidado com os limites de contexto â€” se seu modelo nÃ£o puder lidar com mais tokens, ainda assim serÃ¡ cortado.

---

### 3. Limite de Tokens Muito Curto â³

Mesmo que a recuperaÃ§Ã£o funcione, seu modelo pode ainda nÃ£o processar todo o conteÃºdo recebido â€” porque simplesmente nÃ£o consegue.

Por padrÃ£o, muitos modelos (especialmente os LLMs hospedados pela Ollama) sÃ£o limitados a uma janela de contexto de 2048 tokens. Isso significa que apenas uma fraÃ§Ã£o dos dados recuperados serÃ¡ realmente usada.

âœ… SoluÃ§Ãµes:

- ğŸ› ï¸ Estenda o comprimento do contexto do modelo:
  - Navegue para **Editor de Modelos ou Controle de Chats**
  - Modifique o comprimento do contexto (por exemplo, aumente para 8192+ tokens, se suportado)

â„¹ï¸ Nota: O padrÃ£o de 2048 tokens Ã© uma grande limitaÃ§Ã£o. Para melhores resultados em RAG, recomendamos usar modelos que suportem contextos mais longos.

âœ… Alternativa: Use um LLM externo com maior capacidade de contexto

- Experimente GPT-4, GPT-4o, Claude 3, Gemini 1.5 ou Mixtral com mais de 8k de contexto
- Compare o desempenho com Ollama â€” note a diferenÃ§a de precisÃ£o quando mais conteÃºdo pode ser injetado!

ğŸ“Œ Dica: Prefira modelos externos para um melhor desempenho em RAG em casos de uso em produÃ§Ã£o.

---

### 4. Modelo de Embedding de Baixa Qualidade ou IncompatÃ­vel ğŸ“‰ğŸ§ 

Embeddings ruins = recuperaÃ§Ã£o ruim. Se a representaÃ§Ã£o vetorial do seu conteÃºdo for fraca, o mecanismo de recuperaÃ§Ã£o nÃ£o encontrarÃ¡ o conteÃºdo correto, independentemente de quÃ£o poderoso seja o seu LLM.

âœ… SoluÃ§Ã£o:

- Alterne para um modelo de embedding de alta qualidade (por exemplo, all-MiniLM-L6-v2, Instructor X ou embeddings da OpenAI)
- VÃ¡ para: **ConfiguraÃ§Ãµes Administrativas > Documentos**
- ApÃ³s alterar o modelo, certifique-se de:
  - â³ Reindexar todos os documentos existentes para que os novos embeddings tenham efeito.

ğŸ“Œ Lembre-se: A qualidade dos embeddings afeta diretamente o conteÃºdo que Ã© recuperado.

---

### 5. âŒ 400: Objeto NoneType nÃ£o possui atributo encode

Esse erro indica um modelo de embedding mal configurado ou ausente. Quando o Open WebUI tenta criar embeddings mas nÃ£o possui um modelo vÃ¡lido carregado, ele nÃ£o pode processar o texto â€” e o resultado Ã© esse erro crÃ­ptico.

ğŸ’¥ Causa:
- Seu modelo de embedding nÃ£o estÃ¡ configurado corretamente.
- Pode nÃ£o ter sido transferido completamente.
- Ou, se vocÃª estiver usando um modelo de embedding externo, ele pode nÃ£o estar acessÃ­vel.

âœ… SoluÃ§Ã£o:

- VÃ¡ para: **ConfiguraÃ§Ãµes Administrativas > Documentos > Modelo de Embedding**
- Salve o modelo de embedding novamente â€” mesmo que jÃ¡ esteja selecionado. Isso forÃ§a uma nova verificaÃ§Ã£o/download.
- Se estiver usando uma ferramenta de embedding remota/externa, certifique-se de que ela estÃ¡ executando e acessÃ­vel ao Open WebUI.

ğŸ“Œ Dica: Depois de corrigir a configuraÃ§Ã£o, tente re-embutir um documento e verifique se nenhum erro aparece nos logs.

---

## ğŸ§ª Dica Pro: Teste com GPT-4o ou GPT-4

Se vocÃª nÃ£o tem certeza se o problema Ã© na recuperaÃ§Ã£o, limites de tokens ou embeddings â€” tente usar temporariamente o GPT-4o (por exemplo, via API da OpenAI). Se os resultados repentinamente se tornarem mais precisos, Ã© um forte indicativo de que o limite de contexto do seu modelo local (2048 por padrÃ£o na Ollama) Ã© o gargalo.

- GPT-4o lida com entradas maiores (128k tokens!)
- Oferece um Ã³timo parÃ¢metro para avaliar a confiabilidade da RAG no seu sistema

---

## Checklist Resumido âœ…

| Problema | CorreÃ§Ã£o |
|---------|----------|
| ğŸ¤” Modelo nÃ£o consegue 'ver' o conteÃºdo | Verifique as configuraÃ§Ãµes do extrator de documentos |
| ğŸ§¹ Apenas parte do conteÃºdo foi usado | Ative o Modo de Contexto Completo ou Ignore a Embedding |
| â± Limitado pelo limite de 2048 tokens | Aumente o comprimento do contexto do modelo ou use um LLM de grande contexto |
| ğŸ“‰ RecuperaÃ§Ã£o imprecisa | Troque para um modelo de embedding melhor e reindexe |
| Ainda confuso? | Teste com GPT-4o e compare os resultados |

---

Ao otimizar essas Ã¡reasâ€”extraÃ§Ã£o, embedding, recuperaÃ§Ã£o e contexto do modeloâ€”vocÃª pode melhorar drasticamente a precisÃ£o com que seu LLM trabalha com seus documentos. NÃ£o deixe uma janela de 2048 tokens ou uma pipeline de recuperaÃ§Ã£o fraca limitar o poder da sua IA ğŸ¯.
