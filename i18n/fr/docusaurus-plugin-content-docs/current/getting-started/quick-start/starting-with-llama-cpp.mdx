---
sidebar_position: 3
title: "ü¶ôCommencer avec Llama.cpp"
---

## Vue d'ensemble

Open WebUI rend simple et flexible la connexion et la gestion d'un serveur local Llama.cpp pour ex√©cuter des mod√®les de langage efficaces et quantifi√©s. Que vous ayez compil√© Llama.cpp vous-m√™me ou que vous utilisiez des binaires pr√©compil√©s, ce guide vous expliquera comment :

- Configurer votre serveur Llama.cpp
- Charger des mod√®les volumineux localement
- Int√©grer avec Open WebUI pour une interface fluide

Commen√ßons !

---

## √âtape 1 : Installer Llama.cpp

Pour ex√©cuter des mod√®les avec Llama.cpp, vous devez d'abord installer localement le serveur Llama.cpp.

Vous pouvez soit :

- üì¶ [T√©l√©charger des binaires pr√©compil√©s](https://github.com/ggerganov/llama.cpp/releases)
- üõ†Ô∏è Ou le compiler √† partir du code source en suivant les [instructions officielles de compilation](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)

Apr√®s l'installation, assurez-vous que `llama-server` est disponible dans le chemin syst√®me local ou notez son emplacement.

---

## √âtape 2 : T√©l√©charger un mod√®le pris en charge

Vous pouvez charger et ex√©cuter divers mod√®les LLM quantifi√©s au format GGUF avec Llama.cpp. Un exemple impressionnant est le mod√®le DeepSeek-R1 1,58 bits optimis√© par UnslothAI. Pour t√©l√©charger cette version :

1. Rendez-vous sur le [d√©p√¥t DeepSeek-R1 d'Unsloth sur Hugging Face](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)
2. T√©l√©chargez la version quantifi√©e 1,58 bits ‚Äì environ 131GB.

Sinon, utilisez Python pour t√©l√©charger de mani√®re programm√©e :

```python
# pip install huggingface_hub hf_transfer

from huggingface_hub import snapshot_download

snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",
    local_dir = "DeepSeek-R1-GGUF",
    allow_patterns = ["*UD-IQ1_S*"],  # T√©l√©charger uniquement la variante 1,58 bits
)
```

Cela t√©l√©chargera les fichiers du mod√®le dans un r√©pertoire comme :
```
DeepSeek-R1-GGUF/
‚îî‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/
    ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
    ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf
    ‚îî‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf
```

üìç Notez le chemin complet vers le premier fichier GGUF ‚Äî vous en aurez besoin √† l'√©tape 3.

---

## √âtape 3 : Servir le mod√®le avec Llama.cpp

D√©marrez le serveur de mod√®le √† l'aide du binaire llama-server. Naviguez vers votre dossier llama.cpp (par exemple, build/bin) et ex√©cutez :

```bash
./llama-server \
  --model /chemin/complet/vers/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
  --port 10000 \
  --ctx-size 1024 \
  --n-gpu-layers 40
```

üõ†Ô∏è Ajustez les param√®tres pour convenir √† votre machine :

- --model : Chemin vers votre fichier mod√®le .gguf
- --port : 10000 (ou choisissez un autre port ouvert)
- --ctx-size : Longueur de contexte de token (peut augmenter si la RAM le permet)
- --n-gpu-layers : Couches transf√©r√©es au GPU pour des performances plus rapides

Une fois le serveur lanc√©, il exposera une API locale compatible OpenAI sur :

```
http://127.0.0.1:10000
```

---

## √âtape 4 : Connecter Llama.cpp √† Open WebUI

Pour contr√¥ler et interroger votre mod√®le en cours d'ex√©cution localement √† partir d'Open WebUI :

1. Ouvrez Open WebUI dans votre navigateur
2. Acc√©dez √† ‚öôÔ∏è Param√®tres Admin ‚Üí Connexions ‚Üí Connexions OpenAI
3. Cliquez sur ‚ûï Ajouter une connexion et entrez :

- URL : `http://127.0.0.1:10000/v1`  
  (Ou utilisez `http://host.docker.internal:10000/v1` si vous ex√©cutez WebUI dans Docker)
- Cl√© API : `none` (laissez vide)

üí° Une fois enregistr√©, Open WebUI commencera √† utiliser votre serveur local Llama.cpp comme backend !

![Connexion Llama.cpp dans Open WebUI](/images/tutorials/deepseek/connection.png)

---

## Astuce rapide : Essayez le mod√®le via l'interface de chat

Une fois connect√©, s√©lectionnez le mod√®le dans le menu de chat Open WebUI et commencez √† interagir !

![Aper√ßu du chat du mod√®le](/images/tutorials/deepseek/response.png)

---

## Vous √™tes pr√™t !

Une fois configur√©, Open WebUI permet de :

- G√©rer et basculer entre les mod√®les locaux servis par Llama.cpp
- Utiliser l'API compatible OpenAI sans cl√© n√©cessaire
- Exp√©rimenter des mod√®les massifs comme DeepSeek-R1 ‚Äî directement depuis votre machine !

---

üöÄ Amusez-vous √† exp√©rimenter et √† cr√©er !