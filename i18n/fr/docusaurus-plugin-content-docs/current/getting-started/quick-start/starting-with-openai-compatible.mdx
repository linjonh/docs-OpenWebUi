---

sidebar_position: 4  
title: "üåê Commencer avec des serveurs compatibles OpenAI"

---

## Aper√ßu

Open WebUI nest pas seulement con√ßu pour OpenAI/Ollama/Llama.cpp ‚Äî vous pouvez connecter **tout serveur impl√©mentant lAPI compatible OpenAI**, fonctionnant localement ou √† distance. Cela est parfait si vous souhaitez utiliser diff√©rents mod√®les linguistiques, ou si vous avez d√©j√† un backend ou un √©cosyst√®me pr√©f√©r√©. Ce guide vous montrera comment :

- Configurer un serveur compatible OpenAI (avec quelques options populaires)
- Le connecter √† Open WebUI
- Commencer √† discuter imm√©diatement

## √âtape 1 : Choisir un serveur compatible OpenAI

Il existe de nombreux serveurs et outils qui utilisent une API compatible OpenAI. Voici quelques-uns des plus populaires :

- [Llama.cpp](https://github.com/ggml-org/llama.cpp) : Extr√™mement efficace, fonctionne sur CPU et GPU
- [Ollama](https://ollama.com/) : Super convivial et multiplateforme
- [LM Studio](https://lmstudio.ai/) : Application de bureau riche pour Windows/Mac/Linux
- [Lemonade (ONNX TurnkeyML)](https://github.com/onnx/turnkeyml) : Backend rapide bas√© sur ONNX avec acc√©l√©ration NPU/iGPU

Choisissez celui qui convient le mieux √† votre flux de travail !

---

#### üçã Commencer avec Lemonade (ONNX TurnkeyML)

Lemonade est un serveur compatible OpenAI bas√© sur ONNX pr√™t √† lemploi. Voici comment lessayer sous Windows :

1. [T√©l√©chargez le dernier `.exe`](https://github.com/onnx/turnkeyml/releases)
2. Ex√©cutez `Lemonade_Server_Installer.exe`
3. Installez et t√©l√©chargez un mod√®le √† laide de linstallateur Lemonade
4. Une fois en cours dex√©cution, votre point de terminaison API sera :

   ```
   http://localhost:8000/api/v0
   ```

![Serveur Lemonade](/images/getting-started/lemonade-server.png)

Consultez [leur documentation](https://github.com/onnx/turnkeyml) pour plus de d√©tails.

---

## √âtape 2 : Connecter votre serveur √† Open WebUI

1. Ouvrez Open WebUI dans votre navigateur.
2. Acc√©dez √† ‚öôÔ∏è **Param√®tres administrateur** ‚Üí **Connexions** ‚Üí **Connexions OpenAI**.
3. Cliquez sur ‚ûï **Ajouter une connexion**.

   - **URL** : Utilisez le point de terminaison API de votre serveur (par exemple, `http://localhost:11434/v1` pour Ollama, ou ladresse de votre propre serveur Llama.cpp).
   - **Cl√© API** : Laissez vide sauf si n√©cessaire.

4. Cliquez sur Sauvegarder.

*Astuce : Si vous utilisez Open WebUI dans Docker et votre serveur de mod√®les sur votre machine h√¥te, utilisez `http://host.docker.internal:<votre-port>/v1`.*

##### **Pour Lemonade :** Lors de lajout de Lemonade, utilisez `http://localhost:8000/api/v0` comme URL.

![Connexion Lemonade](/images/getting-started/lemonade-connection.png)

---

## √âtape 3 : Commencez √† discuter !

S√©lectionnez le mod√®le de votre serveur connect√© dans le menu de discussion et commencez !

Cest tout ! Que vous choisissiez Llama.cpp, Ollama, LM Studio ou Lemonade, vous pouvez facilement exp√©rimenter et g√©rer plusieurs serveurs de mod√®les ‚Äî tout cela dans Open WebUI.

---

üöÄ Amusez-vous √† construire votre configuration AI locale id√©ale !