---
sidebar_position: 400
title: "üîó Points de terminaison de l'API"
---

Ce guide fournit des informations essentielles sur la fa√ßon d'interagir efficacement avec les points de terminaison de l'API pour assurer une int√©gration transparente et une automatisation utilisant nos mod√®les. Veuillez noter qu'il s'agit d'une configuration exp√©rimentale qui peut √™tre mise √† jour ult√©rieurement pour des am√©liorations.

## Authentification

Pour garantir un acc√®s s√©curis√© √† l'API, une authentification est requise üõ°Ô∏è. Vous pouvez authentifier vos requ√™tes API en utilisant le m√©canisme de Bearer Token. Obtenez votre cl√© API depuis **Param√®tres > Compte** dans l'Open WebUI, ou, alternativement, utilisez un JWT (JSON Web Token) pour l'authentification.

## Points de terminaison de l'API notables

### üìú R√©cup√©rer tous les mod√®les

- **Point de terminaison** : `GET /api/models`
- **Description** : R√©cup√®re tous les mod√®les cr√©√©s ou ajout√©s via Open WebUI.
- **Exemple** :

  ```bash
  curl -H "Authorization: Bearer YOUR_API_KEY" http://localhost:3000/api/models
  ```

### üí¨ Compl√©tions de Chat

- **Point de terminaison** : `POST /api/chat/completions`
- **Description** : Sert de point de terminaison compatible avec l'API OpenAI pour les compl√©tions de chat pour les mod√®les d'Open WebUI, y compris les mod√®les Ollama, OpenAI et les mod√®les de fonctions d'Open WebUI.

- **Exemple Curl** :

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "llama3.1",
        "messages": [
          {
            "role": "user",
            "content": "Pourquoi le ciel est-il bleu ?"
          }
        ]
      }
  ```
  
- **Exemple Python** :
  ```python
  import requests
  
  def chat_avec_modele(token):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      data = {
        "model": "granite3.1-dense:8b",
        "messages": [
          {
            "role": "user",
            "content": "Pourquoi le ciel est-il bleu ?"
          }
        ]
      }
      response = requests.post(url, headers=headers, json=data)
      return response.json()
  ```

### ü¶ô Support du proxy API Ollama

Si vous souhaitez interagir directement avec les mod√®les Ollama ‚Äî y compris pour la g√©n√©ration d'embeddings ou le streaming brut des invites ‚Äî Open WebUI offre un passage transparent √† l'API native d'Ollama via une route proxy.

- **URL de base** : `/ollama/<api>`
- **R√©f√©rence** : [Documentation API Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)

#### üîÅ G√©n√©rer une compl√©tion (Streaming)

```bash
curl http://localhost:3000/ollama/api/generate -d {
  "model": "llama3.2",
  "prompt": "Pourquoi le ciel est-il bleu ?"
}
```

#### üì¶ Lister les mod√®les disponibles

```bash
curl http://localhost:3000/ollama/api/tags
```

#### üß† G√©n√©rer des embeddings

```bash
curl -X POST http://localhost:3000/ollama/api/embed -d {
  "model": "llama3.2",
  "input": ["Open WebUI est g√©nial !", "G√©n√©rons des embeddings."]
}
```

Ceci est id√©al pour construire des index de recherche, des syst√®mes de r√©cup√©ration ou des pipelines personnalis√©s utilisant les mod√®les Ollama derri√®re Open WebUI.

### üß© G√©n√©ration augment√©e par r√©cup√©ration (RAG)

La fonctionnalit√© de G√©n√©ration augment√©e par r√©cup√©ration (RAG) permet d'am√©liorer les r√©ponses en incorporant des donn√©es provenant de sources externes. Ci-dessous, vous trouverez les m√©thodes pour g√©rer des fichiers et des collections de connaissances via l'API, et comment les utiliser efficacement dans les compl√©tions de chat.

#### T√©l√©chargement de fichiers

Pour utiliser des donn√©es externes dans les r√©ponses RAG, vous devez d'abord t√©l√©charger les fichiers. Le contenu du fichier t√©l√©charg√© est automatiquement extrait et stock√© dans une base de donn√©es vectorielle.

- **Point de terminaison** : `POST /api/v1/files/`
- **Exemple Curl** :

  ```bash
  curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Accept: application/json" \
  -F "file=@/path/to/your/file" http://localhost:3000/api/v1/files/
  ```

- **Exemple Python** :

  ```python
  import requests
  
  def telecharger_fichier(token, file_path):
      url = http://localhost:3000/api/v1/files/
      headers = {
          Authorization: fBearer {token},
          Accept: application/json
      }
      files = {file: open(file_path, rb)}
      response = requests.post(url, headers=headers, files=files)
      return response.json()
  ```

#### Ajouter des fichiers aux collections de connaissances

Apr√®s le t√©l√©chargement, vous pouvez regrouper des fichiers dans une collection de connaissances ou les r√©f√©rencer individuellement dans les chats.

- **Point de terminaison** : `POST /api/v1/knowledge/{id}/file/add`
- **Exemple Curl** :

  ```bash
  curl -X POST http://localhost:3000/api/v1/knowledge/{knowledge_id}/file/add \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {"file_id": "your-file-id-here"}
  ```

- **Exemple Python** :

  ```python
  import requests

  def ajouter_fichier_a_connaissance(token, knowledge_id, file_id):
      url = fhttp://localhost:3000/api/v1/knowledge/{knowledge_id}/file/add
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      data = {file_id: file_id}
      response = requests.post(url, headers=headers, json=data)
      return response.json()
  ```

#### Utilisation des fichiers et des collections dans les compl√©tions de chat

Vous pouvez r√©f√©rencer √† la fois des fichiers individuels ou des collections enti√®res dans vos requ√™tes RAG pour obtenir des r√©ponses enrichies.

##### Utiliser un fichier individuel dans les compl√©tions de chat

Cette m√©thode est b√©n√©fique lorsque vous souhaitez focaliser la r√©ponse du mod√®le de chat sur le contenu d'un fichier sp√©cifique.

- **Endpoint** : `POST /api/chat/completions`
- **Exemple Curl** :

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "gpt-4-turbo",
        "messages": [
          {"role": "user", "content": "Expliquez les concepts dans ce document."}
        ],
        "files": [
          {"type": "file", "id": "your-file-id-here"}
        ]
      }
  ```

- **Exemple Python** :

  ```python
  import requests

  def chat_with_file(token, model, query, file_id):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      payload = {
          model: model,
          messages: [{role: user, content: query}],
          files: [{type: file, id: file_id}]
      }
      response = requests.post(url, headers=headers, json=payload)
      return response.json()
  ```

##### Utiliser une collection de connaissances dans les compl√©tions de chat

Exploitez une collection de connaissances pour am√©liorer la r√©ponse lorsque la requ√™te peut b√©n√©ficier d'un contexte plus large ou de plusieurs documents.

- **Endpoint** : `POST /api/chat/completions`
- **Exemple Curl** :

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "gpt-4-turbo",
        "messages": [
          {"role": "user", "content": "Fournissez des informations sur les perspectives historiques couvertes dans la collection."}
        ],
        "files": [
          {"type": "collection", "id": "your-collection-id-here"}
        ]
      }
  ```

- **Exemple Python** :

  ```python
  import requests
  
  def chat_with_collection(token, model, query, collection_id):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      payload = {
          model: model,
          messages: [{role: user, content: query}],
          files: [{type: collection, id: collection_id}]
      }
      response = requests.post(url, headers=headers, json=payload)
      return response.json()
  ```

Ces m√©thodes permettent une utilisation efficace des connaissances externes via des fichiers t√©l√©charg√©s et des collections de connaissances, am√©liorant ainsi les capacit√©s des applications de chat gr√¢ce √† l'API Open WebUI. Que vous utilisiez des fichiers individuellement ou au sein de collections, vous pouvez personnaliser l'int√©gration en fonction de vos besoins sp√©cifiques.

## Avantages d'utiliser Open WebUI comme fournisseur unifi√© de LLM

Open WebUI offre une myriade d'avantages, en faisant un outil essentiel pour les d√©veloppeurs et les entreprises :

- **Interface unifi√©e** : Simplifiez vos interactions avec diff√©rents LLMs via une plateforme unique et int√©gr√©e.
- **Facilit√© de mise en ≈ìuvre** : Int√©gration rapide avec une documentation compl√®te et un support communautaire.

## Liens vers la documentation Swagger

:::important
Assurez-vous de d√©finir la variable d'environnement `ENV` sur `dev` afin d'acc√©der √† la documentation Swagger pour l'un de ces services. Sans cette configuration, la documentation ne sera pas accessible.
:::

Acc√©dez √† la documentation d√©taill√©e de l'API pour diff√©rents services fournis par Open WebUI :

| Application | Chemin de la documentation |
|-------------|-----------------------------|
| Principal   | `/docs`                     |


En suivant ces directives, vous pouvez rapidement int√©grer et commencer √† utiliser l'API Open WebUI. Si vous rencontrez des probl√®mes ou avez des questions, n'h√©sitez pas √† nous contacter via notre communaut√© Discord ou √† consulter la FAQ. Bon codage ! üåü
