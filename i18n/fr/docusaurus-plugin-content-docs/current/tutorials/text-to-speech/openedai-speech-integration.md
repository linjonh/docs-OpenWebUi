---
sidebar_position: 2
title: "üó®Ô∏è Openedai-speech Utilisation de Docker"
---

:::warning
Ce tutoriel est une contribution communautaire et n'est pas support√© par l'√©quipe d'Open WebUI. Il sert uniquement de d√©monstration sur la mani√®re de personnaliser Open WebUI pour votre cas d'utilisation sp√©cifique. Vous voulez contribuer ? Consultez le tutoriel de contribution.
:::

**Int√©gration de `openedai-speech` dans Open WebUI avec Docker**
==============================================================

**Qu'est-ce que `openedai-speech` ?**
-------------------------------------

:::info
[openedai-speech](https://github.com/matatonic/openedai-speech) est un serveur de synth√®se vocale compatible avec l'API audio/vocale d'OpenAI.

Il sert le point d'acc√®s `/v1/audio/speech` et offre une exp√©rience de synth√®se vocale gratuite et priv√©e avec des capacit√©s de clonage vocal personnalis√©. Ce service n'est en aucun cas affili√© √† OpenAI et ne n√©cessite pas de cl√© API OpenAI.
:::

**Pr√©requis**
-------------

* Docker install√© sur votre syst√®me
* Open WebUI fonctionnant dans un conteneur Docker
* Compr√©hension de base de Docker et Docker Compose

**Option 1 : Utilisation de Docker Compose**
-------------------------------------------

**√âtape 1 : Cr√©er un nouveau dossier pour le service `openedai-speech`**
-----------------------------------------------------------------------

Cr√©ez un nouveau dossier, par exemple, `openedai-speech-service`, pour stocker les fichiers `docker-compose.yml` et `speech.env`.

**√âtape 2 : Cloner le d√©p√¥t GitHub `openedai-speech`**
-----------------------------------------------------

```bash
git clone https://github.com/matatonic/openedai-speech.git
```

Cela t√©l√©chargera le d√©p√¥t `openedai-speech` sur votre machine locale, comprenant les fichiers Docker Compose (`docker-compose.yml`, `docker-compose.min.yml`, et `docker-compose.rocm.yml`) et autres fichiers n√©cessaires.

**√âtape 3 : Renommer le fichier `sample.env` en `speech.env` (Personnaliser si n√©cessaire)**
------------------------------------------------------------------------------------------

Dans le dossier du d√©p√¥t `openedai-speech`, cr√©ez un nouveau fichier nomm√© `speech.env` avec le contenu suivant :

```yaml
TTS_HOME=voices
HF_HOME=voices
#PRELOAD_MODEL=xtts
#PRELOAD_MODEL=xtts_v2.0.2
#PRELOAD_MODEL=parler-tts/parler_tts_mini_v0.1
#EXTRA_ARGS=--log-level DEBUG --unload-timer 300
#USE_ROCM=1
```

**√âtape 4 : Choisir un fichier Docker Compose**
----------------------------------------------

Vous pouvez utiliser l'un des fichiers Docker Compose suivants :

* [docker-compose.yml](https://github.com/matatonic/openedai-speech/blob/main/docker-compose.yml) : Ce fichier utilise l'image `ghcr.io/matatonic/openedai-speech` et se construit √† partir de [Dockerfile](https://github.com/matatonic/openedai-speech/blob/main/Dockerfile).
* [docker-compose.min.yml](https://github.com/matatonic/openedai-speech/blob/main/docker-compose.min.yml) : Ce fichier utilise l'image `ghcr.io/matatonic/openedai-speech-min` et se construit √† partir de [Dockerfile.min](https://github.com/matatonic/openedai-speech/blob/main/Dockerfile.min).
  Cette image est une version minimale qui inclut uniquement le support Piper et ne n√©cessite pas de GPU.
* [docker-compose.rocm.yml](https://github.com/matatonic/openedai-speech/blob/main/docker-compose.rocm.yml) : Ce fichier utilise l'image `ghcr.io/matatonic/openedai-speech-rocm` et se construit √† partir de [Dockerfile](https://github.com/matatonic/openedai-speech/blob/main/Dockerfile) avec support ROCm.

**√âtape 4 : Construire l'image Docker choisie**
---------------------------------------------

Avant d'ex√©cuter le fichier Docker Compose, vous devez construire l'image Docker :

* **GPU Nvidia (support CUDA)** :

```bash
docker build -t ghcr.io/matatonic/openedai-speech .
```

* **GPU AMD (support ROCm)** :

```bash
docker build -f Dockerfile --build-arg USE_ROCM=1 -t ghcr.io/matatonic/openedai-speech-rocm .
```

* **CPU uniquement, Sans GPU (uniquement Piper)** :

```bash
docker build -f Dockerfile.min -t ghcr.io/matatonic/openedai-speech-min .
```

**√âtape 5 : Ex√©cuter la commande `docker compose up -d` correcte**
-----------------------------------------------------------------

* **GPU Nvidia (support CUDA)** : Ex√©cutez la commande suivante pour d√©marrer le service `openedai-speech` en mode d√©tach√© :

```bash
docker compose up -d
```

* **GPU AMD (support ROCm)** : Ex√©cutez la commande suivante pour d√©marrer le service `openedai-speech` en mode d√©tach√© :

```bash
docker compose -f docker-compose.rocm.yml up -d
```

* **ARM64 (Apple M-series, Raspberry Pi)** : XTTS poss√®de uniquement un support CPU ici et sera tr√®s lent. Vous pouvez utiliser l'image Nvidia pour XTTS avec CPU (lent), ou utiliser l'image uniquement Piper (recommand√©) :

```bash
docker compose -f docker-compose.min.yml up -d
```

* **CPU uniquement, Sans GPU (uniquement Piper)** : Pour une image docker minimale avec uniquement le support Piper (< 1GB vs. 8GB) :

```bash
docker compose -f docker-compose.min.yml up -d
```

Cela d√©marrera le service `openedai-speech` en mode d√©tach√©.

**Option 2 : Utilisation des commandes Docker Run**
-------------------------------------------------

Vous pouvez √©galement utiliser les commandes Docker run suivantes pour d√©marrer le service `openedai-speech` en mode d√©tach√© :

* **GPU Nvidia (CUDA)** : Ex√©cutez la commande suivante pour construire et d√©marrer le service `openedai-speech` :

```bash
docker build -t ghcr.io/matatonic/openedai-speech .
docker run -d --gpus=all -p 8000:8000 -v voices:/app/voices -v config:/app/config --name openedai-speech ghcr.io/matatonic/openedai-speech
```

* **ROCm (GPU AMD)** : Ex√©cutez la commande suivante pour construire et d√©marrer le service `openedai-speech` :

> Pour activer la prise en charge de ROCm, d√©commentez la ligne `#USE_ROCM=1` dans le fichier `speech.env`.

```bash
docker build -f Dockerfile --build-arg USE_ROCM=1 -t ghcr.io/matatonic/openedai-speech-rocm .
docker run -d --privileged --init --name openedai-speech -p 8000:8000 -v voices:/app/voices -v config:/app/config ghcr.io/matatonic/openedai-speech-rocm
```

* **CPU uniquement, sans GPU (Piper uniquement)** : Ex√©cutez la commande suivante pour construire et d√©marrer le service `openedai-speech` :

```bash
docker build -f Dockerfile.min -t ghcr.io/matatonic/openedai-speech-min .
docker run -d -p 8000:8000 -v voices:/app/voices -v config:/app/config --name openedai-speech ghcr.io/matatonic/openedai-speech-min
```

**√âtape 6 : Configurer Open WebUI pour utiliser `openedai-speech` comme TTS**
---------------------------------------------------------

![openedai-tts](https://github.com/silentoplayz/docs/assets/50341825/ea08494f-2ebf-41a2-bb0f-9b48dd3ace79)

Ouvrez les param√®tres d'Open WebUI et acc√©dez aux param√®tres TTS sous **Admin Panel > Settings > Audio**. Ajoutez la configuration suivante :

* **URL de base de l'API** : `http://host.docker.internal:8000/v1`
* **Cl√© API** : `sk-111111111` (Notez que c'est une cl√© API fictive, car `openedai-speech` ne n√©cessite pas de cl√© API. Vous pouvez utiliser n'importe quel contenu pour ce champ, tant qu'il est rempli.)

**√âtape 7 : Choisir une voix**
--------------------------

Sous `TTS Voice` dans le m√™me menu de param√®tres audio du panneau d'administration, vous pouvez d√©finir le `TTS Model` √† utiliser parmi les choix suivants pris en charge par `openedai-speech`. Les voix de ces mod√®les sont optimis√©es pour la langue anglaise.

* `tts-1` ou `tts-1-hd` : `alloy`, `echo`, `echo-alt`, `fable`, `onyx`, `nova` et `shimmer` (`tts-1-hd` est configurable ; utilise des √©chantillons OpenAI par d√©faut)

**√âtape 8 : Appuyez sur `Save` pour appliquer les modifications et commencez √† profiter de voix naturelles**
--------------------------------------------------------------------------------------------

Appuyez sur le bouton `Save` pour appliquer les modifications aux param√®tres d'Open WebUI. Actualisez la page pour que les modifications prennent pleinement effet et profitez de l'int√©gration de `openedai-speech` dans Open WebUI pour lire des r√©ponses textuelles √† haute voix avec une voix naturelle.

**D√©tails du mod√®le :**
------------------

`openedai-speech` prend en charge plusieurs mod√®les de synth√®se vocale, chacun avec ses propres forces et exigences. Les mod√®les suivants sont disponibles :

* **Piper TTS** (tr√®s rapide, fonctionne sur CPU) : Utilisez vos propres [voix Piper](https://rhasspy.github.io/piper-samples/) via le fichier de configuration `voice_to_speaker.yaml`. Ce mod√®le est id√©al pour les applications n√©cessitant une faible latence et des performances √©lev√©es. Piper TTS prend √©galement en charge des voix [multilingues](https://github.com/matatonic/openedai-speech#multilingual).
* **Coqui AI/TTS XTTS v2** (rapide, mais n√©cessite environ 4 Go de VRAM GPU et un GPU Nvidia avec CUDA) : Ce mod√®le utilise la technologie de clonage vocal XTTS v2 de Coqui AI pour g√©n√©rer des voix de haute qualit√©. Bien qu'il impose des exigences GPU plus √©lev√©es, il offre d'excellentes performances et un son de haute qualit√©. Coqui prend √©galement en charge des voix [multilingues](https://github.com/matatonic/openedai-speech#multilingual).
* **Support Beta Parler-TTS** (exp√©rimental, plus lent) : Ce mod√®le utilise le framework Parler-TTS pour g√©n√©rer des voix. Bien qu'il soit actuellement en phase b√™ta, il vous permet de d√©crire des caract√©ristiques tr√®s basiques de la voix du locuteur. La voix exacte sera l√©g√®rement diff√©rente √† chaque g√©n√©ration, mais devrait √™tre similaire √† la description du locuteur fournie. Pour des id√©es sur la fa√ßon de d√©crire des voix, voir [Text Description to Speech](https://www.text-description-to-speech.com/).

**D√©pannage**
-------------------

Si vous rencontrez des probl√®mes lors de l'int√©gration de `openedai-speech` avec Open WebUI, suivez ces √©tapes de d√©pannage :

* **V√©rifiez le service `openedai-speech`** : Assurez-vous que le service `openedai-speech` est en cours d'ex√©cution et que le port que vous avez sp√©cifi√© dans le fichier docker-compose.yml est expos√©.
* **V√©rifiez l'acc√®s √† host.docker.internal** : Assurez-vous que le nom d'h√¥te `host.docker.internal` est r√©solvable depuis le conteneur Open WebUI. Cela est n√©cessaire car `openedai-speech` est expos√© via `localhost` sur votre PC, mais `open-webui` ne peut normalement pas y acc√©der depuis son conteneur. Vous pouvez ajouter un volume au fichier docker-compose.yml pour monter un fichier du h√¥te vers le conteneur, par exemple dans un r√©pertoire qui sera desservi par openedai-speech.
* **V√©rifiez la configuration de la cl√© API** : Assurez-vous que la cl√© API est d√©finie sur une valeur factice ou laiss√©e effectivement non coch√©e, car `openedai-speech` ne n√©cessite pas de cl√© API.
* **V√©rifiez la configuration de la voix** : Assurez-vous que la voix que vous essayez d'utiliser pour le TTS existe dans votre fichier `voice_to_speaker.yaml` et que les fichiers correspondants (par exemple, les fichiers XML de voix) sont pr√©sents dans le r√©pertoire correct.
* **V√©rifiez les chemins des mod√®les de voix** : En cas de probl√®mes de chargement des mod√®les de voix, v√©rifiez que les chemins dans votre fichier `voice_to_speaker.yaml` correspondent aux emplacements r√©els de vos mod√®les de voix.

**Conseils suppl√©mentaires pour le d√©pannage**
---------------------------------------------

* Consultez les journaux de `openedai-speech` pour identifier les erreurs ou avertissements qui pourraient indiquer o√π se situe le probl√®me.
* Assurez-vous que le fichier `docker-compose.yml` est correctement configur√© pour votre environnement.
* Si vous rencontrez toujours des probl√®mes, essayez de red√©marrer le service `openedai-speech` ou l'ensemble de l'environnement Docker.
* Si le probl√®me persiste, consultez le d√©p√¥t GitHub de `openedai-speech` ou demandez de l'aide sur un forum communautaire pertinent.

**FAQ**
-------

**Comment puis-je contr√¥ler la gamme √©motionnelle de l'audio g√©n√©r√© ?**

Il n'existe aucun m√©canisme direct pour contr√¥ler la sortie √©motionnelle de l'audio g√©n√©r√©. Certains facteurs, tels que la capitalisation ou la grammaire, peuvent influencer la sortie audio, mais les tests internes ont donn√© des r√©sultats mitig√©s.

**O√π sont stock√©s les fichiers de voix ? Qu'en est-il du fichier de configuration ?**

Les fichiers de configuration, qui d√©finissent les voix disponibles et leurs propri√©t√©s, sont stock√©s dans le volume de configuration. Plus pr√©cis√©ment, les voix par d√©faut sont d√©finies dans `voice_to_speaker.default.yaml`.

**Ressources suppl√©mentaires**
-----------------------------

Pour plus d'informations sur la configuration d'Open WebUI pour utiliser `openedai-speech`, y compris la d√©finition des variables d'environnement, consultez la [documentation Open WebUI](https://docs.openwebui.com/getting-started/env-configuration#text-to-speech).

Pour en savoir plus sur `openedai-speech`, veuillez consulter le [d√©p√¥t GitHub](https://github.com/matatonic/openedai-speech).

**Comment ajouter des voix suppl√©mentaires √† `openedai-speech` :**
[Custom-Voices-HowTo](https://github.com/matatonic/openedai-speech?tab=readme-ov-file#custom-voices-howto)

:::note
Vous pouvez modifier le num√©ro de port dans le fichier `docker-compose.yml` pour tout port ouvert et utilisable, mais assurez-vous de mettre √† jour l'**URL de base de l'API** dans les param√®tres audio Admin d'Open WebUI en cons√©quence.
:::
