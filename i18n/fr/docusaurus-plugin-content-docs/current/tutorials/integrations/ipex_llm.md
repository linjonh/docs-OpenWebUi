---
sidebar_position: 11
title: "üñ•Ô∏è Configuration locale de LLM avec IPEX-LLM sur GPU Intel"
---

:::warning
Ce tutoriel est une contribution de la communaut√© et n'est pas soutenu par l'√©quipe d'Open WebUI. Il sert uniquement de d√©monstration sur la personnalisation d'Open WebUI pour vos besoins sp√©cifiques. Vous souhaitez contribuer ? Consultez le tutoriel de contribution.
:::

:::note
Ce guide est v√©rifi√© avec Open WebUI install√© via [Installation Manuelle](/getting-started/index.md).
:::

# Configuration locale de LLM avec IPEX-LLM sur GPU Intel

:::info
[**IPEX-LLM**](https://github.com/intel-analytics/ipex-llm) est une biblioth√®que PyTorch permettant d'ex√©cuter des LLM sur CPU et GPU Intel (par exemple, un PC local avec iGPU, des GPU discrets comme Arc A-Series, Flex et Max) avec une latence tr√®s faible.
:::

Ce tutoriel montre comment configurer Open WebUI avec un **back-end Ollama acc√©l√©r√© par IPEX-LLM h√©berg√© sur GPU Intel**. En suivant ce guide, vous pourrez configurer Open WebUI m√™me sur un PC √† faible co√ªt (c'est-√†-dire uniquement avec un GPU int√©gr√©) avec une exp√©rience fluide.

## Lancer le serveur Ollama sur GPU Intel

Consultez [ce guide](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html) issu de la documentation officielle de IPEX-LLM pour savoir comment installer et ex√©cuter le serveur Ollama acc√©l√©r√© par IPEX-LLM sur GPU Intel.

:::tip
Si vous souhaitez acc√©der au service Ollama depuis une autre machine, assurez-vous de d√©finir ou d'exporter la variable d'environnement `OLLAMA_HOST=0.0.0.0` avant d'ex√©cuter la commande `ollama serve`.
:::

## Configurer Open WebUI

Acc√©dez aux param√®tres Ollama via **Param√®tres -> Connexions** dans le menu. Par d√©faut, l'**URL de Base Ollama** est pr√©d√©finie sur `https://localhost:11434`, comme illustr√© dans la capture ci-dessous. Pour v√©rifier l'√©tat de la connexion au service Ollama, cliquez sur le **bouton Rafra√Æchir** situ√© √† c√¥t√© de la zone de texte. Si le WebUI ne parvient pas √† √©tablir une connexion avec le serveur Ollama, un message d'erreur s'affichera indiquant, `WebUI ne peut pas se connecter √† Ollama`.

![√âchec de la configuration d'Open WebUI Ollama](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings_0.png)

Si la connexion est r√©ussie, un message indiquera `Connexion au service v√©rifi√©e`, comme illustr√© ci-dessous.

![Succ√®s de la configuration d'Open WebUI Ollama](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings.png)

:::tip
Si vous souhaitez utiliser un serveur Ollama h√©berg√© √† une autre URL, mettez simplement √† jour l'**URL de Base Ollama** avec la nouvelle URL et appuyez sur le bouton **Rafra√Æchir** pour confirmer √† nouveau la connexion √† Ollama.
:::
