---
sidebar_position: 1
title: "üêã Ex√©cuter DeepSeek R1 Dynamic 1.58-bit avec Llama.cpp"
---

Un grand merci √† **UnslothAI** pour leurs efforts incroyables‚ÄØ! Gr√¢ce √† leur travail acharn√©, nous pouvons d√©sormais ex√©cuter le mod√®le complet **DeepSeek-R1** de 671 milliards de param√®tres dans sa version quantifi√©e dynamique 1.58-bit (compress√©e √† seulement 131 Go) sur **Llama.cpp**‚ÄØ! Et le mieux‚ÄØ? Vous n‚Äôavez plus besoin de d√©sesp√©rer de ne pas avoir des GPU ou serveurs de classe entreprise gigantesques ‚Äî il est possible de faire fonctionner ce mod√®le sur votre ordinateur personnel (certes lentement pour la plupart du mat√©riel grand public).

:::note
Le seul vrai mod√®le **DeepSeek-R1** sur Ollama est la **version 671B** disponible ici : [https://ollama.com/library/deepseek-r1:671b](https://ollama.com/library/deepseek-r1:671b). Les autres versions sont des mod√®les **distill√©s**.
:::

Ce guide se concentre sur l'ex√©cution du **mod√®le complet DeepSeek-R1 Dynamic quantifi√© en 1.58-bit** en utilisant **Llama.cpp** int√©gr√© avec **Open WebUI**. Pour ce tutoriel, nous montrerons les √©tapes avec une machine **M4 Max + 128 Go de RAM**. Vous pouvez adapter les param√®tres √† votre propre configuration.

---

## √âtape 1 : Installer Llama.cpp

Vous pouvez soit‚ÄØ:
- [T√©l√©charger les binaires pr√©compil√©s](https://github.com/ggerganov/llama.cpp/releases)
- **Ou le construire vous-m√™me** : Suivez les instructions ici : [Guide de construction de Llama.cpp](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)

## √âtape 2 : T√©l√©chargez le mod√®le fourni par UnslothAI

Rendez-vous sur [la page Hugging Face d'Unsloth](https://huggingface.co/unsloth/DeepSeek-R1-GGUF) et t√©l√©chargez la version **quantifi√©e dynamique** appropri√©e de DeepSeek-R1. Pour ce tutoriel, nous utiliserons la version **1.58-bit (131 Go)**, qui est fortement optimis√©e mais reste √©tonnamment fonctionnelle.


:::tip
Sachez o√π se trouve votre "r√©pertoire de travail" ‚Äî o√π votre script Python ou session terminal est en cours d'ex√©cution. Les fichiers du mod√®le seront t√©l√©charg√©s dans un sous-dossier de ce r√©pertoire par d√©faut, alors assurez-vous de conna√Ætre son chemin‚ÄØ! Par exemple, si vous ex√©cutez la commande ci-dessous dans `/Users/yourname/Documents/projects`, votre mod√®le t√©l√©charg√© sera sauvegard√© sous `/Users/yourname/Documents/projects/DeepSeek-R1-GGUF`.
:::

Pour en savoir plus sur le processus de d√©veloppement d‚ÄôUnslothAI et pourquoi ces versions quantifi√©es dynamiques sont si efficaces, consultez leur article de blog : [UnslothAI DeepSeek R1 Dynamic Quantization](https://unsloth.ai/blog/deepseekr1-dynamic).

Voici comment t√©l√©charger le mod√®le par programmation :
```python
# Installez les d√©pendances Hugging Face avant d'ex√©cuter ceci :
# pip install huggingface_hub hf_transfer

from huggingface_hub import snapshot_download

snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",  # Indiquer le d√©p√¥t Hugging Face
    local_dir = "DeepSeek-R1-GGUF",         # Le mod√®le sera t√©l√©charg√© dans ce r√©pertoire
    allow_patterns = ["*UD-IQ1_S*"],        # T√©l√©charger uniquement la version 1.58-bit
)
```

Une fois le t√©l√©chargement termin√©, vous trouverez les fichiers du mod√®le dans une structure de r√©pertoires comme celle-ci :
```
DeepSeek-R1-GGUF/
‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf
```

:::info
üõ†Ô∏è Mettez √† jour les chemins dans les √©tapes ult√©rieures pour qu'ils correspondent √† votre structure de r√©pertoire sp√©cifique. Par exemple, si votre script √©tait dans `/Users/tim/Downloads`, le chemin complet vers le fichier GGUF serait :
`/Users/tim/Downloads/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`.
:::

## √âtape 3 : Assurez-vous qu'Open WebUI est install√© et en cours d'ex√©cution

Si vous n'avez pas encore install√© **Open WebUI**, pas de souci‚ÄØ! La configuration est simple. Il suffit de suivre la [documentation d'Open WebUI ici](https://docs.openwebui.com/). Une fois install√©, d√©marrez l'application ‚Äî nous la connecterons √† une √©tape ult√©rieure pour interagir avec le mod√®le DeepSeek-R1.


## √âtape 4 : Servir le mod√®le en utilisant Llama.cpp

Maintenant que le mod√®le est t√©l√©charg√©, l'√©tape suivante consiste √† l'ex√©cuter en utilisant le **mode serveur de Llama.cpp**. Avant de commencer :

1. **Localisez le binaire `llama-server`.**
   Si vous avez construit √† partir de la source (comme indiqu√© √† l'√©tape 1), l'ex√©cutable `llama-server` se trouvera dans `llama.cpp/build/bin`. Naviguez vers ce r√©pertoire en utilisant la commande `cd` :
   ```bash
   cd [path-to-llama-cpp]/llama.cpp/build/bin
   ```

   Remplacez `[path-to-llama-cpp]` par l‚Äôendroit o√π vous avez clon√© ou construit Llama.cpp. Par exemple :
   ```bash
   cd ~/Documents/workspace/llama.cpp/build/bin
   ```

2. **Indiquez votre dossier de mod√®le.**
   Utilisez le chemin complet vers les fichiers GGUF t√©l√©charg√©s cr√©√©s √† l'√©tape 2. Lors de la mise en service du mod√®le, sp√©cifiez la premi√®re partie des fichiers GGUF divis√©s (par exemple, `DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`).

Voici la commande pour d√©marrer le serveur :
```bash
./llama-server \
    --model /[your-directory]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --port 10000 \
    --ctx-size 1024 \
    --n-gpu-layers 40
```


:::tip
üîë **Param√®tres √† personnaliser en fonction de votre machine :**

- **`--model` :** Remplacez `/[your-directory]/` par le chemin o√π les fichiers GGUF ont √©t√© t√©l√©charg√©s √† l'√âtape 2.
- **`--port` :** Le port par d√©faut du serveur est `8080`, mais n'h√©sitez pas √† le modifier en fonction de la disponibilit√© de vos ports.
- **`--ctx-size` :** D√©termine la longueur du contexte (nombre de tokens). Vous pouvez l'augmenter si votre mat√©riel le permet, mais faites attention √† l'augmentation de l'utilisation de la RAM/VRAM.
- **`--n-gpu-layers` :** D√©finissez le nombre de couches que vous souhaitez d√©charger sur votre GPU pour une inf√©rence plus rapide. Le nombre exact d√©pend de la capacit√© m√©moire de votre GPU ‚Äî consultez le tableau d'Unsloth pour des recommandations sp√©cifiques.
:::

Par exemple, si votre mod√®le a √©t√© t√©l√©charg√© dans `/Users/tim/Documents/workspace`, votre commande ressemblerait √† ceci :
```bash
./llama-server \
    --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --port 10000 \
    --ctx-size 1024 \
    --n-gpu-layers 40
```

Une fois le serveur d√©marr√©, il h√©bergera un **point de terminaison API local compatible OpenAI** √† :
```
http://127.0.0.1:10000
```

:::info
üñ•Ô∏è **Serveur Llama.cpp en cours d'ex√©cution**

![Capture d'√©cran du serveur](/images/tutorials/deepseek/serve.png)

Apr√®s avoir ex√©cut√© la commande, vous devriez voir un message confirmant que le serveur est actif et √† l'√©coute sur le port 10000.
:::

Assurez-vous de **garder cette session terminal ouverte** car elle sert le mod√®le pour toutes les √©tapes suivantes.

## √âtape 5 : Connecter Llama.cpp √† Open WebUI

1. Acc√©dez aux **Param√®tres Admin** dans Open WebUI.
2. Naviguez jusqu'√† **Connections > OpenAI Connections.**
3. Ajoutez les d√©tails suivants pour la nouvelle connexion :
   - URL : `http://127.0.0.1:10000/v1` (ou `http://host.docker.internal:10000/v1` si vous utilisez Open WebUI via Docker)
   - Cl√© API : `none`

:::info
üñ•Ô∏è **Ajouter une connexion dans Open WebUI**

![Capture d'√©cran de la connexion](/images/tutorials/deepseek/connection.png)

Apr√®s avoir ex√©cut√© la commande, vous devriez voir un message confirmant que le serveur est actif et √† l'√©coute sur le port 10000.
:::

Une fois la connexion enregistr√©e, vous pouvez commencer √† interroger **DeepSeek-R1** directement depuis Open WebUI ! üéâ

---

## Exemple : G√©n√©ration de r√©ponses

Vous pouvez d√©sormais utiliser l'interface de discussion d'Open WebUI pour interagir avec le **mod√®le DeepSeek-R1 Dynamic 1.58-bit**.

![Capture d'√©cran de la r√©ponse](/images/tutorials/deepseek/response.png)

---

## Notes et consid√©rations

- **Performance :**
  Ex√©cuter un mod√®le massif de 131 Go comme DeepSeek-R1 sur du mat√©riel personnel sera **lent**. M√™me avec notre M4 Max (128 Go de RAM), les vitesses d'inf√©rence √©taient modestes. Mais le simple fait que cela fonctionne t√©moigne des optimisations d'UnslothAI.

- **Exigences VRAM/M√©moire :**
  Assurez-vous de disposer de suffisamment de VRAM et de RAM syst√®me pour des performances optimales. Avec des GPU bas de gamme ou des configurations uniquement CPU, attendez-vous √† des vitesses plus lentes (mais c'est toujours faisable !).

---

Gr√¢ce √† **UnslothAI** et **Llama.cpp**, l'ex√©cution de l'un des plus grands mod√®les de raisonnement open-source, **DeepSeek-R1** (version 1,58-bit), est enfin accessible aux particuliers. Bien qu'il soit difficile de faire tourner de tels mod√®les sur du mat√©riel grand public, la possibilit√© de le faire sans infrastructure informatique massive est un jalon technologique important.

‚≠ê Un grand merci √† la communaut√© pour repousser les limites de la recherche en IA ouverte.

Bonnes exp√©rimentations ! üöÄ
