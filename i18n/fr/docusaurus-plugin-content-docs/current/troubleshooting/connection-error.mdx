---
sidebar_position: 0
title: "ğŸš§ ProblÃ¨mes de ConnectivitÃ© Serveur"
---

Nous sommes ici pour vous aider Ã  tout configurer et Ã  fonctionner correctement. Ci-dessous, vous trouverez des instructions dÃ©taillÃ©es adaptÃ©es Ã  diffÃ©rents scÃ©narios pour rÃ©soudre les problÃ¨mes de connexion courants avec Ollama et les serveurs externes comme Hugging Face.

## ğŸŒŸ Connexion au Serveur Ollama

### ğŸš€ AccÃ©der Ã  Ollama depuis Open WebUI

Vous avez du mal Ã  vous connecter Ã  Ollama depuis Open WebUI ? Cela pourrait Ãªtre dÃ» au fait qu'Ollama n'Ã©coute pas sur une interface rÃ©seau permettant des connexions externes. RÃ©glons cela :

1. **Configurer Ollama pour Ã‰couter Ã  Large Ã‰chelle** ğŸ§:
   DÃ©finissez `OLLAMA_HOST` sur `0.0.0.0` pour que Ollama Ã©coute sur toutes les interfaces rÃ©seau.

2. **Mettre Ã  Jour les Variables d'Environnement**:
   Assurez-vous que `OLLAMA_HOST` est correctement dÃ©fini dans votre environnement de dÃ©ploiement.

3. **RedÃ©marrer Ollama**ğŸ”„:
   Un redÃ©marrage est nÃ©cessaire pour que les modifications prennent effet.

ğŸ’¡ AprÃ¨s la configuration, vÃ©rifiez qu'Ollama est accessible en visitant l'interface WebUI.

Pour des instructions plus dÃ©taillÃ©es sur la configuration d'Ollama, veuillez consulter la [Documentation Officielle d'Ollama](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux).

### ğŸ³ Erreur de Connexion Docker

Si vous voyez une erreur de connexion en tentant d'accÃ©der Ã  Ollama, cela pourrait Ãªtre dÃ» au fait que le conteneur Docker de WebUI ne peut pas communiquer avec le serveur Ollama en cours d'exÃ©cution sur votre hÃ´te. RÃ©glons cela :

1. **Ajuster les ParamÃ¨tres RÃ©seau** ğŸ› ï¸:
   Utilisez l'option `--network=host` dans votre commande Docker. Cela lie directement votre conteneur au rÃ©seau de votre hÃ´te.

2. **Changer le Port**:
   Notez que le port interne passe de 3000 Ã  8080.

**Exemple de Commande Docker**:
```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
ğŸ”— AprÃ¨s avoir exÃ©cutÃ© la commande ci-dessus, votre WebUI devrait Ãªtre accessible Ã  `http://localhost:8080`.

## ğŸ”’ ProblÃ¨me de Connexion SSL avec Hugging Face

Vous avez rencontrÃ© une erreur SSL ? Cela pourrait Ãªtre un problÃ¨me avec le serveur Hugging Face. Voici quoi faire :

1. **VÃ©rifier le Statut du Serveur Hugging Face**:
   VÃ©rifiez s'il y a une panne connue ou un problÃ¨me de leur cÃ´tÃ©.

2. **Changer de Point de Terminaison**:
   Si Hugging Face est en panne, changez le point de terminaison dans votre commande Docker.

**Exemple de Commande Docker pour des ProblÃ¨mes de Connexion**:
```bash
docker run -d -p 3000:8080 -e HF_ENDPOINT=https://hf-mirror.com/ --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

## ğŸ Podman sur MacOS

Vous exÃ©cutez sur MacOS avec Podman ? Voici comment assurer la connectivitÃ© :

1. **Activer l'AccÃ¨s Boucle Locale de l'HÃ´te**:
   Utilisez `--network slirp4netns:allow_host_loopback=true` dans votre commande.

2. **DÃ©finir OLLAMA_BASE_URL**:
   Assurez-vous qu'il pointe vers `http://host.containers.internal:11434`.

**Exemple de Commande Podman**:
```bash
podman run -d --network slirp4netns:allow_host_loopback=true -p 3000:8080 -e OLLAMA_BASE_URL=http://host.containers.internal:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

