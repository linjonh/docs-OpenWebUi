---
sidebar_position: 3
title: "ğŸ§  RÃ©solution des problÃ¨mes RAG (RÃ©cupÃ©ration-AugmentÃ©e GÃ©nÃ©ration)"
---

La RÃ©cupÃ©ration-AugmentÃ©e GÃ©nÃ©ration (RAG) permet aux modÃ¨les de langage de raisonner sur du contenu externeâ€”documents, bases de connaissances, et plus encoreâ€”en rÃ©cupÃ©rant les informations pertinentes et en les intÃ©grant dans le modÃ¨le. Mais lorsque les choses ne fonctionnent pas comme prÃ©vu (par exemple, le modÃ¨le "hallucine" ou manque des informations pertinentes), ce nest souvent pas la faute du modÃ¨leâ€”cest une problÃ©matique de contexte.

DÃ©composons les causes courantes et les solutions afin damÃ©liorer la prÃ©cision de votre RAG ! ğŸš€

## ProblÃ¨mes courants avec RAG et comment les rÃ©soudre ğŸ› ï¸

### 1. Le modÃ¨le "ne peut pas voir" votre contenu ğŸ‘ï¸âŒ

Cest le problÃ¨me le plus frÃ©quentâ€”et il est gÃ©nÃ©ralement causÃ© par des problÃ¨mes lors du processus dingestion de votre contenu. Le modÃ¨le ne hallucine pas parce quil est erronÃ©, il hallucine parce quil na jamais reÃ§u le contenu correct au dÃ©part.

âœ… Solution : VÃ©rifiez vos paramÃ¨tres dextraction de contenu

- Naviguez vers : **ParamÃ¨tres dadministration > Documents**.
- Assurez-vous dutiliser un moteur dextraction de contenu robuste tel que :
  - Apache Tika
  - Docling
  - Extracteurs personnalisÃ©s (selon vos types de documents)

ğŸ“Œ Astuce : Essayez de tÃ©lÃ©charger un document et prÃ©visualisez le contenu extrait. Sil est vide ou que des sections clÃ©s manquent, vous devez ajuster vos paramÃ¨tres dextracteur ou utiliser un moteur diffÃ©rent.

---

### 2. Seulement une petite partie du document est utilisÃ©e ğŸ“„â¡ï¸âœ‚ï¸

Open WebUI est conÃ§u pour fonctionner avec des modÃ¨les qui ont des fenÃªtres de contexte limitÃ©es par dÃ©faut. Par exemple, de nombreux modÃ¨les locaux (comme les modÃ¨les par dÃ©faut dOllama) sont limitÃ©s Ã  2048 tokens. Pour cette raison, Open WebUI rÃ©duit agressivement le contenu rÃ©cupÃ©rÃ© pour quil tienne dans lespace disponible prÃ©sumÃ©.

âœ… Solutions :

- Allez sur **ParamÃ¨tres dadministration > Documents**
- Soit :
  - ğŸ’¡ Activez â€œContourner lintÃ©gration et la rÃ©cupÃ©rationâ€ â€” Cela envoie le contenu complet directement sans appliquer de filtres stricts de rÃ©cupÃ©ration.
  - ğŸ” Activez le mode â€œContexte completâ€ â€” Cela injecte un contenu plus complet dans linvite du modÃ¨le.

ğŸ“Œ Attention : Soyez conscient des limites de contexteâ€”si votre modÃ¨le ne peut pas gÃ©rer plus de tokens, il sera quand mÃªme coupÃ©.

---

### 3. La limite de tokens est trop courte â³

MÃªme si la rÃ©cupÃ©ration fonctionne, votre modÃ¨le pourrait ne pas traiter tout le contenu quil reÃ§oitâ€”parce quil ne le peut tout simplement pas.

Par dÃ©faut, de nombreux modÃ¨les (en particulier les LLM hÃ©bergÃ©s par Ollama) sont limitÃ©s Ã  une fenÃªtre de contexte de 2048 tokens. Cela signifie quune fraction seulement des donnÃ©es rÃ©cupÃ©rÃ©es sera effectivement utilisÃ©e.

âœ… Solutions :

- ğŸ› ï¸ Augmentez la longueur du contexte du modÃ¨le :
  - AccÃ©dez Ã  l**Ã‰diteur de modÃ¨le ou aux contrÃ´les de chat**
  - Modifiez la longueur du contexte (p. ex., augmentez Ã  8192+ tokens si pris en charge)

â„¹ï¸ Remarque : La limite par dÃ©faut de 2048 tokens est un frein important. Pour de meilleurs rÃ©sultats RAG, nous recommandons dutiliser des modÃ¨les qui prennent en charge des contextes plus longs.

âœ… Alternative : Utilisez un LLM externe avec une capacitÃ© de contexte plus grande

- Essayez GPT-4, GPT-4o, Claude 3, Gemini 1.5, ou Mixtral avec un contexte de 8k+
- Comparez les performances Ã  Ollamaâ€”notez la diffÃ©rence de prÃ©cision quand plus de contenu peut Ãªtre injectÃ© !

ğŸ“Œ Astuce : PrÃ©fÃ©rez les modÃ¨les externes pour de meilleures performances RAG dans les cas dutilisation en production.

---

### 4. Le modÃ¨le dintÃ©gration est de mauvaise qualitÃ© ou inadaptÃ© ğŸ“‰ğŸ§ 

De mauvaises intÃ©grations = une mauvaise rÃ©cupÃ©ration. Si la reprÃ©sentation vectorielle de votre contenu est mÃ©diocre, le rÃ©cupÃ©rateur ne tirera pas le bon contenuâ€”peu importe la puissance de votre LLM.

âœ… Solution :

- Passez Ã  un modÃ¨le dintÃ©gration de haute qualitÃ© (par ex., all-MiniLM-L6-v2, Instructor X, ou les intÃ©grations OpenAI)
- Allez sur : **ParamÃ¨tres dadministration > Documents**
- AprÃ¨s avoir changÃ© le modÃ¨le, assurez-vous de :
  - â³ RÃ©indexer tous les documents existants pour que les nouvelles intÃ©grations prennent effet.

ğŸ“Œ Souvenez-vous : La qualitÃ© des intÃ©grations influence directement le contenu rÃ©cupÃ©rÃ©.

---

### 5. âŒ Erreur 400 : NoneType object has no attribute encode

Cette erreur indique un modÃ¨le dintÃ©gration mal configurÃ© ou manquant. Lorsque Open WebUI tente de crÃ©er des intÃ©grations mais quil na pas de modÃ¨le valide chargÃ©, il ne peut pas traiter le texteâ€”et le rÃ©sultat est cette erreur cryptique.

ğŸ’¥ Cause :
- Votre modÃ¨le dintÃ©gration nest pas correctement configurÃ©.
- Il na peut-Ãªtre pas Ã©tÃ© complÃ¨tement tÃ©lÃ©chargÃ©.
- Ou si vous utilisez un modÃ¨le dintÃ©gration externe, il peut ne pas Ãªtre accessible.

âœ… Solution :

- Allez sur : **ParamÃ¨tres dadministration > Documents > ModÃ¨le dintÃ©gration**
- Enregistrez Ã  nouveau le modÃ¨le dintÃ©grationâ€”mÃªme sil est dÃ©jÃ  sÃ©lectionnÃ©. Cela force une vÃ©rification/tÃ©lÃ©chargement.
- Si vous utilisez un outil dintÃ©gration distant/externe, assurez-vous quil fonctionne et soit accessible Ã  Open WebUI.

ğŸ“Œ Astuce : AprÃ¨s avoir corrigÃ© la configuration, essayez de rÃ©intÃ©grer un document et vÃ©rifiez quaucune erreur napparaÃ®t dans les journaux.

---

## ğŸ§ª Conseil pro : Testez avec GPT-4o ou GPT-4

Si vous nÃªtes pas sÃ»r que le problÃ¨me vienne de la rÃ©cupÃ©ration, des limites de tokens ou de lintÃ©grationâ€”essayez dutiliser temporairement GPT-4o (par exemple via lAPI OpenAI). Si les rÃ©sultats deviennent soudainement plus prÃ©cis, cest un fort indicateur que la limite de contexte de votre modÃ¨le local (2048 par dÃ©faut dans Ollama) est le goulet dÃ©tranglement.

- GPT-4o traite des entrÃ©es plus grandes (128k tokens !)
- Fournit un excellent point de comparaison pour Ã©valuer la fiabilitÃ© RAG de votre systÃ¨me

---

## Liste de vÃ©rification rÃ©capitulative âœ…

| ProblÃ¨me | Solution |
|--------|---------|
| ğŸ¤” Le modÃ¨le ne "lit" pas le contenu | VÃ©rifiez les paramÃ¨tres de l'extracteur de documents |
| ğŸ§¹ Seulement une partie du contenu utilisÃ©e | Activez le mode Contexte Complet ou contournez l'intÃ©gration |
| â± LimitÃ© par un plafond de 2048 tokens | Augmentez la longueur du contexte du modÃ¨le ou utilisez un LLM avec un grand contexte |
| ğŸ“‰ RÃ©cupÃ©ration inexacte | Passez Ã  un modÃ¨le d'intÃ©gration plus performant, puis rÃ©indexez |
| Toujours confus ? | Testez avec GPT-4o et comparez les rÃ©sultats |

---

En optimisant ces domainesâ€”extraction, intÃ©gration, rÃ©cupÃ©ration et contexte du modÃ¨leâ€”vous pouvez amÃ©liorer considÃ©rablement la prÃ©cision de votre LLM dans le traitement de vos documents. Ne laissez pas une fenÃªtre de 2048 tokens ou une chaÃ®ne de rÃ©cupÃ©ration faible limiter la puissance de votre IA ğŸ¯.
