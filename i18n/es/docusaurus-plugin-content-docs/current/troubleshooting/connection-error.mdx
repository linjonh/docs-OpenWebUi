---
sidebar_position: 0
title: "ğŸš§ Problemas de Conectividad del Servidor"
---

Estamos aquÃ­ para ayudarte a configurar todo y que funcione sin problemas. A continuaciÃ³n, encontrarÃ¡s instrucciones paso a paso adaptadas para diferentes escenarios para resolver problemas comunes de conexiÃ³n con Ollama y servidores externos como Hugging Face.

## ğŸŒŸ ConexiÃ³n al Servidor Ollama

### ğŸš€ Accediendo a Ollama desde Open WebUI

Â¿Problemas para conectarte a Ollama desde Open WebUI? PodrÃ­a ser porque Ollama no estÃ¡ escuchando en una interfaz de red que permita conexiones externas. Vamos a solucionarlo:

1. **Configura Ollama para escuchar ampliamente** ğŸ§:
   Establece `OLLAMA_HOST` en `0.0.0.0` para que Ollama escuche en todas las interfaces de red.

2. **Actualiza las variables de entorno**:
   AsegÃºrate de que `OLLAMA_HOST` estÃ© configurado correctamente en tu entorno de implementaciÃ³n.

3. **Reinicia Ollama**ğŸ”„:
   Es necesario un reinicio para que los cambios surtan efecto.

ğŸ’¡ DespuÃ©s de configurarlo, verifica que Ollama sea accesible visitando la interfaz WebUI.

Para obtener instrucciones mÃ¡s detalladas sobre cÃ³mo configurar Ollama, consulta la [DocumentaciÃ³n Oficial de Ollama](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux).

### ğŸ³ Error de conexiÃ³n con Docker

Si ves un error de conexiÃ³n al intentar acceder a Ollama, podrÃ­a ser porque el contenedor de Docker WebUI no puede comunicarse con el servidor Ollama que se ejecuta en tu host. Vamos a solucionarlo:

1. **Ajusta la configuraciÃ³n de red** ğŸ› ï¸:
   Usa la bandera `--network=host` en tu comando Docker. Esto vincula tu contenedor directamente a la red de tu host.

2. **Cambia el puerto**:
   Recuerda que el puerto interno cambia de 3000 a 8080.

**Ejemplo de Comando Docker**:
```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
ğŸ”— DespuÃ©s de ejecutar el comando anterior, tu WebUI deberÃ­a estar disponible en `http://localhost:8080`.

## ğŸ”’ Problema de ConexiÃ³n SSL con Hugging Face

Â¿Has encontrado un error SSL? PodrÃ­a ser un problema con el servidor de Hugging Face. AquÃ­ tienes quÃ© hacer:

1. **Verifica el estado del servidor Hugging Face**:
   Comprueba si hay un problema o interrupciÃ³n conocida en su sistema.

2. **Cambia de punto de conexiÃ³n**:
   Si Hugging Face estÃ¡ caÃ­do, cambia el punto de conexiÃ³n en tu comando Docker.

**Ejemplo de Comando Docker para problemas de conexiÃ³n**:
```bash
docker run -d -p 3000:8080 -e HF_ENDPOINT=https://hf-mirror.com/ --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

## ğŸ Podman en MacOS

Â¿EstÃ¡s trabajando en MacOS con Podman? AquÃ­ tienes cÃ³mo garantizar la conectividad:

1. **Habilita el loopback del host**:
   Usa `--network slirp4netns:allow_host_loopback=true` en tu comando.

2. **Establece OLLAMA_BASE_URL**:
   AsegÃºrate de que apunte a `http://host.containers.internal:11434`.

**Ejemplo de Comando Podman**:
```bash
podman run -d --network slirp4netns:allow_host_loopback=true -p 3000:8080 -e OLLAMA_BASE_URL=http://host.containers.internal:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

