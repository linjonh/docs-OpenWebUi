---
sidebar_position: 400
title: "üîó Puntos Finales de API"
---

Esta gu√≠a proporciona informaci√≥n esencial sobre c√≥mo interactuar efectivamente con los puntos finales de la API para lograr una integraci√≥n y automatizaci√≥n sin problemas utilizando nuestros modelos. Tenga en cuenta que este es un entorno experimental y puede estar sujeto a futuras actualizaciones para mejoras.

## Autenticaci√≥n

Para garantizar un acceso seguro a la API, se requiere autenticaci√≥n üõ°Ô∏è. Puede autenticar sus solicitudes de API utilizando el mecanismo Bearer Token. Obtenga su clave de API desde **Configuraci√≥n > Cuenta** en la interfaz web abierta, o alternativamente, use un JWT (Token Web JSON) para la autenticaci√≥n.

## Puntos Finales Destacados de la API

### üìú Recuperar Todos los Modelos

- **Punto Final**: `GET /api/models`
- **Descripci√≥n**: Recupera todos los modelos creados o agregados mediante la interfaz web abierta.
- **Ejemplo**:

  ```bash
  curl -H "Authorization: Bearer YOUR_API_KEY" http://localhost:3000/api/models
  ```

### üí¨ Completaciones de Chat

- **Punto Final**: `POST /api/chat/completions`
- **Descripci√≥n**: Act√∫a como un punto final de completaci√≥n de chat compatible con la API de OpenAI para modelos en la interfaz web abierta, incluidos los modelos de Ollama, modelos de OpenAI y modelos de Funci√≥n de Open WebUI.

- **Ejemplo Curl**:

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "llama3.1",
        "messages": [
          {
            "role": "user",
            "content": "¬øPor qu√© el cielo es azul?"
          }
        ]
      }
  ```
  
- **Ejemplo Python**:
  ```python
  import requests
  
  def chat_with_model(token):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      data = {
        "model": "granite3.1-dense:8b",
        "messages": [
          {
            "role": "user",
            "content": "¬øPor qu√© el cielo es azul?"
          }
        ]
      }
      response = requests.post(url, headers=headers, json=data)
      return response.json()
  ```

### ü¶ô Soporte Proxy de API de Ollama

Si desea interactuar directamente con modelos de Ollama, incluidos los casos de generaci√≥n de incrustaciones o transmisi√≥n de prompts sin procesar, Open WebUI ofrece un paso transparente a la API nativa de Ollama mediante una ruta proxy.

- **URL Base**: `/ollama/<api>`
- **Referencia**: [Documentaci√≥n de API de Ollama](https://github.com/ollama/ollama/blob/main/docs/api.md)

#### üîÅ Generar Completaci√≥n (Streaming)

```bash
curl http://localhost:3000/ollama/api/generate -d {
  "model": "llama3.2",
  "prompt": "¬øPor qu√© el cielo es azul?"
}
```

#### üì¶ Listar Modelos Disponibles

```bash
curl http://localhost:3000/ollama/api/tags
```

#### üß† Generar Incrustaciones

```bash
curl -X POST http://localhost:3000/ollama/api/embed -d {
  "model": "llama3.2",
  "input": ["Open WebUI es genial!", "Vamos a generar incrustaciones."]
}
```

Esto es ideal para construir √≠ndices de b√∫squeda, sistemas de recuperaci√≥n o tuber√≠as personalizadas utilizando modelos de Ollama detr√°s de la interfaz web abierta.

### üß© Generaci√≥n Aumentada por Recuperaci√≥n (RAG)

La funci√≥n Generaci√≥n Aumentada por Recuperaci√≥n (RAG) le permite mejorar las respuestas incorporando datos de fuentes externas. A continuaci√≥n, encontrar√° los m√©todos para gestionar archivos y colecciones de conocimientos mediante la API, y c√≥mo usarlos eficazmente en las completaciones de chat.

#### Carga de Archivos

Para utilizar datos externos en respuestas RAG, primero debe cargar los archivos. El contenido del archivo cargado se extrae autom√°ticamente y se almacena en una base de datos vectorial.

- **Punto Final**: `POST /api/v1/files/`
- **Ejemplo Curl**:

  ```bash
  curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Accept: application/json" \
  -F "file=@/path/to/your/file" http://localhost:3000/api/v1/files/
  ```

- **Ejemplo Python**:

  ```python
  import requests
  
  def upload_file(token, file_path):
      url = http://localhost:3000/api/v1/files/
      headers = {
          Authorization: fBearer {token},
          Accept: application/json
      }
      files = {file: open(file_path, rb)}
      response = requests.post(url, headers=headers, files=files)
      return response.json()
  ```

#### Agregar Archivos a Colecciones de Conocimientos

Despu√©s de cargar, puede agrupar archivos en una colecci√≥n de conocimientos o referenciarlos individualmente en chats.

- **Punto Final**: `POST /api/v1/knowledge/{id}/file/add`
- **Ejemplo Curl**:

  ```bash
  curl -X POST http://localhost:3000/api/v1/knowledge/{knowledge_id}/file/add \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {"file_id": "your-file-id-here"}
  ```

- **Ejemplo Python**:

  ```python
  import requests

  def add_file_to_knowledge(token, knowledge_id, file_id):
      url = fhttp://localhost:3000/api/v1/knowledge/{knowledge_id}/file/add
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      data = {file_id: file_id}
      response = requests.post(url, headers=headers, json=data)
      return response.json()
  ```

#### Uso de archivos y colecciones en finalizaciones de chat

Puedes referenciar tanto archivos individuales como colecciones completas en tus consultas RAG para obtener respuestas enriquecidas.

##### Utilizaci√≥n de un archivo individual en finalizaciones de chat

Este m√©todo es beneficioso cuando deseas enfocar la respuesta del modelo de chat en el contenido de un archivo espec√≠fico.

- **Endpoint**: `POST /api/chat/completions`
- **Ejemplo en Curl**:

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "gpt-4-turbo",
        "messages": [
          {"role": "user", "content": "Explica los conceptos en este documento."}
        ],
        "files": [
          {"type": "file", "id": "your-file-id-here"}
        ]
      }
  ```

- **Ejemplo en Python**:

  ```python
  import requests

  def chat_with_file(token, model, query, file_id):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      payload = {
          model: model,
          messages: [{role: user, content: query}],
          files: [{type: file, id: file_id}]
      }
      response = requests.post(url, headers=headers, json=payload)
      return response.json()
  ```

##### Utilizaci√≥n de una colecci√≥n de conocimiento en finalizaciones de chat

Aprovecha una colecci√≥n de conocimiento para mejorar la respuesta cuando la consulta pueda beneficiarse de un contexto m√°s amplio o de m√∫ltiples documentos.

- **Endpoint**: `POST /api/chat/completions`
- **Ejemplo en Curl**:

  ```bash
  curl -X POST http://localhost:3000/api/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d {
        "model": "gpt-4-turbo",
        "messages": [
          {"role": "user", "content": "Proporciona informaci√≥n sobre las perspectivas hist√≥ricas cubiertas en la colecci√≥n."}
        ],
        "files": [
          {"type": "collection", "id": "your-collection-id-here"}
        ]
      }
  ```

- **Ejemplo en Python**:

  ```python
  import requests
  
  def chat_with_collection(token, model, query, collection_id):
      url = http://localhost:3000/api/chat/completions
      headers = {
          Authorization: fBearer {token},
          Content-Type: application/json
      }
      payload = {
          model: model,
          messages: [{role: user, content: query}],
          files: [{type: collection, id: collection_id}]
      }
      response = requests.post(url, headers=headers, json=payload)
      return response.json()
  ```

Estos m√©todos permiten la utilizaci√≥n efectiva de conocimientos externos mediante archivos subidos y colecciones de conocimiento, mejorando las capacidades de las aplicaciones de chat mediante la API de Open WebUI. Ya sea utilizando archivos individualmente o dentro de colecciones, puedes personalizar la integraci√≥n seg√∫n tus necesidades espec√≠ficas.

## Ventajas de usar Open WebUI como un proveedor unificado de LLM

Open WebUI ofrece una multitud de beneficios, convirti√©ndose en una herramienta esencial para desarrolladores y empresas por igual:

- **Interfaz Unificada**: Simplifica tus interacciones con diferentes LLMs a trav√©s de una plataforma √∫nica e integrada.
- **Facilidad de Implementaci√≥n**: Integraci√≥n r√°pida con documentaci√≥n completa y soporte comunitario.

## Enlaces de documentaci√≥n Swagger

:::importante
Aseg√∫rate de configurar la variable de entorno `ENV` en `dev` para acceder a la documentaci√≥n Swagger de cualquiera de estos servicios. Sin esta configuraci√≥n, la documentaci√≥n no estar√° disponible.
:::

Accede a la documentaci√≥n detallada de la API para los diferentes servicios proporcionados por Open WebUI:

| Aplicaci√≥n | Ruta de la Documentaci√≥n |
|------------|-------------------------|
| Principal  | `/docs`                 |


Siguiendo estas pautas, puedes integrar y comenzar a utilizar la API de Open WebUI r√°pidamente. Si encuentras alg√∫n problema o tienes preguntas, no dudes en comunicarte a trav√©s de nuestra Comunidad de Discord o consultar las preguntas frecuentes. ¬°Feliz programaci√≥n! üåü
