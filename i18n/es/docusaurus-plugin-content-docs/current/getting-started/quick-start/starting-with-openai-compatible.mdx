---

sidebar_position: 4  
title: "üåê Comenzando con servidores compatibles con OpenAI"

---

## Resumen

Open WebUI no es solo para OpenAI/Ollama/Llama.cpp: puedes conectar **cualquier servidor que implemente la API compatible con OpenAI**, ya sea localmente o de forma remota. Esto es perfecto si deseas ejecutar diferentes modelos de lenguaje o si ya tienes un backend o ecosistema favorito. Esta gu√≠a te mostrar√° c√≥mo:

- Configurar un servidor compatible con OpenAI (con algunas opciones populares)
- Conectarlo a Open WebUI
- Comenzar a chatear de inmediato

## Paso 1: Elige un Servidor Compatible con OpenAI

Existen muchos servidores y herramientas que exponen una API compatible con OpenAI. Aqu√≠ hay algunos de los m√°s populares:

- [Llama.cpp](https://github.com/ggml-org/llama.cpp): Extremadamente eficiente, funciona en CPU y GPU
- [Ollama](https://ollama.com/): Muy f√°cil de usar y multiplataforma
- [LM Studio](https://lmstudio.ai/): Aplicaci√≥n de escritorio rica en funciones para Windows/Mac/Linux
- [Lemonade (ONNX TurnkeyML)](https://github.com/onnx/turnkeyml): Backend r√°pido basado en ONNX con aceleraci√≥n NPU/iGPU

¬°Elige el que m√°s se adapte a tu flujo de trabajo!

---

#### üçã Empieza con Lemonade (ONNX TurnkeyML)

Lemonade es un servidor compatible con OpenAI basado en ONNX, f√°cil de configurar. As√≠ es como puedes probarlo en Windows:

1. [Descarga el √∫ltimo `.exe`](https://github.com/onnx/turnkeyml/releases)
2. Ejecuta `Lemonade_Server_Installer.exe`
3. Instala y descarga un modelo usando el instalador de Lemonade
4. Una vez en operaci√≥n, tu punto final de API ser√°:

   ```
   http://localhost:8000/api/v0
   ```

![Lemonade Server](/images/getting-started/lemonade-server.png)

Consulta [su documentaci√≥n](https://github.com/onnx/turnkeyml) para m√°s detalles.

---

## Paso 2: Conecta tu Servidor a Open WebUI

1. Abre Open WebUI en tu navegador.
2. Ve a ‚öôÔ∏è **Configuraci√≥n de Administrador** ‚Üí **Conexiones** ‚Üí **Conexiones OpenAI**.
3. Haz clic en ‚ûï **A√±adir Conexi√≥n**.

   - **URL**: Usa el punto final de API de tu servidor (por ejemplo, `http://localhost:11434/v1` para Ollama, o la direcci√≥n del servidor de tu propio Llama.cpp).
   - **Clave API**: D√©jalo en blanco a menos que sea requerido.

4. Haz clic en Guardar.

*Consejo: Si ejecutas Open WebUI en Docker y tu servidor de modelos en tu m√°quina anfitri√≥n, usa `http://host.docker.internal:<tu-puerto>/v1`.*

##### **Para Lemonade:**  Al a√±adir Lemonade, usa `http://localhost:8000/api/v0` como la URL.

![Lemonade Connection](/images/getting-started/lemonade-connection.png)

---

## Paso 3: ¬°Empieza a chatear!

¬°Selecciona el modelo de tu servidor conectado en el men√∫ de chat y comienza!

¬°Eso es todo! Ya sea que elijas Llama.cpp, Ollama, LM Studio o Lemonade, puedes experimentar y gestionar f√°cilmente m√∫ltiples servidores de modelos, todo dentro de Open WebUI.

--- 

üöÄ ¬°Disfruta construyendo tu configuraci√≥n local de IA perfecta!