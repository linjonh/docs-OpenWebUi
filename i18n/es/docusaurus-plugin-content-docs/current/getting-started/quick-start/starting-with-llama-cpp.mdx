---
sidebar_position: 3
title: "ü¶ôComenzando con Llama.cpp"
---

## Resumen

Open WebUI lo hace simple y flexible para conectar y gestionar un servidor local de Llama.cpp para ejecutar modelos de lenguaje eficientes y cuantificados. Ya sea que hayas compilado Llama.cpp t√∫ mismo o que est√©s usando binarios precompilados, esta gu√≠a te guiar√° a trav√©s de c√≥mo:

- Configurar tu servidor Llama.cpp
- Cargar grandes modelos localmente
- Integrar con Open WebUI para una interfaz fluida

¬°Empecemos!

---

## Paso 1: Instalar Llama.cpp

Para ejecutar modelos con Llama.cpp, primero necesitas instalar el servidor Llama.cpp localmente.

Puedes:

- üì¶ [Descargar binarios precompilados](https://github.com/ggerganov/llama.cpp/releases)
- üõ†Ô∏è O compilarlo desde el c√≥digo fuente siguiendo las [instrucciones oficiales de compilaci√≥n](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)

Despu√©s de instalar, aseg√∫rate de que `llama-server` est√© disponible en tu ruta del sistema local o toma nota de su ubicaci√≥n.

---

## Paso 2: Descargar un Modelo Compatible

Puedes cargar y ejecutar varios modelos LLM cuantificados en formato GGUF usando Llama.cpp. Un ejemplo impresionante es el modelo DeepSeek-R1 optimizado a 1.58 bits por UnslothAI. Para descargar esta versi√≥n:

1. Visita el [repositorio Unsloth DeepSeek-R1 en Hugging Face](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)
2. Descarga la versi√≥n cuantificada de 1.58 bits (aproximadamente 131GB).

Alternativamente, usa Python para descargarlo program√°ticamente:

```python
# pip install huggingface_hub hf_transfer

from huggingface_hub import snapshot_download

snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",
    local_dir = "DeepSeek-R1-GGUF",
    allow_patterns = ["*UD-IQ1_S*"],  # Descargar solo la variante de 1.58 bits
)
```

Esto descargar√° los archivos del modelo en un directorio como:
```
DeepSeek-R1-GGUF/
‚îî‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/
    ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
    ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf
    ‚îî‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf
```

üìç Toma nota de la ruta completa al primer archivo GGUF, lo necesitar√°s en el Paso 3.

---

## Paso 3: Servir el Modelo con Llama.cpp

Inicia el servidor del modelo usando el binario llama-server. Navega a tu carpeta llama.cpp (por ejemplo, build/bin) y ejecuta:

```bash
./llama-server \
  --model /ruta/completa/al/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
  --port 10000 \
  --ctx-size 1024 \
  --n-gpu-layers 40
```

üõ†Ô∏è Ajusta los par√°metros para adaptarlos a tu m√°quina:

- --model: Ruta al archivo .gguf del modelo
- --port: 10000 (o elige otro puerto abierto)
- --ctx-size: Longitud del contexto de tokens (puede aumentar si la RAM lo permite)
- --n-gpu-layers: Capas transferidas a la GPU para un rendimiento m√°s r√°pido

Una vez que el servidor est√© en ejecuci√≥n, expondr√° una API local compatible con OpenAI en:

```
http://127.0.0.1:10000
```

---

## Paso 4: Conectar Llama.cpp a Open WebUI

Para controlar y consultar tu modelo ejecut√°ndose localmente directamente desde Open WebUI:

1. Abre Open WebUI en tu navegador
2. Ve a ‚öôÔ∏è Configuraci√≥n de Administraci√≥n ‚Üí Conexiones ‚Üí Conexiones OpenAI
3. Haz clic en ‚ûï Agregar conexi√≥n e ingresa:

- URL: `http://127.0.0.1:10000/v1`  
  (O usa `http://host.docker.internal:10000/v1` si est√°s ejecutando WebUI dentro de Docker)
- API Key: `none` (d√©jalo en blanco)

üí° Una vez guardado, Open WebUI comenzar√° a usar tu servidor local de Llama.cpp como backend.

![Conexi√≥n de Llama.cpp en Open WebUI](/images/tutorials/deepseek/connection.png)

---

## Consejo R√°pido: Prueba el Modelo a trav√©s de la Interfaz de Chat

Una vez conectado, selecciona el modelo desde el men√∫ de chat de Open WebUI y comienza a interactuar.

![Vista previa del chat del modelo](/images/tutorials/deepseek/response.png)

---

## ¬°Est√°s Listo para Comenzar!

Una vez configurado, Open WebUI hace que sea f√°cil:

- Gestionar y alternar entre modelos locales servidos por Llama.cpp
- Usar la API compatible con OpenAI sin necesidad de clave
- Experimentar con modelos masivos como DeepSeek-R1, ¬°directamente desde tu m√°quina!

---

üöÄ ¬°Divi√©rtete experimentando y construyendo!