---
sidebar_position: 2
title: "üó®Ô∏è Usando Docker con Openedai-speech"
---

:::warning
Este tutorial es una contribuci√≥n comunitaria y no est√° respaldado por el equipo de Open WebUI. Solo sirve como demostraci√≥n de c√≥mo personalizar Open WebUI para su caso de uso espec√≠fico. ¬øDeseas contribuir? Consulta el tutorial de contribuci√≥n.
:::

**Integrando `openedai-speech` en Open WebUI usando Docker**
============================================================

**¬øQu√© es `openedai-speech`?**
-----------------------------

:::info
[openedai-speech](https://github.com/matatonic/openedai-speech) es un servidor de texto a voz compatible con la API de audio/voz de OpenAI.

Sirve el endpoint `/v1/audio/speech` y ofrece una experiencia de texto a voz gratuita y privada con capacidades personalizadas de clonaci√≥n de voz. Este servicio no est√° afiliado de ninguna manera a OpenAI y no requiere una clave de API de OpenAI.
:::

**Requisitos**
--------------

* Docker instalado en tu sistema
* Open WebUI ejecut√°ndose en un contenedor Docker
* Conocimientos b√°sicos sobre Docker y Docker Compose

**Opci√≥n 1: Usar Docker Compose**
--------------------------------

**Paso 1: Crea una nueva carpeta para el servicio `openedai-speech`**
--------------------------------------------------------------------

Crea una nueva carpeta, por ejemplo, `openedai-speech-service`, para almacenar los archivos `docker-compose.yml` y `speech.env`.

**Paso 2: Clona el repositorio de `openedai-speech` desde GitHub**
-----------------------------------------------------------------

```bash
git clone https://github.com/matatonic/openedai-speech.git
```

Esto descargar√° el repositorio `openedai-speech` en tu m√°quina local, que incluye los archivos de Docker Compose (`docker-compose.yml`, `docker-compose.min.yml` y `docker-compose.rocm.yml`) y otros archivos necesarios.

**Paso 3: Renombra el archivo `sample.env` a `speech.env` (personaliza si es necesario)**
------------------------------------------------------------------------------------------

En la carpeta del repositorio `openedai-speech`, crea un nuevo archivo llamado `speech.env` con el siguiente contenido:

```yaml
TTS_HOME=voices
HF_HOME=voices
#PRELOAD_MODEL=xtts
#PRELOAD_MODEL=xtts_v2.0.2
#PRELOAD_MODEL=parler-tts/parler_tts_mini_v0.1
#EXTRA_ARGS=--log-level DEBUG --unload-timer 300
#USE_ROCM=1
```

**Paso 4: Elige un archivo de Docker Compose**
---------------------------------------------

Puedes usar cualquiera de los siguientes archivos de Docker Compose:

* [docker-compose.yml](https://github.com/matatonic/openedai-speech/blob/main/docker-compose.yml): Este archivo utiliza la imagen `ghcr.io/matatonic/openedai-speech` y se construye desde el [Dockerfile](https://github.com/matatonic/openedai-speech/blob/main/Dockerfile).
* [docker-compose.min.yml](https://github.com/matatonic/openedai-speech/blob/main/docker-compose.min.yml): Este archivo utiliza la imagen `ghcr.io/matatonic/openedai-speech-min` y se construye desde [Dockerfile.min](https://github.com/matatonic/openedai-speech/blob/main/Dockerfile.min).
  Esta imagen es una versi√≥n m√≠nima que solo incluye soporte para Piper y no requiere una GPU.
* [docker-compose.rocm.yml](https://github.com/matatonic/openedai-speech/blob/main/docker-compose.rocm.yml): Este archivo utiliza la imagen `ghcr.io/matatonic/openedai-speech-rocm` y se construye desde [Dockerfile](https://github.com/matatonic/openedai-speech/blob/main/Dockerfile) con soporte ROCm.

**Paso 4: Construye la imagen de Docker elegida**
------------------------------------------------

Antes de ejecutar el archivo de Docker Compose, necesitas construir la imagen de Docker:

* **GPU Nvidia (soporte CUDA)**:

```bash
docker build -t ghcr.io/matatonic/openedai-speech .
```

* **GPU AMD (soporte ROCm)**:

```bash
docker build -f Dockerfile --build-arg USE_ROCM=1 -t ghcr.io/matatonic/openedai-speech-rocm .
```

* **Solo CPU, Sin GPU (solo Piper)**:

```bash
docker build -f Dockerfile.min -t ghcr.io/matatonic/openedai-speech-min .
```

**Paso 5: Ejecuta el comando correcto `docker compose up -d`**
-------------------------------------------------------------

* **GPU Nvidia (soporte CUDA)**: Ejecuta el siguiente comando para iniciar el servicio `openedai-speech` en modo desacoplado:

```bash
docker compose up -d
```

* **GPU AMD (soporte ROCm)**: Ejecuta el siguiente comando para iniciar el servicio `openedai-speech` en modo desacoplado:

```bash
docker compose -f docker-compose.rocm.yml up -d
```

* **ARM64 (Serie M de Apple, Raspberry Pi)**: XTTS solo tiene soporte para CPU aqu√≠ y ser√° muy lento. Puedes usar la imagen Nvidia para XTTS con CPU (lenta) o usar la imagen solo Piper (recomendada):

```bash
docker compose -f docker-compose.min.yml up -d
```

* **Solo CPU, Sin GPU (solo Piper)**: Para una imagen m√≠nima de Docker con solo soporte para Piper (< 1GB vs. 8GB):

```bash
docker compose -f docker-compose.min.yml up -d
```

Esto iniciar√° el servicio `openedai-speech` en modo desacoplado.

**Opci√≥n 2: Usar comandos Docker Run**
-----------------------------------

Tambi√©n puedes usar los siguientes comandos de ejecuci√≥n de Docker para iniciar el servicio `openedai-speech` en modo separado:

* **Nvidia GPU (CUDA)**: Ejecuta el siguiente comando para construir e iniciar el servicio `openedai-speech`:

```bash
docker build -t ghcr.io/matatonic/openedai-speech .
docker run -d --gpus=all -p 8000:8000 -v voices:/app/voices -v config:/app/config --name openedai-speech ghcr.io/matatonic/openedai-speech
```

* **ROCm (AMD GPU)**: Ejecuta el siguiente comando para construir e iniciar el servicio `openedai-speech`:

> Para habilitar el soporte de ROCm, descomenta la l√≠nea `#USE_ROCM=1` en el archivo `speech.env`.

```bash
docker build -f Dockerfile --build-arg USE_ROCM=1 -t ghcr.io/matatonic/openedai-speech-rocm .
docker run -d --privileged --init --name openedai-speech -p 8000:8000 -v voices:/app/voices -v config:/app/config ghcr.io/matatonic/openedai-speech-rocm
```

* **Solo CPU, sin GPU (Solo Piper)**: Ejecuta el siguiente comando para construir e iniciar el servicio `openedai-speech`:

```bash
docker build -f Dockerfile.min -t ghcr.io/matatonic/openedai-speech-min .
docker run -d -p 8000:8000 -v voices:/app/voices -v config:/app/config --name openedai-speech ghcr.io/matatonic/openedai-speech-min
```

**Paso 6: Configurar Open WebUI para usar `openedai-speech` para TTS**
---------------------------------------------------------

![openedai-tts](https://github.com/silentoplayz/docs/assets/50341825/ea08494f-2ebf-41a2-bb0f-9b48dd3ace79)

Abre la configuraci√≥n de Open WebUI y navega a la configuraci√≥n de TTS bajo **Panel de Administraci√≥n > Configuraci√≥n > Audio**. Agrega la siguiente configuraci√≥n:

* **API Base URL**: `http://host.docker.internal:8000/v1`
* **API Key**: `sk-111111111` (Ten en cuenta que esta es una clave API de prueba, ya que `openedai-speech` no requiere una clave API. Puedes usar cualquier valor para este campo, siempre y cuando sea llenado.)

**Paso 7: Elegir una voz**
--------------------------

En `TTS Voice` dentro del mismo men√∫ de configuraci√≥n de audio en el panel de administraci√≥n, puedes configurar el `TTS Model` para usar a partir de las siguientes opciones que soporta `openedai-speech`. Las voces de estos modelos est√°n optimizadas para el idioma ingl√©s.

* `tts-1` o `tts-1-hd`: `alloy`, `echo`, `echo-alt`, `fable`, `onyx`, `nova`, y `shimmer` (`tts-1-hd` es configurable; usa muestras de OpenAI por defecto)

**Paso 8: Presiona `Save` para aplicar los cambios y empezar a disfrutar voces naturales**
--------------------------------------------------------------------------------------------

Presiona el bot√≥n `Save` para aplicar los cambios a la configuraci√≥n de Open WebUI. Actualiza la p√°gina para que el cambio surta efecto por completo y disfruta de la integraci√≥n de `openedai-speech` dentro de Open WebUI para leer en voz alta respuestas de texto con texto a voz en una voz de sonido natural.

**Detalles del modelo:**
------------------

`openedai-speech` soporta m√∫ltiples modelos de texto a voz, cada uno con sus propias fortalezas y requisitos. Los siguientes modelos est√°n disponibles:

* **Piper TTS** (muy r√°pido, corre en CPU): Usa tus propias [voces Piper](https://rhasspy.github.io/piper-samples/) v√≠a el archivo de configuraci√≥n `voice_to_speaker.yaml`. Este modelo es excelente para aplicaciones que requieren baja latencia y alto rendimiento. Piper TTS tambi√©n soporta voces [multiling√ºes](https://github.com/matatonic/openedai-speech#multilingual).
* **Coqui AI/TTS XTTS v2** (r√°pido, pero requiere aproximadamente 4GB de VRAM GPU y GPU Nvidia con CUDA): Este modelo utiliza la tecnolog√≠a de clonaci√≥n de voz XTTS v2 de Coqui AI para generar voces de alta calidad. Aunque requiere una GPU m√°s potente, proporciona excelente rendimiento y audio de alta calidad. Coqui tambi√©n soporta voces [multiling√ºes](https://github.com/matatonic/openedai-speech#multilingual).
* **Soporte Beta Parler-TTS** (experimental, m√°s lento): Este modelo utiliza el marco Parler-TTS para generar voces. Aunque est√° actualmente en beta, permite describir caracter√≠sticas muy b√°sicas de la voz del hablante. La voz exacta ser√° ligeramente diferente en cada generaci√≥n, pero deber√≠a ser similar a la descripci√≥n del hablante proporcionada. Para inspiraci√≥n sobre c√≥mo describir voces, consulta [Text Description to Speech](https://www.text-description-to-speech.com/).

**Soluci√≥n de problemas**
-------------------

Si encuentras alg√∫n problema al integrar `openedai-speech` con Open WebUI, sigue estos pasos de soluci√≥n de problemas:

* **Verifica el servicio `openedai-speech`**: Aseg√∫rate de que el servicio `openedai-speech` est√© en ejecuci√≥n y que el puerto que especificaste en el archivo docker-compose.yml est√© expuesto.
* **Comprueba el acceso a host.docker.internal**: Verifica que el nombre de host `host.docker.internal` sea resolvible desde dentro del contenedor de Open WebUI. Esto es necesario porque `openedai-speech` est√° expuesto a trav√©s de `localhost` en tu PC, pero `open-webui` normalmente no puede acceder a √©l desde dentro de su contenedor. Puedes agregar un volumen al archivo docker-compose.yml para montar un archivo del host al contenedor, por ejemplo, a un directorio que ser√° servido por openedai-speech.
* **Revisar la configuraci√≥n de la clave API**: Aseg√∫rate de que la clave API est√© configurada con un valor de prueba o efectivamente sin marcar porque `openedai-speech` no requiere una clave API.
* **Verificar configuraci√≥n de voz**: Comprueba que la voz que est√°s intentando usar para TTS exista en tu archivo `voice_to_speaker.yaml` y que los archivos correspondientes (por ejemplo, archivos XML de voz) est√©n presentes en el directorio correcto.
* **Verificar rutas del modelo de voz**: Si experimentas problemas al cargar el modelo de voz, verifica que las rutas en tu archivo `voice_to_speaker.yaml` coincidan con las ubicaciones reales de tus modelos de voz.

**Consejos Adicionales para Solucionar Problemas**
------------------------------------

* Revisa los registros de openedai-speech en busca de errores o advertencias que puedan indicar d√≥nde est√° el problema.
* Verifica que el archivo `docker-compose.yml` est√© correctamente configurado para tu entorno.
* Si a√∫n tienes problemas, intenta reiniciar el servicio `openedai-speech` o todo el entorno Docker.
* Si el problema persiste, consulta el repositorio de GitHub de `openedai-speech` o busca ayuda en un foro comunitario relevante.

**FAQ**
-------

**¬øC√≥mo puedo controlar el rango emocional del audio generado?**

No existe un mecanismo directo para controlar la salida emocional del audio generado. Factores como la capitalizaci√≥n o la gram√°tica pueden afectar el audio de salida, pero las pruebas internas han producido resultados mixtos.

**¬øD√≥nde se almacenan los archivos de voz? ¬øQu√© hay del archivo de configuraci√≥n?**.

Los archivos de configuraci√≥n, que definen las voces disponibles y sus propiedades, est√°n almacenados en el volumen de configuraci√≥n. Espec√≠ficamente, las voces predeterminadas est√°n definidas en voice_to_speaker.default.yaml.

**Recursos Adicionales**
------------------------

Para m√°s informaci√≥n sobre c√≥mo configurar Open WebUI para usar `openedai-speech`, incluyendo la configuraci√≥n de las variables de entorno, consulta la [documentaci√≥n de Open WebUI](https://docs.openwebui.com/getting-started/env-configuration#text-to-speech).

Para m√°s informaci√≥n sobre `openedai-speech`, visita el [repositorio de GitHub](https://github.com/matatonic/openedai-speech).

**C√≥mo a√±adir m√°s voces a openedai-speech:**
[Custom-Voices-HowTo](https://github.com/matatonic/openedai-speech?tab=readme-ov-file#custom-voices-howto)

:::note
Puedes cambiar el n√∫mero de puerto en el archivo `docker-compose.yml` por cualquier puerto abierto y utilizable, pero aseg√∫rate de actualizar la **URL Base API** en la configuraci√≥n de Audio del Administrador de Open WebUI en consecuencia.
:::
