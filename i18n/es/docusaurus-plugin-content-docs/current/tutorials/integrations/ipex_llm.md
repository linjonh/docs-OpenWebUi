---
sidebar_position: 11
title: "üñ•Ô∏è Configuraci√≥n local de LLM con IPEX-LLM en GPU de Intel"
---

:::warning
Este tutorial es una contribuci√≥n de la comunidad y no cuenta con el soporte del equipo de Open WebUI. Solo sirve como demostraci√≥n de c√≥mo personalizar Open WebUI para su caso de uso espec√≠fico. ¬øQuieres contribuir? Consulta el tutorial de contribuci√≥n.
:::

:::note
Esta gu√≠a ha sido verificada con la configuraci√≥n de Open WebUI mediante [Instalaci√≥n Manual](/getting-started/index.md).
:::

# Configuraci√≥n local de LLM con IPEX-LLM en GPU de Intel

:::info
[**IPEX-LLM**](https://github.com/intel-analytics/ipex-llm) es una biblioteca de PyTorch para ejecutar LLM en CPU y GPU de Intel (por ejemplo, PC local con iGPU, GPU discreta como Arc A-Series, Flex y Max) con una latencia muy baja.
:::

Este tutorial demuestra c√≥mo configurar Open WebUI con **backend Ollama acelerado por IPEX-LLM alojado en GPU de Intel**. Siguiendo esta gu√≠a, podr√° configurar Open WebUI incluso en una PC de bajo costo (es decir, solo con GPU integrada) con una experiencia fluida.

## Inicie Ollama Serve en GPU de Intel

Consulte [esta gu√≠a](https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html) de la documentaci√≥n oficial de IPEX-LLM sobre c√≥mo instalar y ejecutar Ollama serve acelerado por IPEX-LLM en GPU de Intel.

:::tip
Si desea acceder al servicio Ollama desde otra m√°quina, aseg√∫rese de configurar o exportar la variable de entorno `OLLAMA_HOST=0.0.0.0` antes de ejecutar el comando `ollama serve`.
:::

## Configure Open WebUI

Acceda a la configuraci√≥n de Ollama a trav√©s de **Settings -> Connections** en el men√∫. Por defecto, la **URL Base de Ollama** est√° configurada como `https://localhost:11434`, como se ilustra en la imagen a continuaci√≥n. Para verificar el estado de la conexi√≥n al servicio Ollama, haga clic en el **bot√≥n de refrescar** ubicado junto al cuadro de texto. Si el WebUI no puede establecer una conexi√≥n con el servidor Ollama, ver√° un mensaje de error que indica, `WebUI no pudo conectarse a Ollama`.

![Error de configuraci√≥n de Open WebUI Ollama](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings_0.png)

Si la conexi√≥n es exitosa, ver√° un mensaje indicando `Conexi√≥n al servicio verificada`, como se ilustra a continuaci√≥n.

![√âxito en la configuraci√≥n de Open WebUI Ollama](https://llm-assets.readthedocs.io/en/latest/_images/open_webui_settings.png)

:::tip
Si desea utilizar un servidor Ollama alojado en una URL diferente, simplemente actualice la **URL Base de Ollama** a la nueva URL y presione el **bot√≥n de refrescar** para volver a confirmar la conexi√≥n con Ollama.
:::
