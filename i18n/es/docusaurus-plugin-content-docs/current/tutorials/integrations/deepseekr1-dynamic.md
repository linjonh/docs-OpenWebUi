---
sidebar_position: 1
title: "üêã Ejecutar DeepSeek R1 Din√°mico 1.58-bit con Llama.cpp"
---

¬°Un enorme agradecimiento a **UnslothAI** por sus incre√≠bles esfuerzos! Gracias a su arduo trabajo, ahora podemos ejecutar el modelo completo **DeepSeek-R1** con 671 mil millones de par√°metros en su forma din√°mica de 1.58-bit cuantificada (comprimido a solo 131GB) en **Llama.cpp**. ¬øLa mejor parte? Ya no tienes que desesperarte por necesitar GPU de clase empresarial o servidores masivos: es posible ejecutar este modelo en tu m√°quina personal (aunque de manera lenta en la mayor√≠a del hardware de consumo).

:::note
El √∫nico modelo verdadero **DeepSeek-R1** en Ollama es la **versi√≥n 671B** disponible aqu√≠: [https://ollama.com/library/deepseek-r1:671b](https://ollama.com/library/deepseek-r1:671b). Otras versiones son modelos **destilados**.
:::

Esta gu√≠a se centra en ejecutar el **modelo completo DeepSeek-R1 Dynamic 1.58-bit cuantificado** utilizando **Llama.cpp** integrado con **Open WebUI**. Para este tutorial, demostraremos los pasos en una m√°quina **M4 Max + 128GB RAM**. Puedes adaptar la configuraci√≥n a tu propia instalaci√≥n.

---

## Paso 1: Instalar Llama.cpp

Puedes:
- [Descargar los binarios precompilados](https://github.com/ggerganov/llama.cpp/releases)
- **O compilarlo t√∫ mismo**: Sigue las instrucciones aqu√≠: [Gu√≠a de compilaci√≥n de Llama.cpp](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)

## Paso 2: Descargar el modelo proporcionado por UnslothAI

Dir√≠gete a la [p√°gina de Hugging Face de Unsloth](https://huggingface.co/unsloth/DeepSeek-R1-GGUF) y descarga la versi√≥n **din√°mica cuantificada** apropiada de DeepSeek-R1. Para este tutorial, utilizaremos la versi√≥n **1.58-bit (131GB)**, que est√° altamente optimizada y sigue siendo sorprendentemente funcional.


:::tip
Conoce tu "directorio de trabajo" ‚Äî d√≥nde se est√° ejecutando tu script Python o sesi√≥n de terminal. Los archivos del modelo se descargar√°n en una subcarpeta de ese directorio por defecto, ¬°as√≠ que aseg√∫rate de conocer su ruta! Por ejemplo, si est√°s ejecutando el comando siguiente en `/Users/tu_nombre/Documents/projects`, tu modelo descargado se guardar√° bajo `/Users/tu_nombre/Documents/projects/DeepSeek-R1-GGUF`.
:::

Para entender m√°s sobre el proceso de desarrollo de UnslothAI y por qu√© estas versiones din√°micamente cuantificadas son tan eficientes, revisa su publicaci√≥n de blog: [UnslothAI Cuantizaci√≥n Din√°mica DeepSeek R1](https://unsloth.ai/blog/deepseekr1-dynamic).

Aqu√≠ est√° c√≥mo descargar el modelo program√°ticamente:
```python
# Instala las dependencias de Hugging Face antes de ejecutarlo:
# pip install huggingface_hub hf_transfer

from huggingface_hub import snapshot_download

snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",  # Especifica el repositorio de Hugging Face
    local_dir = "DeepSeek-R1-GGUF",         # El modelo se descargar√° en este directorio
    allow_patterns = ["*UD-IQ1_S*"],        # Solo descargar la versi√≥n de 1.58-bit
)
```

Una vez completada la descarga, encontrar√°s los archivos del modelo en una estructura de directorios como esta:
```
DeepSeek-R1-GGUF/
‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf
```

:::info
üõ†Ô∏è Actualiza las rutas en los pasos posteriores para **coincidir con tu estructura de directorios espec√≠fica**. Por ejemplo, si tu script estaba en `/Users/tim/Downloads`, la ruta completa al archivo GGUF ser√≠a:
`/Users/tim/Downloads/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`.
:::

## Paso 3: Aseg√∫rate de que Open WebUI est√© instalado y ejecut√°ndose

Si a√∫n no tienes instalado **Open WebUI**, ¬°no te preocupes! Es una configuraci√≥n sencilla. Solo sigue la [documentaci√≥n de Open WebUI aqu√≠](https://docs.openwebui.com/). Una vez instalado, inicia la aplicaci√≥n: la conectaremos en un paso posterior para interactuar con el modelo DeepSeek-R1.


## Paso 4: Servir el modelo utilizando Llama.cpp

Ahora que el modelo est√° descargado, el siguiente paso es ejecutarlo utilizando el **modo servidor de Llama.cpp**. Antes de comenzar:

1. **Ubica el binario `llama-server`.**
   Si lo compilaste desde el c√≥digo fuente (como se detalla en el Paso 1), el ejecutable `llama-server` estar√° ubicado en `llama.cpp/build/bin`. Navega a este directorio usando el comando `cd`:
   ```bash
   cd [ruta-a-llama-cpp]/llama.cpp/build/bin
   ```

   Reemplaza `[ruta-a-llama-cpp]` con la ubicaci√≥n donde clonaste o compilaste Llama.cpp. Por ejemplo:
   ```bash
   cd ~/Documents/workspace/llama.cpp/build/bin
   ```

2. **Apunta a tu carpeta del modelo.**
   Usa la ruta completa de los archivos GGUF descargados en el Paso 2. Al servir el modelo, especifica la primera parte de los archivos divididos GGUF (por ejemplo, `DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`).

Aqu√≠ est√° el comando para iniciar el servidor:
```bash
./llama-server \
    --model /[tu-directorio]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --port 10000 \
    --ctx-size 1024 \
    --n-gpu-layers 40
```


:::tip
üîë **Par√°metros para personalizar seg√∫n su m√°quina:**

- **`--model`:** Sustituya `/[your-directory]/` con la ruta donde se descargaron los archivos GGUF en el Paso 2.
- **`--port`:** El servidor utiliza por defecto el puerto `8080`, pero puede cambiarlo seg√∫n la disponibilidad de su puerto.
- **`--ctx-size`:** Determina la longitud del contexto (n√∫mero de tokens). Puede aumentarlo si su hardware lo permite, pero tenga cuidado con el aumento en el uso de RAM/VRAM.
- **`--n-gpu-layers`:** Establezca el n√∫mero de capas que desea descargar a su GPU para una inferencia m√°s r√°pida. El n√∫mero exacto depende de la capacidad de memoria de su GPU ‚Äî consulte la tabla de Unsloth para recomendaciones espec√≠ficas.
:::

Por ejemplo, si su modelo se descarg√≥ en `/Users/tim/Documents/workspace`, su comando se ver√≠a as√≠:
```bash
./llama-server \
    --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --port 10000 \
    --ctx-size 1024 \
    --n-gpu-layers 40
```

Cuando el servidor comience, alojar√° un **endpoint API local compatible con OpenAI** en:
```
http://127.0.0.1:10000
```

:::info
üñ•Ô∏è **Servidor Llama.cpp en ejecuci√≥n**

![Captura de pantalla del servidor](/images/tutorials/deepseek/serve.png)

Despu√©s de ejecutar el comando, deber√≠a ver un mensaje que confirma que el servidor est√° activo y escuchando en el puerto 10000.
:::

Aseg√∫rese de **mantener esta sesi√≥n del terminal activa**, ya que sirve el modelo para todos los pasos subsecuentes.

## Paso 5: Conectar Llama.cpp a Open WebUI

1. Vaya a **Configuraci√≥n de administrador** en Open WebUI.
2. Navegue a **Conexiones > Conexiones OpenAI.**
3. A√±ada los siguientes detalles para la nueva conexi√≥n:
   - URL: `http://127.0.0.1:10000/v1` (o `http://host.docker.internal:10000/v1` si est√° ejecutando Open WebUI en Docker)
   - Clave API: `none`

:::info
üñ•Ô∏è **Agregando conexi√≥n en Open WebUI**

![Captura de pantalla de la conexi√≥n](/images/tutorials/deepseek/connection.png)

Despu√©s de ejecutar el comando, deber√≠a ver un mensaje que confirma que el servidor est√° activo y escuchando en el puerto 10000.
:::

Una vez que la conexi√≥n se haya guardado, ¬°puede comenzar a consultar **DeepSeek-R1** directamente desde Open WebUI! üéâ

---

## Ejemplo: Generaci√≥n de respuestas

Ahora puede usar la interfaz de chat de Open WebUI para interactuar con el **modelo din√°mico DeepSeek-R1 de 1.58 bits**.

![Captura de pantalla de la respuesta](/images/tutorials/deepseek/response.png)

---

## Notas y consideraciones

- **Rendimiento:**
  Ejecutar un modelo masivo de 131GB como DeepSeek-R1 en hardware personal ser√° **lento**. Incluso con nuestra M4 Max (128GB RAM), las velocidades de inferencia fueron modestas. Pero el hecho de que funcione es un testimonio de las optimizaciones de UnslothAI.

- **Requisitos de VRAM/Memoria:**
  Aseg√∫rese de tener suficiente VRAM y RAM del sistema para un rendimiento √≥ptimo. Con GPUs de gama baja o configuraciones s√≥lo CPU, espere velocidades m√°s lentas (pero ¬°a√∫n es posible!).

---

Gracias a **UnslothAI** y **Llama.cpp**, ejecutar uno de los modelos de razonamiento m√°s grandes de c√≥digo abierto, **DeepSeek-R1** (versi√≥n de 1.58 bits), finalmente es accesible para particulares. Aunque es desafiante ejecutar tales modelos en hardware de consumo, la capacidad de hacerlo sin infraestructura computacional masiva es un hito tecnol√≥gico significativo.

‚≠ê Muchas gracias a la comunidad por expandir los l√≠mites de la investigaci√≥n en IA abierta.

¬°Feliz experimentaci√≥n! üöÄ
