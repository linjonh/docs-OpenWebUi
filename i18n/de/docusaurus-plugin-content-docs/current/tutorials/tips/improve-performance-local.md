---
sidebar_position: 12
title: "‚ö° Verbesserung der Leistung lokaler LLMs durch dedizierte Aufgabenmodelle"
---

# Verbesserung der Leistung mit dedizierten Aufgabenmodellen

Open-WebUI stellt mehrere automatisierte Funktionen bereit ‚Äì wie die Generierung von Titeln, die Erstellung von Tags, die Autovervollst√§ndigung und die Generierung von Suchanfragen, um das Benutzererlebnis zu verbessern. Allerdings k√∂nnen diese Funktionen mehrere gleichzeitige Anfragen an Ihr lokales Modell ausl√∂sen, was die Leistung auf Systemen mit begrenzten Ressourcen beeintr√§chtigen k√∂nnte.

Dieser Leitfaden erkl√§rt, wie Sie Ihre Einrichtung optimieren, indem Sie ein dediziertes, leichtgewichtiges Aufgabenmodell konfigurieren oder automatisierte Funktionen gezielt deaktivieren, um sicherzustellen, dass Ihre Haupt-Chat-Funktionalit√§t reaktionsschnell und effizient bleibt.

---

> [!TIPP]
>## Warum f√ºhlt sich Open-WebUI langsam an?
>Standardm√§√üig hat Open-WebUI mehrere Hintergrundaufgaben, die wie Magie wirken k√∂nnen, aber auch eine hohe Belastung f√ºr lokale Ressourcen darstellen k√∂nnen:
>- **Titelerstellung**
>- **Tag-Erstellung**
>- **Autovervollst√§ndigung** (Diese Funktion wird bei jedem Tastendruck ausgel√∂st)
>- **Suchanfragenerstellung**
>
>Jede dieser Funktionen f√ºhrt asynchrone Anfragen an Ihr Modell aus. Beispielsweise k√∂nnen kontinuierliche Anrufe der Autovervollst√§ndigungsfunktion die Antworten auf Ger√§ten mit begrenztem Arbeitsspeicher oder Rechenleistung erheblich verz√∂gern, wie einem Mac mit 32 GB RAM, der ein 32B quantisiertes Modell ausf√ºhrt.
>
>Die Optimierung des Aufgabenmodells kann helfen, diese Hintergrundaufgaben von Ihrer Haupt-Chat-Anwendung zu isolieren und somit die Gesamtreaktionsf√§higkeit zu verbessern.
>
---

## ‚ö° Wie man die Leistung des Aufgabenmodells optimiert

Folgen Sie diesen Schritten, um ein effizientes Aufgabenmodell zu konfigurieren:

### Schritt 1: Zugriff auf das Admin-Panel

1. √ñffnen Sie Open-WebUI in Ihrem Browser.
2. Navigieren Sie zum **Admin-Panel**.
3. Klicken Sie auf **Einstellungen** in der Seitenleiste.

### Schritt 2: Konfigurieren Sie das Aufgabenmodell

1. Gehen Sie zu **Interface > Aufgabenmodell festlegen**.
2. W√§hlen Sie eine der folgenden Optionen basierend auf Ihren Bed√ºrfnissen:

   - **Leichtgewichtiges lokales Modell (empfohlen)**
     - W√§hlen Sie ein kompaktes Modell wie **Llama 3.2 3B** oder **Qwen2.5 3B**.
     - Diese Modelle bieten schnelle Antworten bei minimalem Ressourcenverbrauch.

   - **Gehosteter API-Endpunkt (f√ºr maximale Geschwindigkeit)**
     - Verbinden Sie sich mit einem gehosteten API-Dienst, um die Aufgabenbearbeitung zu √ºbernehmen.
     - Dies kann sehr kosteng√ºnstig sein. Zum Beispiel bietet OpenRouter Llama- und Qwen-Modelle f√ºr weniger als **1,5 Cent pro Million Eingangstoken** an.

   - **Deaktivieren unn√∂tiger Automatisierung**
     - Wenn bestimmte automatisierte Funktionen nicht ben√∂tigt werden, deaktivieren Sie diese, um √ºberfl√ºssige Hintergrundanfragen ‚Äì insbesondere Funktionen wie Autovervollst√§ndigung ‚Äì zu reduzieren.

![Lokale Modellkonfiguration auf Qwen2.5:3b eingestellt](/images/tutorials/tips/set-task-model.png)

### Schritt 3: √Ñnderungen speichern und testen

1. Speichern Sie die neue Konfiguration.
2. Interagieren Sie mit Ihrer Chat-Oberfl√§che und beobachten Sie die Reaktionsf√§higkeit.
3. Falls erforderlich, passen Sie die Einstellungen weiter an, indem Sie ungenutzte Automatisierungsfunktionen deaktivieren oder mit anderen Aufgabenmodellen experimentieren.

---

## üöÄ Empfohlene Einrichtung f√ºr lokale Modelle

| Optimierungsstrategie           | Vorteil                                  | Empfohlen f√ºr                         |
|---------------------------------|------------------------------------------|---------------------------------------|
| **Leichtgewichtiges lokales Modell** | Minimiert Ressourcenverbrauch             | Systeme mit begrenzter Hardware        |
| **Gehosteter API-Endpunkt**         | Bietet die schnellsten Antwortzeiten      | Benutzer mit zuverl√§ssigem Internet-API-Zugriff |
| **Automatisierungsfunktionen deaktivieren** | Maximiert die Leistung durch Reduzierung der Belastung | Benutzer, die sich auf die Kern-Chat-Funktionalit√§t konzentrieren |

Die Implementierung dieser Empfehlungen kann die Reaktionsf√§higkeit von Open-WebUI erheblich verbessern, w√§hrend Ihre lokalen Modelle effizient Chat-Interaktionen verarbeiten k√∂nnen.

---

## üí° Zus√§tzliche Tipps

- **Systemressourcen √ºberwachen:** Verwenden Sie die Tools Ihres Betriebssystems (wie Aktivit√§tsmonitor auf macOS oder Task-Manager auf Windows), um die Ressourcennutzung im Auge zu behalten.
- **Parallele Modellaufrufe reduzieren:** Begrenzt Hintergrundautomatisierung, um zu verhindern, dass gleichzeitige Anfragen Ihr LLM √ºberfordern.
- **Mit Konfigurationen experimentieren:** Testen Sie verschiedene leichte Modelle oder gehostete Endpunkte, um die optimale Balance zwischen Geschwindigkeit und Funktionalit√§t zu finden.
- **Aktuell bleiben:** Regelm√§√üige Updates f√ºr Open-WebUI enthalten oft Leistungsverbesserungen und Fehlerbehebungen ‚Äì halten Sie Ihre Software auf dem neuesten Stand.

---

Durch die Anwendung dieser Konfigurations√§nderungen unterst√ºtzen Sie eine reaktionsschnellere und effizientere Open-WebUI-Erfahrung, sodass Ihr lokales LLM sich auf qualitativ hochwertige Chat-Interaktionen konzentrieren kann, ohne unn√∂tige Verz√∂gerungen.
