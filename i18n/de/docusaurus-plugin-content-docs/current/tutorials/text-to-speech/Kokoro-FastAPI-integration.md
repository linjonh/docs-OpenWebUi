---
sidebar_position: 2
title: "üó®Ô∏è Kokoro-FastAPI Using Docker"
---

:::warning
Dieses Tutorial ist ein Beitrag der Community und wird nicht vom Open WebUI-Team unterst√ºtzt. Es dient nur als Demonstration, wie Open WebUI f√ºr einen spezifischen Anwendungsfall angepasst werden kann. M√∂chten Sie beitragen? Schauen Sie sich das Beitragstutorial an.
:::

## Was ist `Kokoro-FastAPI`?

[Kokoro-FastAPI](https://github.com/remsky/Kokoro-FastAPI) ist ein dockerisierter FastAPI-Wrapper f√ºr das [Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M) Text-zu-Sprache-Modell, das die OpenAI-API-Endpunktspezifikation implementiert. Es bietet Hochleistungstext-zu-Sprache mit beeindruckenden Generierungsgeschwindigkeiten.

## Hauptmerkmale

- OpenAI-kompatibler Sprachendpunkt mit Inline-Stimmenkombination
- NVIDIA GPU-beschleunigte oder CPU Onnx-Inferenz
- Streaming-Unterst√ºtzung mit variabler Chunk-Gr√∂√üe
- Unterst√ºtzung f√ºr mehrere Audioformate (`.mp3`, `.wav`, `.opus`, `.flac`, `.aac`, `.pcm`)
- Integriertes Webinterface auf localhost:8880/web (oder zus√§tzliches Container im Repo f√ºr Gradio)
- Phonem-Endpunkte f√ºr Umwandlung und Generierung

## Stimmen

- af
- af_bella
- af_irulan
- af_nicole
- af_sarah
- af_sky
- am_adam
- am_michael
- am_gurney
- bf_emma
- bf_isabella
- bm_george
- bm_lewis

## Sprachen

- en_us
- en_uk

## Voraussetzungen

- Docker muss auf Ihrem System installiert sein
- Open WebUI l√§uft
- F√ºr GPU-Unterst√ºtzung: NVIDIA GPU mit CUDA 12.3
- F√ºr CPU-only: Keine speziellen Anforderungen

## ‚ö°Ô∏è Schnellstart

### Sie k√∂nnen zwischen GPU- oder CPU-Versionen w√§hlen

### GPU-Version (Erfordert NVIDIA GPU mit CUDA 12.8)

Mit einem Docker-Run-Befehl:

```bash
docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu
```

Oder mit Docker Compose, indem eine `docker-compose.yml`-Datei erstellt und `docker compose up` ausgef√ºhrt wird. Beispiel:

```yaml
name: kokoro
services:
    kokoro-fastapi-gpu:
        ports:
            - 8880:8880
        image: ghcr.io/remsky/kokoro-fastapi-gpu:v0.2.1
        restart: always
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities:
                              - gpu
```

:::info
M√∂glicherweise m√ºssen Sie [das NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installieren und konfigurieren.
:::

### CPU-Version (Onnx-optimierte Inferenz)

Mit einem Docker-Run-Befehl:

```bash
docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu
```

Mit Docker Compose:

```yaml
name: kokoro
services:
    kokoro-fastapi-cpu:
        ports:
            - 8880:8880
        image: ghcr.io/remsky/kokoro-fastapi-cpu
        restart: always
```

## Einrichtung von Open WebUI zur Nutzung von `Kokoro-FastAPI`

Um Kokoro-FastAPI mit Open WebUI zu verwenden, f√ºhren Sie die folgenden Schritte aus:

- √ñffnen Sie das Admin-Panel und gehen Sie zu `Settings` -> `Audio`
- Passen Sie Ihre TTS-Einstellungen folgenderma√üen an:
- - Text-to-Speech Engine: OpenAI
  - API Base URL: `http://localhost:8880/v1` # M√∂glicherweise m√ºssen Sie `host.docker.internal` anstelle von `localhost` verwenden
  - API-Schl√ºssel: `not-needed`
  - TTS Modell: `kokoro`
  - TTS Stimme: `af_bella` # akzeptiert auch Mapping bestehender OAI-Stimmen zur Kompatibilit√§t

:::info
Der Standard-API-Schl√ºssel ist der String `not-needed`. Sie m√ºssen diesen Wert nicht √§ndern, wenn Sie keine zus√§tzliche Sicherheit ben√∂tigen.
:::

## Erstellen des Docker-Containers

```bash
git clone https://github.com/remsky/Kokoro-FastAPI.git
cd Kokoro-FastAPI
cd docker/cpu # oder docker/gpu
docker compose up --build
```

**Das war's!**

Weitere Informationen zum Erstellen des Docker-Containers, einschlie√ülich Port√§nderungen, finden Sie im [Kokoro-FastAPI](https://github.com/remsky/Kokoro-FastAPI) Repository.
