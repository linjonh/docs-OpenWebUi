---
sidebar_position: 1
title: "üêã F√ºhre DeepSeek R1 Dynamic 1.58-bit mit Llama.cpp aus"
---

Ein gro√ües Dankesch√∂n an **UnslothAI** f√ºr ihre unglaublichen Bem√ºhungen! Dank ihrer Arbeit k√∂nnen wir jetzt das vollst√§ndige **DeepSeek-R1** Modell mit 671 Milliarden Parametern in seiner dynamischen 1.58-bit quantisierten Form (auf nur 131 GB komprimiert) auf **Llama.cpp** ausf√ºhren! Und das Beste daran? Sie m√ºssen nicht mehr verzweifeln, weil Sie riesige GPU-Cluster auf Unternehmensniveau oder Server ben√∂tigen ‚Äì es ist m√∂glich, dieses Modell auf Ihrem pers√∂nlichen Computer auszuf√ºhren (wenn auch langsam auf den meisten Verbraucherger√§ten).

:::note
Das einzige echte **DeepSeek-R1** Modell auf Ollama ist die **671B-Version**, die hier verf√ºgbar ist: [https://ollama.com/library/deepseek-r1:671b](https://ollama.com/library/deepseek-r1:671b). Andere Versionen sind **distillierte** Modelle.
:::

Dieser Leitfaden konzentriert sich darauf, das vollst√§ndige **DeepSeek-R1 Dynamic 1.58-bit quantisierte Modell** mit **Llama.cpp** zu verwenden, das in **Open WebUI** integriert ist. In diesem Tutorial demonstrieren wir die Schritte mit einem **M4 Max + 128 GB RAM** Rechner. Sie k√∂nnen die Einstellungen an Ihre eigene Konfiguration anpassen.

---

## Schritt 1: Installieren Sie Llama.cpp

Sie k√∂nnen entweder:
- [Die vorkompilierten Bin√§rdateien herunterladen](https://github.com/ggerganov/llama.cpp/releases)
- **Oder selbst kompilieren**: Befolgen Sie die Anleitung hier: [Llama.cpp Build Guide](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md)

## Schritt 2: Laden Sie das Modell von UnslothAI herunter

Besuchen Sie die [Hugging Face Seite von Unsloth](https://huggingface.co/unsloth/DeepSeek-R1-GGUF) und laden Sie die passende **dynamisch quantisierte Version** von DeepSeek-R1 herunter. F√ºr dieses Tutorial verwenden wir die **1.58-bit-Version (131 GB)**, die hoch optimiert ist und dennoch √ºberraschend funktional bleibt.


:::tip
Kennen Sie Ihr ‚ÄûArbeitsverzeichnis‚Äú ‚Äì den Ort, an dem Ihr Python-Skript oder Ihre Terminal-Sitzung ausgef√ºhrt wird. Die Mod-Dateien werden standardm√§√üig in einem Unterordner dieses Verzeichnisses heruntergeladen, daher achten Sie darauf, den Pfad zu kennen! Wenn Sie beispielsweise den folgenden Befehl unter `/Users/IhrName/Documents/projects` ausf√ºhren, wird Ihr heruntergeladenes Modell unter `/Users/IhrName/Documents/projects/DeepSeek-R1-GGUF` gespeichert.
:::

Um mehr √ºber den Entwicklungsprozess von UnslothAI und warum diese dynamischen quantisierten Versionen so effizient sind zu erfahren, lesen Sie ihren Blogbeitrag: [UnslothAI DeepSeek R1 Dynamic Quantization](https://unsloth.ai/blog/deepseekr1-dynamic).

So laden Sie das Modell programmgesteuert herunter:
```python
# Installieren Sie Hugging Face-Abh√§ngigkeiten vor dem Ausf√ºhren:
# pip install huggingface_hub hf_transfer

from huggingface_hub import snapshot_download

snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",  # Hugging Face Repository angeben
    local_dir = "DeepSeek-R1-GGUF",         # Das Modell wird in dieses Verzeichnis heruntergeladen
    allow_patterns = ["*UD-IQ1_S*"],        # Nur die 1.58-bit-Version herunterladen
)
```

Nach Abschluss des Downloads finden Sie die Mod-Dateien in einer Ordnerstruktur wie dieser:
```
DeepSeek-R1-GGUF/
‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S/
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf
‚îÇ   ‚îú‚îÄ‚îÄ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf
```

:::info
üõ†Ô∏è Aktualisieren Sie die Pfade in den folgenden Schritten, damit sie **zu Ihrer spezifischen Ordnerstruktur passen**. Wenn Ihr Skript beispielsweise unter `/Users/tim/Downloads` war, w√§re der vollst√§ndige Pfad zur GGUF-Datei:  
`/Users/tim/Downloads/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`.
:::

## Schritt 3: Stellen Sie sicher, dass Open WebUI installiert und ausgef√ºhrt wird

Falls Sie **Open WebUI** noch nicht installiert haben, keine Sorge! Die Einrichtung ist einfach. Befolgen Sie einfach die [Open WebUI Dokumentation hier](https://docs.openwebui.com/). Sobald installiert, starten Sie die Anwendung ‚Äì wir verbinden sie sp√§ter, um mit dem DeepSeek-R1 Modell zu interagieren.


## Schritt 4: Das Modell mit Llama.cpp bereitstellen

Jetzt, da das Modell heruntergeladen ist, f√ºhren Sie es mit dem **Servermodus von Llama.cpp** aus. Bevor Sie beginnen:

1. **Lokalisieren Sie die `llama-server`-Bin√§rdatei.**
   Wenn Sie die Quelle wie in Schritt 1 beschrieben gebaut haben, befindet sich die ausf√ºhrbare Datei `llama-server` unter `llama.cpp/build/bin`. Wechseln Sie zu diesem Verzeichnis mit dem Befehl `cd`:
   ```bash
   cd [Pfad-zu-llama-cpp]/llama.cpp/build/bin
   ```

   Ersetzen Sie `[Pfad-zu-llama-cpp]` durch den Speicherort, an dem Sie Llama.cpp geklont oder erstellt haben. Zum Beispiel:
   ```bash
   cd ~/Documents/workspace/llama.cpp/build/bin
   ```

2. **Weisen Sie auf Ihren Modellordner hin.**
   Verwenden Sie den vollst√§ndigen Pfad zu den heruntergeladenen GGUF-Dateien, die in Schritt 2 erstellt wurden. Geben Sie beim Bereitstellen des Modells den ersten Teil der geteilten GGUF-Dateien an (z. B. `DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf`).

Hier ist der Befehl, um den Server zu starten:
```bash
./llama-server \
    --model /[Ihr-Verzeichnis]/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --port 10000 \
    --ctx-size 1024 \
    --n-gpu-layers 40
```


:::tip
üîë **Parameter zur Anpassung basierend auf Ihrer Maschine:**

- **`--model`:** Ersetzen Sie `/[your-directory]/` durch den Pfad, an dem die GGUF-Dateien in Schritt 2 heruntergeladen wurden.
- **`--port`:** Der Standardserver-Port ist `8080`, aber Sie k√∂nnen ihn entsprechend Ihrer Portverf√ºgbarkeit √§ndern.
- **`--ctx-size`:** Bestimmt die Kontextl√§nge (Anzahl der Tokens). Sie k√∂nnen diese erh√∂hen, wenn Ihre Hardware dies erlaubt, aber achten Sie auf den steigenden RAM/VRAM-Bedarf.
- **`--n-gpu-layers`:** Legen Sie die Anzahl der Schichten fest, die Sie f√ºr schnelleres Inferencing auf Ihre GPU auslagern m√∂chten. Die genaue Anzahl h√§ngt von der Speicherkapazit√§t Ihrer GPU ab ‚Äî nutzen Sie die Tabelle von Unsloth als Referenz f√ºr spezifische Empfehlungen.
:::

Wenn Ihr Modell beispielsweise in `/Users/tim/Documents/workspace` heruntergeladen wurde, w√ºrde Ihr Befehl folgenderma√üen aussehen:
```bash
./llama-server \
    --model /Users/tim/Documents/workspace/DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
    --port 10000 \
    --ctx-size 1024 \
    --n-gpu-layers 40
```

Sobald der Server startet, hostet er einen **lokalen OpenAI-kompatiblen API**-Endpunkt unter:
```
http://127.0.0.1:10000
```

:::info
üñ•Ô∏è **Llama.cpp-Server l√§uft**

![Server Screenshot](/images/tutorials/deepseek/serve.png)

Nachdem Sie den Befehl ausgef√ºhrt haben, sollten Sie eine Nachricht sehen, die best√§tigt, dass der Server aktiv ist und auf Port 10000 lauscht.
:::

Stellen Sie sicher, dass Sie **dieses Terminal ge√∂ffnet halten**, da es das Modell f√ºr alle nachfolgenden Schritte bereitstellt.

## Schritt 5: Verbinden Sie Llama.cpp mit Open WebUI

1. Gehen Sie zu **Admin Settings** in Open WebUI.
2. Navigieren Sie zu **Connections > OpenAI Connections.**
3. F√ºgen Sie die folgenden Details f√ºr die neue Verbindung hinzu:
   - URL: `http://127.0.0.1:10000/v1` (oder `http://host.docker.internal:10000/v1`, wenn Sie Open WebUI in Docker verwenden)
   - API-Schl√ºssel: `none`

:::info
üñ•Ô∏è **Verbindung in Open WebUI hinzuf√ºgen**

![Connection Screenshot](/images/tutorials/deepseek/connection.png)

Nachdem Sie den Befehl ausgef√ºhrt haben, sollten Sie eine Nachricht sehen, die best√§tigt, dass der Server aktiv ist und auf Port 10000 lauscht.
:::

Sobald die Verbindung gespeichert ist, k√∂nnen Sie **DeepSeek-R1** direkt aus Open WebUI anfragen! üéâ

---

## Beispiel: Antworten generieren

Sie k√∂nnen nun die Chat-Oberfl√§che von Open WebUI verwenden, um mit dem **DeepSeek-R1 Dynamic 1.58-bit Model** zu interagieren.

![Response Screenshot](/images/tutorials/deepseek/response.png)

---

## Hinweise und √úberlegungen

- **Leistung:**
  Das Ausf√ºhren eines enormen 131GB-Modells wie DeepSeek-R1 auf pers√∂nlicher Hardware wird **langsam** sein. Selbst mit unserem M4 Max (128GB RAM) waren die Inferenzgeschwindigkeiten bescheiden. Aber die Tatsache, dass es √ºberhaupt funktioniert, ist ein Beweis f√ºr die Optimierungen von UnslothAI.

- **VRAM-/Speicheranforderungen:**
  Stellen Sie sicher, dass ausreichend VRAM und Systemspeicher f√ºr eine optimale Leistung verf√ºgbar sind. Bei GPUs mit niedriger Leistung oder CPU-only-Setups erwarten Sie langsamere Geschwindigkeiten (aber es ist trotzdem machbar!).

---

Dank **UnslothAI** und **Llama.cpp** ist es endlich m√∂glich, eines der gr√∂√üten Open-Source-Reasoning-Modelle, **DeepSeek-R1** (1.58-bit Version), f√ºr Einzelpersonen zug√§nglich zu machen. W√§hrend es herausfordernd ist, solche Modelle auf Verbraucherhardware auszuf√ºhren, ist die F√§higkeit, dies ohne massiven Recheninfrastruktur zu tun, ein bedeutender technologischer Meilenstein.

‚≠ê Ein gro√ües Dankesch√∂n an die Community, die die Grenzen der offenen KI-Forschung verschiebt.

Viel Spa√ü beim Experimentieren! üöÄ
