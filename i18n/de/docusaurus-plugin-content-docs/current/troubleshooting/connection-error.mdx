---
sidebar_position: 0
title: "ğŸš§ Probleme mit der Server-KonnektivitÃ¤t"
---

Wir sind hier, um Ihnen zu helfen, alles einzurichten und reibungslos laufen zu lassen. Nachfolgend finden Sie Schritt-fÃ¼r-Schritt-Anleitungen, die auf verschiedene Szenarien zugeschnitten sind, um hÃ¤ufige Verbindungsprobleme mit Ollama und externen Servern wie Hugging Face zu lÃ¶sen.

## ğŸŒŸ Verbindung zum Ollama-Server

### ğŸš€ Zugriff auf Ollama von Open WebUI

Haben Sie Schwierigkeiten, von Open WebUI aus eine Verbindung zu Ollama herzustellen? MÃ¶glicherweise hÃ¶rt Ollama nicht auf einem Netzwerk-Interface, das externe Verbindungen erlaubt. Lassen Sie uns das beheben:

1. **Konfigurieren Sie Ollama fÃ¼r breite ZuhÃ¶rbereiche** ğŸ§:
   Setzen Sie `OLLAMA_HOST` auf `0.0.0.0`, damit Ollama auf allen Netzwerk-Interfaces lauscht.

2. **Umgebungsvariablen aktualisieren**:
   Stellen Sie sicher, dass `OLLAMA_HOST` korrekt in Ihrer Bereitstellungsumgebung eingestellt ist.

3. **Ollama neu starten**ğŸ”„:
   Ein Neustart ist erforderlich, damit die Ã„nderungen wirksam werden.

ğŸ’¡ Nach dem Einrichten Ã¼berprÃ¼fen Sie, ob Ollama Ã¼ber die WebUI-Schnittstelle zugÃ¤nglich ist.

FÃ¼r detailliertere Anweisungen zur Konfiguration von Ollama konsultieren Sie bitte die [Offizielle Dokumentation von Ollama](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux).

### ğŸ³ Docker-Verbindungsfehler

Wenn Sie einen Verbindungsfehler beim Versuch, auf Ollama zuzugreifen, sehen, liegt es mÃ¶glicherweise daran, dass der WebUI-Docker-Container nicht mit dem Ollama-Server auf Ihrem Host kommunizieren kann. Lassen Sie uns das beheben:

1. **Netzwerkeinstellungen anpassen** ğŸ› ï¸:
   Verwenden Sie das Flag `--network=host` in Ihrem Docker-Befehl. Dies verbindet Ihren Container direkt mit dem Netzwerk Ihres Hosts.

2. **Port Ã¤ndern**:
   Denken Sie daran, dass sich der interne Port von 3000 auf 8080 Ã¤ndert.

**Beispiel-Docker-Befehl**:
```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
ğŸ”— Nach der AusfÃ¼hrung des obigen Befehls sollte Ihre WebUI unter `http://localhost:8080` verfÃ¼gbar sein.

## ğŸ”’ SSL-Verbindungsproblem mit Hugging Face

Ein SSL-Fehler aufgetreten? Dies kÃ¶nnte ein Problem mit dem Hugging Face-Server sein. Hier ist, was zu tun ist:

1. **Serverstatus von Hugging Face Ã¼berprÃ¼fen**:
   Stellen Sie fest, ob es auf deren Seite eine bekannte StÃ¶rung oder ein Problem gibt.

2. **Endpoints wechseln**:
   Falls Hugging Face nicht erreichbar ist, Ã¤ndern Sie den Endpunkt in Ihrem Docker-Befehl.

**Beispiel-Docker-Befehl fÃ¼r Verbindungsprobleme**:
```bash
docker run -d -p 3000:8080 -e HF_ENDPOINT=https://hf-mirror.com/ --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

## ğŸ Podman auf MacOS

Arbeiten Sie auf MacOS mit Podman? So stellen Sie die KonnektivitÃ¤t sicher:

1. **Host-Loopback aktivieren**:
   Nutzen Sie `--network slirp4netns:allow_host_loopback=true` in Ihrem Befehl.

2. **OLLAMA_BASE_URL setzen**:
   Stellen Sie sicher, dass es auf `http://host.containers.internal:11434` zeigt.

**Beispiel-Podman-Befehl**:
```bash
podman run -d --network slirp4netns:allow_host_loopback=true -p 3000:8080 -e OLLAMA_BASE_URL=http://host.containers.internal:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

