---
sidebar_position: 3
title: "ğŸ§  Fehlerbehebung bei RAG (Retrieval-Augmented Generation)"
---

Retrieval-Augmented Generation (RAG) ermÃ¶glicht es Sprachmodellen, Ã¼ber externe Inhalte â€“ Dokumente, Wissensdatenbanken und mehr â€“ nachzudenken, indem relevante Informationen abgerufen und in das Modell eingespeist werden. Wenn jedoch Dinge nicht wie erwartet funktionieren (z.B. das Modell "halluziniert" oder Ã¼bersieht relevante Informationen), liegt es oft nicht am Modell selbst â€“ es ist ein Problem mit dem Kontext.

Lassen Sie uns die hÃ¤ufigsten Ursachen und LÃ¶sungen aufschlÃ¼sseln, damit Sie die Genauigkeit von RAG maximieren kÃ¶nnen! ğŸš€

## HÃ¤ufige Probleme mit RAG und deren Behebung ğŸ› ï¸

### 1. Das Modell "sieht" Ihre Inhalte nicht ğŸ‘ï¸âŒ

Dies ist das hÃ¤ufigste Problem â€“ und es wird typischerweise durch Fehler im Prozess der Inhaltsaufnahme verursacht. Das Modell halluziniert nicht, weil es falsch liegt, sondern weil es von Anfang an nicht mit den richtigen Inhalten versorgt wurde.

âœ… LÃ¶sung: ÃœberprÃ¼fen Sie Ihre Einstellungen zur Inhaltsauslesung

- Navigieren Sie zu: **Admin-Einstellungen > Dokumente**.
- Stellen Sie sicher, dass Sie eine leistungsstarke Inhaltsauslese-Engine verwenden, wie z.B.:
  - Apache Tika
  - Docling
  - Eigene Extraktoren (je nach Dokumenttyp)

ğŸ“Œ Tipp: Laden Sie ein Dokument hoch und sehen Sie sich die extrahierten Inhalte an. Wenn diese leer sind oder wichtige Abschnitte fehlen, mÃ¼ssen Sie Ihre Extraktions-Einstellungen anpassen oder eine andere Engine verwenden.

---

### 2. Nur ein kleiner Teil des Dokuments wird verwendet ğŸ“„â¡ï¸âœ‚ï¸

Open WebUI ist fÃ¼r Modelle ausgelegt, die standardmÃ¤ÃŸig begrenzte Kontextfenster haben. Zum Beispiel sind viele lokale Modelle (z.B. die Standardmodelle von Ollama) auf 2048 Tokens beschrÃ¤nkt. Daher kÃ¼rzt Open WebUI abgerufene Inhalte aggressiv, um in den verfÃ¼gbaren Platz zu passen.

âœ… LÃ¶sungen:

- Gehen Sie zu **Admin-Einstellungen > Dokumente**
- WÃ¤hlen Sie eine der folgenden Optionen:
  - ğŸ’¡ Aktivieren Sie â€Embedding und Abruf umgehenâ€œ â€“ Dies sendet den gesamten Inhalt direkt, ohne strenge Abruf-Filter anzuwenden.
  - ğŸ” Schalten Sie â€Vollkontextmodusâ€œ ein â€“ Dies fÃ¼gt umfassendere Inhalte in die Modelleingabe ein.

ğŸ“Œ Warnung: Beachten Sie die Kontextgrenzen â€“ wenn Ihr Modell keine grÃ¶ÃŸere Token-Anzahl verarbeiten kann, wird es dennoch abgeschnitten.

---

### 3. Die Token-Grenze ist zu kurz â³

Selbst wenn der Abruf funktioniert, kann es sein, dass Ihr Modell nicht den gesamten Inhalt verarbeitet, den es erhÃ¤lt â€“ einfach, weil es dazu nicht in der Lage ist.

StandardmÃ¤ÃŸig sind viele Modelle (insbesondere die von Ollama gehosteten LLMs) auf ein Kontextfenster von 2048 Tokens beschrÃ¤nkt. Das bedeutet, dass nur ein Bruchteil der abgerufenen Daten tatsÃ¤chlich genutzt wird.

âœ… LÃ¶sungen:

- ğŸ› ï¸ VerlÃ¤ngern Sie die KontextlÃ¤nge des Modells:
  - Navigieren Sie zum **Modelleditor oder Chat-Kontrollen**
  - Ã„ndern Sie die KontextlÃ¤nge (z.B. erhÃ¶hen Sie sie auf 8192+ Tokens, falls unterstÃ¼tzt)

â„¹ï¸ Hinweis: Die Standardgrenze von 2048 Tokens ist eine groÃŸe EinschrÃ¤nkung. FÃ¼r bessere RAG-Ergebnisse empfehlen wir die Verwendung von Modellen, die lÃ¤ngere Kontexte unterstÃ¼tzen.

âœ… Alternative: Verwenden Sie ein externes LLM mit grÃ¶ÃŸerer KontextkapazitÃ¤t

- Versuchen Sie GPT-4, GPT-4o, Claude 3, Gemini 1.5 oder Mixtral mit 8k+ Kontext
- Vergleichen Sie die Leistung mit Ollama â€“ achten Sie auf die Genauigkeitsunterschiede, wenn mehr Inhalte eingefÃ¼gt werden kÃ¶nnen!

ğŸ“Œ Tipp: Verwenden Sie externe Modelle fÃ¼r bessere RAG-Leistung in Produktionsumgebungen.

---

### 4. Das Embedding-Modell ist von niedriger QualitÃ¤t oder nicht passend ğŸ“‰ğŸ§ 

Schlechte Embeddings = schlechter Abruf. Wenn die VektorreprÃ¤sentation Ihrer Inhalte schlecht ist, wird der Abrufmotor nicht die richtigen Inhalte ziehen â€“ egal wie leistungsstark Ihr LLM ist.

âœ… LÃ¶sung:

- Wechseln Sie zu einem hochwertigen Embedding-Modell (z.B. all-MiniLM-L6-v2, Instructor X oder OpenAI-Embeddings)
- Gehen Sie zu: **Admin-Einstellungen > Dokumente**
- Stellen Sie nach dem Ã„ndern des Modells sicher, dass Sie:
  - â³ Alle vorhandenen Dokumente neu indexieren, damit die neuen Embeddings wirksam werden.

ğŸ“Œ Denken Sie daran: Die QualitÃ¤t der Embeddings beeinflusst direkt, welche Inhalte abgerufen werden.

---

### 5. âŒ 400: NoneType-Objekt hat kein Attribut encode

Dieser Fehler zeigt an, dass das Embedding-Modell falsch konfiguriert oder nicht vorhanden ist. Wenn Open WebUI versucht, Embeddings zu erstellen, aber kein gÃ¼ltiges Modell geladen ist, kann der Text nicht verarbeitet werden â€“ und das Ergebnis ist dieser kryptische Fehler.

ğŸ’¥ Ursache:
- Ihr Embedding-Modell ist nicht korrekt eingerichtet.
- Es wurde mÃ¶glicherweise nicht vollstÃ¤ndig heruntergeladen.
- Oder wenn Sie ein externes Embedding-Modell verwenden, ist es mÃ¶glicherweise nicht zugÃ¤nglich.

âœ… LÃ¶sung:

- Gehen Sie zu: **Admin-Einstellungen > Dokumente > Embedding-Modell**
- Speichern Sie das Embedding-Modell erneut â€“ auch wenn es bereits ausgewÃ¤hlt ist. Dadurch wird ein erneuter Download/ÃœberprÃ¼fung erzwungen.
- Wenn Sie ein entferntes/externes Embedding-Tool verwenden, stellen Sie sicher, dass es lÃ¤uft und von Open WebUI zugÃ¤nglich ist.

ğŸ“Œ Tipp: Nach der Korrektur der Konfiguration versuchen Sie, ein Dokument erneut zu embeddieren und Ã¼berprÃ¼fen, ob im Protokoll keine Fehler angezeigt werden.

---

## ğŸ§ª Pro Tipp: Testen Sie mit GPT-4o oder GPT-4

Wenn Sie sich nicht sicher sind, ob das Problem beim Abruf, bei den Token-Grenzen oder bei den Embeddings liegt, versuchen Sie, vorÃ¼bergehend GPT-4o zu verwenden (z.B. Ã¼ber die OpenAI-API). Wenn die Ergebnisse plÃ¶tzlich genauer werden, ist dies ein starkes Indiz dafÃ¼r, dass die Kontextgrenze Ihres lokalen Modells (2048 standardmÃ¤ÃŸig bei Ollama) das Bottleneck ist.

- GPT-4o verarbeitet grÃ¶ÃŸere Eingaben (128k Tokens!)
- Bietet eine groÃŸartige Benchmark zur Bewertung der ZuverlÃ¤ssigkeit Ihres Systems bei RAG

---

## Zusammenfassende Checkliste âœ…

| Problem | LÃ¶sung |
|--------|------|
| ğŸ¤” Modell kann den Inhalt nicht "sehen" | ÃœberprÃ¼fen Sie die Einstellungen des Dokumentenextraktors |
| ğŸ§¹ Nur ein Teil des Inhalts wird verwendet | Aktivieren Sie den Vollkontextmodus oder umgehen Sie das Embedding |
| â± Begrenzung durch 2048-Token-Grenze | ErhÃ¶hen Sie die KontextlÃ¤nge des Modells oder nutzen Sie ein groÃŸes Kontext-LLM |
| ğŸ“‰ Ungenaue Abrufung | Wechseln Sie zu einem besseren Embedding-Modell und indizieren Sie neu |
| Immer noch verwirrt? | Testen Sie mit GPT-4o und vergleichen Sie die Ausgaben |

---

Durch die Optimierung dieser Bereicheâ€”Extraktion, Embedding, Abruf und Modellkontextâ€”kÃ¶nnen Sie die Genauigkeit Ihres LLM im Umgang mit Dokumenten erheblich verbessern. Lassen Sie nicht ein 2048-Token-Fenster oder eine schwache Abrufpipeline die Leistung Ihrer KI einschrÃ¤nken ğŸ¯.
