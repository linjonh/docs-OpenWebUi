---
sidebar_position: 3
title: "ğŸ¦™Starten mit Llama.cpp"
---

## Ãœberblick

Open WebUI macht es einfach und flexibel, einen lokalen Llama.cpp-Server zu verbinden und zu verwalten, um effiziente, quantisierte Sprachmodelle auszufÃ¼hren. Egal, ob Sie Llama.cpp selbst kompiliert haben oder vorkompilierte BinÃ¤rdateien verwenden, diese Anleitung zeigt Ihnen, wie Sie:

- Ihren Llama.cpp-Server einrichten
- GroÃŸe Modelle lokal laden
- Mit Open WebUI fÃ¼r eine nahtlose BenutzeroberflÃ¤che integrieren

Legen wir los!

---

## Schritt 1: Llama.cpp installieren

Um Modelle mit Llama.cpp auszufÃ¼hren, mÃ¼ssen Sie zunÃ¤chst den Llama.cpp-Server lokal installieren.

Sie kÃ¶nnen entweder:

- ğŸ“¦ [Vorgebaute BinÃ¤rdateien herunterladen](https://github.com/ggerganov/llama.cpp/releases)
- ğŸ› ï¸ Oder es aus dem Quellcode erstellen, indem Sie den [offiziellen Build-Anweisungen](https://github.com/ggerganov/llama.cpp/blob/master/docs/build.md) folgen

Nach der Installation stellen Sie sicher, dass `llama-server` in Ihrem lokalen Systempfad verfÃ¼gbar ist oder notieren Sie sich seinen Speicherort.

---

## Schritt 2: Ein unterstÃ¼tztes Modell herunterladen

Sie kÃ¶nnen verschiedene in GGUF-Format quantisierte LLMs mit Llama.cpp laden und ausfÃ¼hren. Ein beeindruckendes Beispiel ist das DeepSeek-R1 1.58-Bit-Modell, optimiert von UnslothAI. Um diese Version herunterzuladen:

1. Besuchen Sie das [Unsloth DeepSeek-R1-Repository auf Hugging Face](https://huggingface.co/unsloth/DeepSeek-R1-GGUF)
2. Laden Sie die quantisierte 1.58-Bit-Version herunter â€“ etwa 131GB.

Alternativ kÃ¶nnen Sie Python verwenden, um das Modell programmgesteuert herunterzuladen:

```python
# pip install huggingface_hub hf_transfer

from huggingface_hub import snapshot_download

snapshot_download(
    repo_id = "unsloth/DeepSeek-R1-GGUF",
    local_dir = "DeepSeek-R1-GGUF",
    allow_patterns = ["*UD-IQ1_S*"],  # Nur die 1.58-Bit-Variante herunterladen
)
```

Dies wird die Modelldateien in ein Verzeichnis wie folgendes herunterladen:
```
DeepSeek-R1-GGUF/
â””â”€â”€ DeepSeek-R1-UD-IQ1_S/
    â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf
    â”œâ”€â”€ DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf
    â””â”€â”€ DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf
```

ğŸ“ Merken Sie sich den vollstÃ¤ndigen Pfad zur ersten GGUF-Datei â€” Sie benÃ¶tigen ihn in Schritt 3.

---

## Schritt 3: Das Modell mit Llama.cpp bereitstellen

Starten Sie den Modellserver mithilfe der llama-server-BinÃ¤rdatei. Wechseln Sie zu Ihrem llama.cpp-Ordner (z. B. build/bin) und fÃ¼hren Sie aus:

```bash
./llama-server \
  --model /Ihr/vollstÃ¤ndiger/Pfad/zu/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \
  --port 10000 \
  --ctx-size 1024 \
  --n-gpu-layers 40
```

ğŸ› ï¸ Passen Sie die Parameter an Ihre Maschine an:

- --model: Pfad zu Ihrer .gguf-Modelldatei
- --port: 10000 (oder wÃ¤hlen Sie einen anderen offenen Port)
- --ctx-size: Token-KontextlÃ¤nge (kann erhÃ¶ht werden, wenn RAM verfÃ¼gbar ist)
- --n-gpu-layers: An GPU ausgelagerte Schichten fÃ¼r schnellere Leistung

Sobald der Server lÃ¤uft, stellt er eine lokale OpenAI-kompatible API bereit unter:

```
http://127.0.0.1:10000
```

---

## Schritt 4: Llama.cpp mit Open WebUI verbinden

Um Ihr lokal laufendes Modell direkt von Open WebUI aus zu steuern und abzufragen:

1. Ã–ffnen Sie Open WebUI in Ihrem Browser
2. Gehen Sie zu âš™ï¸ Admin-Einstellungen â†’ Verbindungen â†’ OpenAI-Verbindungen
3. Klicken Sie auf â• Verbindung hinzufÃ¼gen und geben Sie ein:

- URL: `http://127.0.0.1:10000/v1`
  (Oder verwenden Sie `http://host.docker.internal:10000/v1`, wenn Sie WebUI innerhalb von Docker ausfÃ¼hren)
- API-SchlÃ¼ssel: `none` (leer lassen)

ğŸ’¡ Sobald gespeichert, beginnt Open WebUI, Ihren lokalen Llama.cpp-Server als Backend zu verwenden!

![Llama.cpp-Verbindung in Open WebUI](/images/tutorials/deepseek/connection.png)

---

## Tipp: Modell im Chat-Interface ausprobieren

Sobald verbunden, wÃ¤hlen Sie das Modell im Open WebUI-Chat-MenÃ¼ aus und beginnen Sie zu interagieren!

![Modell-Chat-Vorschau](/images/tutorials/deepseek/response.png)

---

## Sie sind bereit!

Sobald konfiguriert, macht Open WebUI es einfach:

- Lokale Modelle, die von Llama.cpp bereitgestellt werden, zu verwalten und zwischen ihnen zu wechseln
- Die OpenAI-kompatible API ohne SchlÃ¼ssel zu verwenden
- Mit groÃŸen Modellen wie DeepSeek-R1 direkt von Ihrer Maschine aus zu experimentieren!

---

ğŸš€ Viel SpaÃŸ beim Experimentieren und Erstellen!