---

sidebar_position: 4  
title: "üåê Einstieg mit OpenAI-kompatiblen Servern"

---

## √úberblick

Open WebUI ist nicht nur f√ºr OpenAI/Ollama/Llama.cpp gedacht ‚Äì Sie k√∂nnen **jeden Server nutzen, der die OpenAI-kompatible API implementiert**, sei es lokal oder remote. Dies ist ideal, wenn Sie verschiedene Sprachmodelle ausf√ºhren m√∂chten oder bereits ein bevorzugtes Backend oder √ñkosystem haben. Dieser Leitfaden zeigt Ihnen:

- Wie Sie einen OpenAI-kompatiblen Server einrichten (mit einigen beliebten Optionen)
- Wie Sie ihn mit Open WebUI verbinden
- Wie Sie direkt loschatten k√∂nnen

## Schritt 1: W√§hlen Sie einen OpenAI-kompatiblen Server

Es gibt viele Server und Tools, die eine OpenAI-kompatible API bereitstellen. Hier sind einige der beliebtesten:

- [Llama.cpp](https://github.com/ggml-org/llama.cpp): Extrem effizient, l√§uft auf CPU und GPU
- [Ollama](https://ollama.com/): Super benutzerfreundlich und plattform√ºbergreifend
- [LM Studio](https://lmstudio.ai/): Funktionsreiche Desktop-App f√ºr Windows/Mac/Linux
- [Lemonade (ONNX TurnkeyML)](https://github.com/onnx/turnkeyml): Schnelles ONNX-basiertes Backend mit NPU/iGPU-Beschleunigung

W√§hlen Sie, was zu Ihrem Workflow passt!

---

#### üçã Erste Schritte mit Lemonade (ONNX TurnkeyML)

Lemonade ist ein Plug-and-Play-Server auf ONNX-Basis, der mit OpenAI kompatibel ist. So k√∂nnen Sie ihn auf Windows ausprobieren:

1. [Laden Sie die neueste `.exe` herunter](https://github.com/onnx/turnkeyml/releases)
2. F√ºhren Sie `Lemonade_Server_Installer.exe` aus
3. Installieren Sie Lemonade und laden Sie ein Modell mit dem Installer herunter
4. Sobald der Server l√§uft, lautet Ihr API-Endpunkt:

   ```
   http://localhost:8000/api/v0
   ```

![Lemonade Server](/images/getting-started/lemonade-server.png)

Details finden Sie in [deren Dokumentation](https://github.com/onnx/turnkeyml).

---

## Schritt 2: Verbinden Sie Ihren Server mit Open WebUI

1. √ñffnen Sie Open WebUI in Ihrem Browser.
2. Gehen Sie zu ‚öôÔ∏è **Admin-Einstellungen** ‚Üí **Verbindungen** ‚Üí **OpenAI-Verbindungen**.
3. Klicken Sie auf ‚ûï **Verbindung hinzuf√ºgen**.

   - **URL**: Verwenden Sie den API-Endpunkt Ihres Servers (zum Beispiel `http://localhost:11434/v1` f√ºr Ollama oder die Adresse Ihres eigenen Llama.cpp-Servers).
   - **API-Schl√ºssel**: Leer lassen, es sei denn, es wird ben√∂tigt.

4. Klicken Sie auf Speichern.

*Tipp: Wenn Sie Open WebUI in Docker ausf√ºhren und Ihr Modellserver auf Ihrem Host-Rechner l√§uft, verwenden Sie `http://host.docker.internal:<Ihr-Port>/v1`.*

##### **F√ºr Lemonade:**  Verwenden Sie beim Hinzuf√ºgen von Lemonade `http://localhost:8000/api/v0` als URL.

![Lemonade-Verbindung](/images/getting-started/lemonade-connection.png)

---

## Schritt 3: Loschatten!

W√§hlen Sie das Modell Ihres verbundenen Servers im Chat-Men√º und legen Sie los!

Das war's! Egal, ob Sie Llama.cpp, Ollama, LM Studio oder Lemonade w√§hlen, Sie k√∂nnen problemlos mit mehreren Modellservern experimentieren und diese verwalten ‚Äì alles in Open WebUI.

--- 

üöÄ Viel Spa√ü beim Aufbau Ihres perfekten lokalen KI-Setups!