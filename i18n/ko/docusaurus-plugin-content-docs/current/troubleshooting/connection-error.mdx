---
sidebar_position: 0
title: "🚧 서버 연결 문제"
---

우리가 모든 것을 설정하고 원활하게 실행할 수 있도록 도와드리겠습니다. 아래에는 Ollama 및 Hugging Face와 같은 외부 서버에서 일반적인 연결 문제를 해결하기 위한 다양한 상황별 단계별 지침이 있습니다.

## 🌟 Ollama 서버 연결

### 🚀 Open WebUI에서 Ollama에 접근하기

Open WebUI에서 Ollama에 연결하는 데 어려움을 겪고 계신가요? Ollama가 외부 연결을 허용하는 네트워크 인터페이스를 듣지 않기 때문일 수도 있습니다. 이를 해결합시다:

1. **Ollama를 넓게 듣도록 설정하기** 🎧:
   `OLLAMA_HOST`를 `0.0.0.0`으로 설정하여 Ollama가 모든 네트워크 인터페이스를 들을 수 있도록 만드세요.

2. **환경 변수 업데이트**:
   배포 환경에서 `OLLAMA_HOST`가 정확하게 설정되었는지 확인하세요.

3. **Ollama 재시작**🔄:
   변경 사항이 적용되려면 재시작이 필요합니다.

💡 설정 후, WebUI 인터페이스를 방문하여 Ollama가 접근 가능한지 확인하세요.

Ollama 설정에 대한 자세한 지침은 [Ollama 공식 문서](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux)를 참조하세요.

### 🐳 Docker 연결 오류

Ollama에 접근하려고 할 때 연결 오류가 발생하면 WebUI Docker 컨테이너가 호스트에서 실행 중인 Ollama 서버에 연결할 수 없기 때문일 수 있습니다. 해결 방법은 다음과 같습니다:

1. **네트워크 설정 조정하기** 🛠️:
   Docker 명령에서 `--network=host` 플래그를 사용하세요. 이렇게 하면 컨테이너가 호스트의 네트워크에 직접 연결됩니다.

2. **포트 변경하기**:
   내부 포트가 3000에서 8080으로 변경된다는 점을 기억하세요.

**Docker 명령 예제**:
```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
🔗 위 명령을 실행한 후, WebUI는 `http://localhost:8080`에서 사용할 수 있어야 합니다.

## 🔒 Hugging Face SSL 연결 문제

SSL 오류가 발생했습니까? Hugging Face 서버에 문제가 있을 수 있습니다. 해결 방법은 다음과 같습니다:

1. **Hugging Face 서버 상태 확인하기**:
   서버에서 알려진 중단 또는 문제가 있는지 확인하세요.

2. **엔드포인트 변경하기**:
   Hugging Face가 다운된 경우 Docker 명령에서 엔드포인트를 변경하세요.

**연결 문제를 위한 Docker 명령 예제**:
```bash
docker run -d -p 3000:8080 -e HF_ENDPOINT=https://hf-mirror.com/ --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

## 🍏 MacOS의 Podman

MacOS에서 Podman을 실행 중이신가요? 연결을 보장하는 방법은 다음과 같습니다:

1. **호스트 루프백 활성화하기**:
   명령에서 `--network slirp4netns:allow_host_loopback=true`을 사용하세요.

2. **OLLAMA_BASE_URL 설정하기**:
   `http://host.containers.internal:11434`를 가리키도록 설정하세요.

**Podman 명령 예제**:
```bash
podman run -d --network slirp4netns:allow_host_loopback=true -p 3000:8080 -e OLLAMA_BASE_URL=http://host.containers.internal:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

