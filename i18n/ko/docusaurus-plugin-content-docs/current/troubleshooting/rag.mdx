---
sidebar_position: 3
title: "🧠 RAG(검색 증강 생성) 문제 해결"
---

RAG(Retrieval-Augmented Generation)은 외부 콘텐츠(문서, 지식베이스 등)에 대한 추론을 가능하게 하며, 관련 정보를 검색하고 이를 모델에 전달합니다. 그러나 예상대로 작동하지 않을 때(예: 모델이 "환각"하거나 관련 정보를 놓치는 경우), 이는 종종 모델의 문제가 아니라 컨텍스트 문제일 가능성이 높습니다.

일반적인 원인과 해결책을 분석하여 RAG 정확도를 획기적으로 향상시켜 봅시다! 🚀

## 일반적인 RAG 문제와 해결 방법 🛠️

### 1. 모델이 콘텐츠를 "볼 수 없음" 👁️❌

이 문제는 가장 일반적이며, 콘텐츠 수집 과정에서 문제가 발생했을 가능성이 높습니다. 모델이 잘못된 정보를 제공하는 이유는 모델이 틀렸기 때문이 아니라, 처음부터 올바른 콘텐츠를 제공받지 못했기 때문입니다.

✅ 해결책: 콘텐츠 추출 설정 확인

- 이동 경로: **관리자 설정 > 문서**.
- 아래와 같은 강력한 콘텐츠 추출 엔진을 사용하고 있는지 확인하십시오:
  - Apache Tika
  - Docling
  - 문서 유형에 따른 사용자 정의 추출기

📌 팁: 문서를 업로드한 후 추출된 콘텐츠를 미리 확인하십시오. 비어 있거나 중요한 섹션이 누락된 경우 추출기 설정을 조정하거나 다른 엔진을 사용해야 합니다.

---

### 2. 문서의 작은 부분만 사용됨 📄➡️✂️

Open WebUI는 기본적으로 제한된 컨텍스트 창을 가진 모델과 연동됩니다. 예를 들어, 많은 로컬 모델(Ollama의 기본 모델 등)은 2048 토큰으로 제한됩니다. 따라서 Open WebUI는 검색된 콘텐츠를 모델로 전송하기 위해 제한된 공간에 맞게 과감히 줄여버립니다.

✅ 해결책:

- 이동 경로: **관리자 설정 > 문서**
- 다음 중 하나를 선택하십시오:
  - 💡 “임베딩 및 검색 우회” 활성화 — 이는 엄격한 검색 필터를 적용하지 않고 전체 콘텐츠를 직접 전송합니다.
  - 🔍 “전체 컨텍스트 모드” 활성화 — 이는 모델 프롬프트에 보다 포괄적인 콘텐츠를 주입합니다.

📌 경고: 컨텍스트 한계에 유의하십시오. 모델이 더 많은 토큰을 처리할 수 없는 경우 여전히 잘릴 수 있습니다.

---

### 3. 토큰 제한이 너무 짧음 ⏳

검색이 제대로 작동하더라도, 모델이 전달받은 모든 콘텐츠를 처리하지 못할 가능성이 있습니다—단순히 처리 용량이 부족하기 때문입니다.

기본적으로 많은 모델(특히 Ollama가 제공하는 LLM)은 2048-토큰의 컨텍스트 창으로 제한됩니다. 이는 검색된 데이터의 일부만 실제로 사용된다는 것을 의미합니다.

✅ 해결책:

- 🛠️ 모델의 컨텍스트 길이 확장:
  - 이동 경로: **모델 편집기 또는 채팅 컨트롤**
  - 컨텍스트 길이 수정(예: 지원하는 경우 8192+ 토큰으로 증가)

ℹ️ 참고: 2048-토큰 기본값은 큰 제한 요소입니다. 더 나은 RAG 결과를 위해 더 긴 컨텍스트를 지원하는 모델을 사용하는 것을 권장합니다.

✅ 대안: 더 큰 컨텍스트 용량을 가진 외부 LLM 사용

- GPT-4, GPT-4o, Claude 3, Gemini 1.5, 또는 8k+ 컨텍스트를 가진 Mixtral을 사용해 보세요.
- Ollama와 성능을 비교해 보세요—더 많은 콘텐츠를 주입할 수 있을 때 정확도 차이를 확인할 수 있습니다!

📌 팁: 생산 환경에서 더 나은 RAG 성능을 위해 외부 모델을 사용하는 것이 좋습니다.

---

### 4. 임베딩 모델이 저품질이거나 부적합함 📉🧠

임베딩 품질이 저조하면 검색 성능이 떨어집니다. 콘텐츠의 벡터 표현이 좋지 않으면 검색 도구가 올바른 콘텐츠를 가져오지 못하며, 이로 인해 LLM의 성능이 아무리 뛰어나도 문제가 발생할 수 있습니다.

✅ 해결책:

- 고품질 임베딩 모델로 변경 (예: all-MiniLM-L6-v2, Instructor X, 또는 OpenAI 임베딩)
- 이동 경로: **관리자 설정 > 문서**
- 모델 변경 후, 반드시:
  - ⏳ 기존 문서를 모두 다시 인덱싱하여 새로운 임베딩이 적용되도록 만드십시오.

📌 기억하세요: 임베딩 품질은 검색되는 콘텐츠에 직접적인 영향을 미칩니다.

---

### 5. ❌ 400: NoneType object has no attribute encode

이 오류는 임베딩 모델이 잘못 구성되었거나 누락되었음을 나타냅니다. Open WebUI가 임베딩을 생성하려고 시도하지만 유효한 모델이 로드되지 않은 경우, 텍스트를 처리할 수 없어서 이러한 오류가 발생합니다.

💥 원인:
- 임베딩 모델이 올바르게 설정되지 않았습니다.
- 다운로드가 완전히 이루어지지 않았을 수 있습니다.
- 외부 임베딩 모델을 사용하는 경우, 해당 모델이 접근 가능하지 않을 수 있습니다.

✅ 해결책:

- 이동 경로: **관리자 설정 > 문서 > 임베딩 모델**
- 임베딩 모델을 다시 저장하십시오 — 이미 선택되어 있다 하더라도 이를 강제 재확인/재다운로드합니다.
- 원격/외부 임베딩 도구를 사용하는 경우, Open WebUI에서 실행 중이며 접근 가능한지 확인하십시오.

📌 팁: 구성을 수정한 후 문서를 다시 임베딩하고 로그에서 오류가 표시되지 않는지 확인하십시오.

---

## 🧪 전문가 팁: GPT-4o 또는 GPT-4 테스트

검색, 토큰 제한, 또는 임베딩 문제인지 확신할 수 없는 경우—일시적으로 GPT-4o를 사용해 보십시오(예: OpenAI API를 통해). 결과가 갑자기 정확해진다면, 이는 로컬 모델의 컨텍스트 제한(기본적으로 Ollama에서 2048 토큰)이 병목 현상임을 강하게 시사합니다.

- GPT-4o는 더 큰 입력(128k 토큰)을 처리합니다!
- 시스템의 RAG 신뢰성을 평가하기 위한 훌륭한 기준 제공

---

## 요약 체크리스트 ✅

| 문제 | 해결 |
|--------|------|
| 🤔 모델이 콘텐츠를 “볼 수 없음” | 문서 추출기 설정 확인 |
| 🧹 콘텐츠의 일부만 사용됨 | 전체 문맥 모드 활성화 또는 임베딩 우회 |
| ⏱ 2048 토큰 제한으로 제약됨 | 모델 문맥 길이를 늘리거나 대용량 문맥 LLM 사용 |
| 📉 부정확한 검색 결과 | 더 나은 임베딩 모델로 전환한 후 재색인 |
| 여전히 헷갈리나요? | GPT-4o로 테스트하고 결과 비교 |

---

이 추출, 임베딩, 검색 및 모델 문맥 부분을 최적화하면 LLM이 문서와 더 정확하게 작동하도록 크게 개선할 수 있습니다. 2048 토큰 창이나 약한 검색 파이프라인이 AI의 능력을 제한하지 않도록 하세요 🎯.
