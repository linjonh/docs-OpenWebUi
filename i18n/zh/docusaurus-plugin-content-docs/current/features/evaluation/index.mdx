---
sidebar_position: 6
title: "📝 评估"
---


## 为什么需要评估模型？

认识一下 **Alex**，他是一家中型公司的机器学习工程师。Alex 知道市面上有许多 AI 模型——GPTs、LLaMA 等等——但哪个模型最适合手头的工作呢？这些模型在纸面上看起来都很棒，但 Alex 不能仅仅依赖公开的排行榜。这些模型的表现会因上下文而异，有些模型甚至可能在评估数据集上经过了训练（这就有点不公平了！）。此外，这些模型的写作方式有时会感觉……不对劲。

这就是 Open WebUI 的用武之地。它为 Alex 和他的团队提供了一种简单的方法，根据他们的实际需求评估模型。无需复杂的数学计算，无需繁重的操作，只需在与模型交互时给予点赞或点踩即可。

### 简要总结

- **为什么评估重要**：模型太多，但并非所有都符合你的具体需求。公开的排行榜不一定可靠。
- **如何解决问题**：Open WebUI 提供了内置的评估系统，使用点赞/点踩来评价模型响应。
- **幕后发生了什么**：评分会调整你的个性化排行榜，被评分的聊天快照将用于未来的模型微调！
- **评估选项**：
  - **竞斗模式**：随机选择模型供你比较。
  - **普通交互**：像平常一样聊天并对响应进行评分。

---

### 为什么公开评估不够？

- 公开排行榜未针对**你的**具体使用场景量身定制。
- 某些模型在评估数据集上经过训练，这影响了结果的公平性。
- 模型可能整体表现不错，但其沟通风格或响应可能与你想要的“氛围”不符。

### 解决方案：使用 Open WebUI 进行个性化评估

Open WebUI 具有内置的评估功能，让你和你的团队能够发现最适合特定需求的模型——同时还能与模型进行交互。

它如何工作？非常简单！

- **在聊天过程中**，如果你喜欢某个响应就点赞，如果不喜欢就点踩。如果该消息有**兄弟消息**（例如重新生成的响应或某种并排对比中的一部分），你就在为你的**个性化排行榜**贡献力量。
- **排行榜**可在管理部分轻松访问，帮助你跟踪哪些模型在团队中表现最佳。

还有一个很酷的功能？**每当你评分一个响应**时，系统会捕获该对话的**快照**，以后可以用来优化模型或者支持未来的模型训练。（请注意，这项功能仍在开发中！）

---

### 两种评估 AI 模型的方法

Open WebUI 提供了两种简单的方法来评估 AI 模型。

### **1. 竞斗模式**

**竞斗模式**随机从可用模型池中选择，确保评估公平且无偏。这有助于消除手动比较中的潜在缺陷：**生态效度**——确保你不会有意或无意中偏向某个模型。

如何使用：
- 从竞斗模式选择器中选择一个模型。
- 像平时一样使用它，但现在你处于“竞斗模式”。

为了让你的反馈影响排行榜，你需要一个所谓的**兄弟消息**。什么是兄弟消息？兄弟消息就是由同一查询生成的任何替代响应（比如消息重新生成或多个模型并排生成响应）。这样，你是在进行**对比评估**。

- **评分提示**：当你点赞一个响应时，另一个会自动被点踩。所以，请注意，只点赞你认为真正最好的消息！
- 一旦你对响应进行了评分，你可以查看排行榜以了解模型的表现对比。

以下是竞斗模式界面工作方式的预览：

![竞斗模式示例](/images/evaluation/arena.png)

需要更多深度？你甚至可以复制一个类似于 [**Chatbot Arena**](https://lmarena.ai/) 的设置！

![Chatbot Arena 示例](/images/evaluation/arena-many.png)

### **2. 普通交互**

如果你不想切换到“竞斗模式”，也没关系。你可以正常使用 Open WebUI，并像日常操作一样对 AI 模型响应进行评分。只需在模型响应时随时点赞/点踩即可。然而，**如果你希望你的反馈被用于排行榜排名**，你需要**替换模型并与不同的模型进行互动**。这样确保有一个**兄弟响应**与之比较——只有两个不同模型之间的比较才能影响排名。

例如，这是你在普通交互期间评分的方式：

![普通模型评分界面](/images/evaluation/normal.png)

以下是设置多模型对比的示例，类似于竞斗模式：

![多模型对比](/images/evaluation/normal-many.png)

---

## 排行榜

评分之后，请查看管理员面板下的**排行榜**。这里会以视觉方式展示模型的表现，使用**Elo评分系统**（参考国际象棋排名！）排名。你会直观地看到哪些模型在评估中真正脱颖而出。

以下是一个示例排行榜布局：

![排行榜示例](/images/evaluation/leaderboard.png)

### 基于主题的重新排名

在对对话进行评分时，你可以按**主题标记**它们，以获得更细化的洞察。这在不同领域（例如**客户服务、创意写作、技术支持**等）中尤其有用。

#### 自动标记
Open WebUI会尝试根据对话主题**自动标记聊天**。但是，根据你使用的模型，自动标记功能有时候可能**失败**或误解对话内容。这种情况下，最好用**手动标记聊天**来确保反馈准确。

- **如何手动标记**：当你给某个回复评分时，你可以根据对话的上下文添加自己的标签。
  
不要跳过这一步！标记非常强大，因为它可以让你**根据特定主题重新排名模型**。例如，你可能想知道哪个模型在回答技术支持问题时表现最佳，而哪个模型在处理一般客户咨询时更出色。

以下是重新排名的示例：

![按主题重新排名的排行榜](/images/evaluation/leaderboard-reranked.png)

---

### 旁注：用于模型微调的聊天快照

每当你对模型的回复进行评分时，Open WebUI会*捕捉该聊天的快照*。这些快照最终可以用来**微调你自己的模型**——因此你的评估会为AI的持续改进提供支持。

*（敬请期待该功能的更多更新，它正在积极开发中！）*

---

## 总结

**简而言之**，Open WebUI的评估系统有两个明确目标：
1. 帮助你**轻松比较模型**。
2. 最终找到最符合你个人需求的模型。

从本质上来说，这个系统旨在让AI模型评估对每个用户来说变得**简单、透明且可定制**。无论是通过“竞技场模式”还是普通聊天互动，**你完全可以自主决定哪个AI模型最适合你的具体用例**！

**一如既往**，你的所有数据都安全存储在**你的实例**中，除非你特别选择**参与社区共享**，否则不会共享任何数据。你的隐私和数据自主权始终处于优先地位。