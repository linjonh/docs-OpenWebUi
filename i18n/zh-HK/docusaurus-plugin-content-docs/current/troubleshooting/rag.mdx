---
sidebar_position: 3
title: "🧠 疑難排解 RAG（檢索增強生成）"
---

檢索增強生成（RAG）使語言模型能夠處理外部內容——如文件、知識庫等，通過檢索相關信息並將其輸入模型來實現推理。然而當事情未如預期進行（例如，模型"產生幻覺"或遺漏相關信息）時，通常問題不在於模型本身，而是出在上下文方面。

讓我們解析常見原因和解決方法，以便全面提升 RAG 的準確性！🚀

## 常見 RAG 問題及其解決方法 🛠️

### 1. 模型「看不見」您的內容 👁️❌

這是最常見的問題——通常是由內容攝取過程中的問題引發的。模型並非因為出錯而產生幻覺，而是因為它壓根沒接收到正確的內容。

✅ 解決方法：檢查您的內容提取設置

- 導航至：**管理設置 > 文檔**。
- 確保您正在使用功能強大的內容提取引擎，例如：
  - Apache Tika
  - Docling
  - 根據文檔類型自定義的提取器

📌 提示：嘗試上傳一個文檔並預覽提取的內容。如果內容是空白的或缺少關鍵部分，則需要調整提取器設置或使用不同的引擎。

---

### 2. 僅使用了文檔的一小部分 📄➡️✂️

Open WebUI 設計與默認具有有限上下文窗口的模型配合工作。例如，許多本地模型（如 Ollama 的默認模型）被限制為 2048 個 Token。因此，Open WebUI 會積極修剪檢索的內容以適應假定的可用空間。

✅ 解決方法：

- 前往 **管理設置 > 文檔**
- 進行以下任一操作：
  - 💡 啟用「繞過編碼與檢索」——此功能會將完整內容直接發送，而不應用嚴格的檢索篩選。
  - 🔍 打開「完整上下文模式」——此功能會將更全面的內容注入到模型提示中。

📌 警告：需注意上下文限制——如果您的模型無法處理更多的 Token，仍然會出現被截斷的情況。

---

### 3. Token 限制過短 ⏳

即使檢索成功，您的模型可能仍然無法處理收到的全部內容——因為它根本無法。

許多模型（尤其是 Ollama 托管的 LLM）默認限制在 2048 個 Token 的上下文窗口。所以，實際上只有一部分被檢索的數據會被使用。

✅ 解決方法：

- 🛠️ 延展模型的上下文長度：
  - 前往 **模型編輯器或聊天控件**
  - 修改上下文長度（例如，如果支持，可增加到 8192+ Token）

ℹ️ 注意：2048 Token 的默認設置是一個重大瓶頸。我們建議使用支持更長上下文的模型來改進 RAG 效果。

✅ 替代方案：使用支持更大上下文容量的外部 LLM

- 試試 GPT-4、GPT-4o、Claude 3、Gemini 1.5 或 Mixtral，這些可以提供 8k+ 上下文
- 將性能與 Ollama 進行比較——注意當注入更多內容時準確性的差異！

📌 提示：在生產使用中，依賴外部模型會帶來更好的 RAG 表現。

---

### 4. 嵌入模型質量低下或不匹配 📉🧠

糟糕的嵌入 = 糟糕的檢索。如果您的內容的向量表徵質量差，檢索器將無法提取正確的內容——無論您的 LLM 有多麼強大。

✅ 解決方法：

- 切換到高質量的嵌入模型（例如 all-MiniLM-L6-v2、Instructor X 或 OpenAI 的嵌入）
- 前往：**管理設置 > 文檔**
- 更換模型後，務必：
  - ⏳ 重新索引所有現有文檔，以使新嵌入生效。

📌 記得：嵌入質量直接影響檢索的內容。

---

### 5. ❌ 400: NoneType object has no attribute encode

此錯誤表明嵌入模型的配置有誤或缺失。當 Open WebUI 嘗試創建嵌入但沒有加載有效的模型時，它無法處理文本，導致此加密的錯誤發生。

💥 原因：
- 您的嵌入模型未正確設置。
- 可能未完全下載。
- 如果您使用的是外部嵌入模型，它可能無法訪問。

✅ 解決方法：

- 前往：**管理設置 > 文檔 > 嵌入模型**
- 再次保存嵌入模型——即使它已被選中。此操作將強制重新檢查/下載。
- 如果您使用的是遠程/外部嵌入工具，請確保它正在運行並且可以被 Open WebUI 訪問。

📌 提示：修復配置後，嘗試重新嵌入文檔，並確認日誌中未顯示錯誤。

---

## 🧪 專家提示：使用 GPT-4o 或 GPT-4 進行測試

如果您不確定問題出在檢索、Token 限制還是嵌入環節——可以暫時使用 GPT-4o（例如通過 OpenAI API）。如果結果突然變得更準確，則明顯表明您的本地模型上下文限制（Ollama 默認為 2048）是瓶頸所在。

- GPT-4o 可處理更大輸入（128k Token！）
- 提供了一個很好的標準來評估系統的 RAG 可靠性

---

## 總結檢查清單 ✅

| 問題 | 修正 |
|--------|------|
| 🤔 模型無法“看到”內容 | 檢查文檔提取器設置 |
| 🧹 僅使用部分內容 | 啟用完整上下文模式或繞過嵌入 |
| ⏱ 受限於2048個token限制 | 增加模型上下文長度或使用大上下文LLM |
| 📉 檢索不準確 | 切換到更好的嵌入模型，然後重新索引 |
| 仍感到困惑？ | 使用GPT-4o進行測試並比較輸出結果 |

---

通過優化這些方面——提取、嵌入、檢索和模型上下文——您可以大幅提升您的LLM與文檔交互的準確性。不要讓2048-token的窗口或弱檢索管線限制您的AI能力 🎯。
